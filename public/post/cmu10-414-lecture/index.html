<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=13448&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>CMU10-414 lecture | Shihong Yuan's Course Note</title>
<meta name=keywords content="System"><meta name=description content="Desc Text."><meta name=author content="Me"><link rel=canonical href=https://canonical.url/to/page><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><link rel=icon href=http://localhost:13448/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=http://localhost:13448/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=http://localhost:13448/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=http://localhost:13448/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=http://localhost:13448/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:13448/post/cmu10-414-lecture/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><meta property="og:url" content="http://localhost:13448/post/cmu10-414-lecture/"><meta property="og:site_name" content="Shihong Yuan's Course Note"><meta property="og:title" content="CMU10-414 lecture"><meta property="og:description" content="Desc Text."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="post"><meta property="article:published_time" content="2025-01-23T11:30:03+00:00"><meta property="article:modified_time" content="2025-01-23T11:30:03+00:00"><meta property="article:tag" content="System"><meta property="og:image" content="http://localhost:13448/%3Cimage%20path/url%3E"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:13448/%3Cimage%20path/url%3E"><meta name=twitter:title content="CMU10-414 lecture"><meta name=twitter:description content="Desc Text."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:13448/post/"},{"@type":"ListItem","position":2,"name":"CMU10-414 lecture","item":"http://localhost:13448/post/cmu10-414-lecture/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"CMU10-414 lecture","name":"CMU10-414 lecture","description":"Desc Text.","keywords":["System"],"articleBody":"《CMU 10-414 deep learning system》学习笔记 写在最前面 从 2024-04-28 到 2024-09-08，历时四个多月，总算把 DLSys 学完了。这门课的一些收获：\n自动微分理论知识和在实践过程中衍生的包括计算图等知识 系统学习了 ML 中几个基本模型和组件 Tensor 的 strides 相关内容 基础 CUDA 编程 个人认为这门课一些没达到我预期的地方：\nCUDA 编程的内容太浅 后续讲 CNN、RNN、Transformer 的部分没必要，可以继续深入 CUDA 或者压缩课时 本门课程的核心内容在 Lecture 015，对应的 homework 是 hw03，后面的内容没有时间可以跳过。\nps：全文章两万余字，Chrome 渲染图片时可能会很卡，建议使用 Microsoft Edge 浏览。\nLecture 1: Introduction and Logistics 课程的目标 本课程的目标是学习现代深度学习系统，了解包括自动微分、神经网络架构、优化以及 GPU 上的高效操作在内的技术的底层原理。作为实践，本课程将实现一个 needle（deep learning library）库，类似 PyTorch。\n为什么学习深度学习系统？ 为什么学习？深度学习这一概念很早就存在了，但直到 PyTorch、TensorFlow 此类现代深度学习框架发布，深度学习才开始迅速发展。简单易用的自动差分库是深度学习发展的最大动力。\n除了使用这些库，我们为什么还要学习深度学习系统？\n为了构建深度学习系统\n如果想要从事深度学习系统的开发，那毫无疑问得先学习它。目前深度学习框架并没完全成熟，还有很多开发新功能，乃至新的框架的机会。\n为了能够更高效地使用现有系统\n了解现有系统的内部实现，可以帮助我们写出更加高效的深度学习代码。如果想要提高自定义算子的效率，那必须先了解相关操作是如何实现的。\n深度学习系统本身就很有趣\n尽管这个系统看上去很复杂，但是其核心算法的原理确实相当简单的。两千行左右的代码，就可以写出一个深度学习库。\n预备知识 systems programming 线性代数 其他数学知识：计算、概率、简单的证明 Python 和 C++ 经验 机器学习的相关经验 Lecture 2: ML Refresher \u0026 Softmax Regression 机器学习基础 深度学习是由数据驱动的，所谓数据驱动，这意味着当我们想要写一个用于识别手写数字的模型时，我们关注的不是某个数字形状上有什么特点，如何通过编程识别该特点，而是直接将数据集喂给模型，模型自动训练并识别数字类别。\n深度学习模型由三部分组成：\n假说模型：模型的结构，包括一系列参数，其描述了模型从输入到输出的映射关系； 损失函数：指定了对模型的评价，损失函数值越小，说明该模型在指定任务上完成得更好； 优化方法：用于对模型中参数进行优化，使得损失函数最小的方法。 Softmax 回归 以经典的 softmax 回归模型为例，简单回顾一下 ML 模型。\n考虑一个 k 分类任务，其中数据集为 x(i)∈Rn , y(i)∈1,…,k i=1,…,mx(i)∈Rn , y(i)∈1,…,k i=1,…,m，nn 标识输入数据集的维度，kk 标识标签类别数，mm 标识数据集样本数量。\n一个假说模型就是将一个 nn 维的输入映射到一个 kk 维的输出，即：h:Rn→Rkh:Rn→Rk。注意，模型并不会直接输出类别的序号，而是通过输出一个 kk 维向量 h(x)h(x)，其中第 ii 个元素 hi(x)hi​(x) 表示是第 ii 个类别的概率。\n对于线性模型来说，使用 θ∈Rn×kθ∈Rn×k 这个模型中的参数，那么 $hθ(x)=θTx$。\n如果一次输入多个数据，那么输入数据就可以组织成一个矩阵，相比起多个向量操作，矩阵的操作通常效率更高，我们在代码实现中一般也是用矩阵操作。数据集可以表示为：\n$$X∈Rm×n=[x(1)Tx(m)T],y∈{1,…,k}m=[y(1)y(m)]​​$$\n数据集的矩阵是一个个样本转置后堆叠 stack 起来的。那么输出可以表示为：\n$$hθ(X)=[hθ(x(1))Thθ(x(m))T]=[x(1)Tθx(m)Tθ]=Xθ$$\n关于损失函数 lerrlerr​，一种朴素的想法是将模型预测错误的模型数据量作为损失函数，即如果模型预测的正确率最高的那个类别与真实类别不相同，则损失函数为 1，否则为 0：\n$$lerr(h(x),y)={0 if argmaxihi(x)=y1 otherwiselerr​(h(x),y)={0 1 ​if argmaxi​hi​(x)=yotherwise​$$\n遗憾的是，这个符合直觉函数是不可微分的，难以对参数进行优化。更合适的做法是使用交叉熵损失函数。\n在此之前，我们将先讲输出过一个 softmax 函数，使之的行为更像一个概率——各个类别的概率之和为 1：\n$$zi=p(label=i)=exp⁡(hi(x))∑j=1kexp⁡(hj(x))​$$\n那么交叉熵损失函数就可以定义为：\n$$lce(h(x),y)=−log⁡p(label=y)=−hy(x)+log⁡∑j=1kexp⁡(hj(x))lce​(h(x),y)=−logp(label=y)=−hy​(x)+logj=1∑k​exp(hj​(x))$$\n注意在计算交叉熵时，通过运算进行了化简，这使得我们可以省去计算 softmax 的过程，直接计算最终的结果。不但如此，交叉熵的计算中，如果 hi(x)hi​(x) 的值很小，那么取对数会出现很大的值，化简后的计算则避免了这种情况。\n所有的深度学习问题，都可以归结为一下这个最优化问题：\n$$θ 1m∑i=1ml(hθ(x(i)),y(i)))θminimize​ m1​i=1∑m​l(hθ​(x(i)),y(i)))$$\n我们使用梯度下降法对该问题进行优化。在此之前，首先介绍一下关于梯度。我们的优化目标可以看作一个关于θ∈Rn×kθ∈Rn×k的函数ff，那么其在θ0θ0​处的梯度可以表示为：\n$$∇θf(θ0)∈Rn×k=[∂f(θ0)∂θ11⋯∂f(θ0)∂θk1⋱∂f(θ0)∂θn1⋯∂f(θ0)∂θnk]∇θ​f(θ0​)∈Rn×k=​∂θ11​∂f(θ0​)​⋮∂θn1​∂f(θ0​)​​⋯⋱⋯​∂θk1​∂f(θ0​)​⋮∂θnk​∂f(θ0​)​​​$$\n其中，第ii行第jj个元素表示除θijθij​之外的参数都被当作常数，对θijθij​求偏导。\n梯度下降，就是沿着梯度方向不断进行迭代，以求找到最佳的θθ使得目标函数值最小。\n$$θ:=θ0−α∇f(θ0)θ:=θ0​−α∇f(θ0​)$$\n上式中，αα被称为学习率或者步长。\n事实上，在现代深度学习中，并不是使用的传统梯度下降的方案，因为其无法将所有训练集一次性读入并计算梯度。现代使用的是随机梯度下降（Stochastic Gradient Descent，SGD）\n首先将m个训练集样本划分一个个小batch，每个batch都有B条数据。那每一batch的数据表示为X∈RB×nX∈RB×n，更新参数θθ的公式变为：\nθ:=θ0−αB∇f(θ0)θ:=θ0​−Bα​∇f(θ0​)\n我们的梯度变成了每个小batch对全体样本梯度的估计。\n那如何计算梯度表达式呢？梯度矩阵中每个元素都是一个偏导数，我们就先从计算偏导数开始。假设hh是个向量，我们来计算偏导数∂lce(h,y)∂hi∂hi​∂lce​(h,y)​：\n$$∂lce(h,y)∂hi=∂∂hi(−hy+log⁡∑j=1kexp⁡hj)=−1{i=y}+exp⁡(hj)∑j=1kexp⁡hj=−1{i=y}+softmax(h)=z−ey∂hi​∂lce​(h,y)​​=∂hi​∂​(−hy​+logj=1∑k​exphj​)=−1{i=y}+∑j=1k​exphj​exp(hj​)​=−1{i=y}+softmax(h)=z−ey​​$$\n如果hh是个向量，那么梯度∇hlce(h,y)∇h​lce​(h,y)就能够以向量的形式表示为：\n$$∇hlce(h,y)=z−ey$$\n这里我们将对hh进行softmax标准化记为zz，eyey​表示对应的单位向量。\n事实上，我们要计算的梯度是关于θθ的，具体来说，表达式为∇θlce(θTx,y)∇θ​lce​(θTx,y)，其中，θθ是个矩阵。或许，可以使用链式法则进行求解，但是太麻烦了，这里还涉及矩阵对向量的求导。我们需要一种更加通用的求导方案。\n有两个解决办法：\n正确且官方的做法：使用矩阵微分学、雅可比矩阵、克罗内克积和向量化等知识进行求解。 一个hacky、登不上台面、但大家都在用的方案：将所有的矩阵和向量当作标量，使用链式法则求解，并进行转置操作使得结果的size符合预期，最后检查数值上结果是否正确。 按照第二个方法的逻辑，过程为：\n∂∂θlce(θTx,y)=∂lce(θTx,y)∂θTx⋅∂θTx∂θ=[z−ey]k×1⋅xn×1=x⋅[z−ey]∂θ∂​lce​(θTx,y)​=∂θTx∂lce​(θTx,y)​⋅∂θ∂θTx​=[z−ey​]k×1​⋅xn×1​=x⋅[z−ey​]​\n其中，z=softmax(θTx)z=softmax(θTx)。注意，倒数第二步求出的结果是两个列向量相乘，不能运算。又已知结果应该是n×kn×k的矩阵，调整向量之间的顺序即可。\n照猫画虎，可以写出batch的情况，X∈RB×nX∈RB×n：\n∂∂θlce(θTX,y)=∂lce(θTX,y)∂θTX⋅∂θTX∂θ=[Z−Ey]B×k⋅XB×n=XT⋅[Z−Ey]∂θ∂​lce​(θTX,y)​=∂θTX∂lce​(θTX,y)​⋅∂θ∂θTX​=[Z−Ey​]B×k​⋅XB×n​=XT⋅[Z−Ey​]​\nLecture 3: Manual Neural Networks 这节课，我们将人工实现全连接神经网络，之后的课程，将引入自动微分技术。\n从线性模型转变为非线性模型 如上图所示，线性模型本质上是将样本空间划分为线性的几个部分，这样的模型性能十分有限，因此很多不满足这样分布的实际问题就不能被解决。\n一种解决方案是，在将样本输入到线性分类器前，先人工挑选出某些特征，即对XX应用一个函数ϕϕ，其将XX映射到ϕ(X)ϕ(X)上，映射后的空间可以被线性划分。一方面，它确实是早期实践中行之有效的方案；另一方面，人工提取特征的泛化性能有限，受限于具体问题和研究人员的对问题的洞察程度。\n如果我们使用线性网络提取特征，并直接接上一个线性分类头，这两个线性层等效为一个线性层，并不能做到非线性化的要求（基础知识，此处不再解释）。\n因此，在使用线性网络提取特征后，需要再接上一个非线性函数σσ，即ϕ=σ(WTX)ϕ=σ(WTX)。\n神经网络 上文提到的使用非线性函数后的模型，就可以视作一种最简单的神经网络。所谓神经网络，值得是机器学习中某一类特定的假说模型，其由多层组成，每一层都有大量可以微分的参数。\n神经网络最初的确起源于模拟人类神经元这一动机，但随着其发展，越来越多的神经网络模型出现，与人类大脑神经网络越来越不相关。\n以双层神经网络为例，其形式化表示为hθ(x)=W2Tσ(W1Tx)hθ​(x)=W2T​σ(W1T​x)，所有可学习的参数使用θθ表示。以batch的矩阵形式表示为：\nhθ(X)=σ(XW1)W2hθ​(X)=σ(XW1​)W2​\n接下来给出L层多层感知机（a.k.a. MLP、前馈神经网络、全连接层）的形式化表达：\n{Zi+1=σi(ZiWi),i=1,…,LZ1=Xhθ(X)=ZL+1[Zi∈Rm×ni,Wi∈Rni×ni+1]σi:R→R⎩⎨⎧​Zi+1​=σi​(Zi​Wi​),i=1,…,LZ1​=Xhθ​(X)=ZL+1​[Zi​∈Rm×ni​,Wi​∈Rni​×ni+1​]σi​:R→R​\n每一层的输入为ZiZi​，输出为Zi+1Zi+1​。\n为什么要是用深度网络而不是宽度网络？没有很完美的解释，但最好并且最现实的解释是：经验证明，当参数量固定时，深度网络性能优于宽度网络。\n反向传播（梯度计算） 与Lecture 2一致，使用交叉熵作为损失函数，使用SGD作为优化算法，唯一的区别是，这次要对MLP网络求解梯度。\n对于两层神经网络hθ(X)=σ(XW1)W2hθ​(X)=σ(XW1​)W2​，待求的梯度表达式为：\n∇{W1,W2}lce(σ(XW1)W2,y)∇{W1​,W2​}​lce​(σ(XW1​)W2​,y)\n对于W2W2​的梯度，其与Lecture 2的计算类似：\n∂lce(σ(XW1)W2,y)∂W2=∂lce(σ(XW1)W2,y)∂σ(XW1)W2⋅∂σ(XW1)W2∂W2=(S−Iy)m×k⋅σ(XW1)m×d=σ(XW1)T⋅(S−Iy)[S=softmax(σ(XW1))]∂W2​∂lce​(σ(XW1​)W2​,y)​​=∂σ(XW1​)W2​∂lce​(σ(XW1​)W2​,y)​⋅∂W2​∂σ(XW1​)W2​​=(S−Iy​)m×k​⋅σ(XW1​)m×d​=σ(XW1​)T⋅(S−Iy​)[S=softmax(σ(XW1​))]​\n对于W1W1​的梯度，其需要多次应用链式法则，但并不难计算：\n∂lce(σ(XW1)W2,y)∂W1=∂lce(σ(XW1)W2,y)∂σ(XW1)W2⋅∂σ(XW1)W2∂σ(XW1)⋅∂σ(XW1)∂XW1⋅∂XW1∂X1=(S−Iy)m×k⋅[W2]d×k⋅σ′(XW1)m×d⋅Xm×n=XT⋅[σ′(XW1)⊙((S−Iy)⋅W2T)][S=softmax(σ(XW1))]∂W1​∂lce​(σ(XW1​)W2​,y)​​=∂σ(XW1​)W2​∂lce​(σ(XW1​)W2​,y)​⋅∂σ(XW1​)∂σ(XW1​)W2​​⋅∂XW1​∂σ(XW1​)​⋅∂X1​∂XW1​​=(S−Iy​)m×k​⋅[W2​]d×k​⋅σ′(XW1​)m×d​⋅Xm×n​=XT⋅[σ′(XW1​)⊙((S−Iy​)⋅W2T​)][S=softmax(σ(XW1​))]​\n以上公式中⊙⊙表示逐元素乘法。至于为啥这么算，俺也不知道。\n接下来将其推广到一般情况，即LL层的MLP中对WiWi​求导：\n∂l(Zl+1,y)∂Wi=∂l∂Zl+1⋅∂Zl+1∂Zl⋅…⋅∂Zi+2∂Zi+1⋅∂Zi+1∂Wi=Gi+1⋅∂Zi+1∂Wi=∂l∂Zi+1⋅∂Zi+1Wi∂Wi​∂l(Zl+1​,y)​​=∂Zl+1​∂l​⋅∂Zl​∂Zl+1​​⋅…⋅∂Zi+1​∂Zi+2​​⋅∂Wi​∂Zi+1​​=Gi+1​⋅∂Wi​∂Zi+1​​=∂Zi+1​∂l​⋅Wi​∂Zi+1​​​\n由上述公式，我们可以得到一个反向迭代计算的GiGi​，即：\nGi=Gi+1⋅Zi+1Zi=Gi+1⋅∂σ(ZiWi)∂ZiWi⋅∂ZiWiZi=Gi+1⋅σ′(ZiWi)⋅WiGi​​=Gi+1​⋅Zi​Zi+1​​=Gi+1​⋅∂Zi​Wi​∂σ(Zi​Wi​)​⋅Zi​∂Zi​Wi​​=Gi+1​⋅σ′(Zi​Wi​)⋅Wi​​\n上面的计算都是将矩阵当作标量进行的，接下来我们考虑其维度。已知，Zi∈Rm×niZi​∈Rm×ni​是第ii层的输入，Gi=∂l∂ZiGi​=∂Zi​∂l​，其维度如何呢？GiGi​每个元素表示损失函数ll对第ii层输入的每一项求偏导，也可以记作是ll对ZiZi​求梯度，即∇Zil∇Zi​​l，其维度显然是m×nim×ni​，继续计算前文GiGi​：\nGi=[Gi+1]m×ni+1⋅σ′(ZiWi)m×ni+1⋅[Wi]ni×ni+1=[Gi+1⊙σ′(ZiWi)]WiTGi​​=[Gi+1​]m×ni+1​​⋅σ′(Zi​Wi​)m×ni+1​​⋅[Wi​]ni​×ni+1​​=[Gi+1​⊙σ′(Zi​Wi​)]WiT​​\n有了GiGi​，就可以继续计算ll对WiWi​的偏导数了：\n∂l(Zl+1,y)∂Wi=Gi+1⋅∂Zi+1∂Wi=Gi+1⋅∂σ(ZiWi)∂ZiWi⋅∂ZiWi∂Wi=[Gi+1]m×ni+1⋅σ′(ZiWi)m×ni+1⋅[Zi]m×ni=ZiT⋅[Gi+1⊙σ′(ZiWi)]∂Wi​∂l(Zl+1​,y)​​=Gi+1​⋅∂Wi​∂Zi+1​​=Gi+1​⋅∂Zi​Wi​∂σ(Zi​Wi​)​⋅∂Wi​∂Zi​Wi​​=[Gi+1​]m×ni+1​​⋅σ′(Zi​Wi​)m×ni+1​​⋅[Zi​]m×ni​​=ZiT​⋅[Gi+1​⊙σ′(Zi​Wi​)]​\n至此，每个小组件都已制造完毕，让我们来把它装起来吧！\n前向传播 初始化：Z1=XZ1​=X 迭代：Zi+1=σ(ZiWi)Zi+1​=σ(Zi​Wi​) 直至i=Li=L（注意，最后一层没有非线性部分，此处没有展示出来） 反向传播 初始化：GL+1=S−IyGL+1​=S−Iy​ 迭代：Gi=[Gi+1⊙σ′(ZiWi)]WiTGi​=[Gi+1​⊙σ′(Zi​Wi​)]WiT​ 直至i=1i=1 值得注意的是，在反向传播中，需要用到前向传播的中间结果ZiZi​。为了更高效地计算梯度，不得不以牺牲内存空间为代价，即空间换时间。 许多课程，讲到这里就结束了，但对我们这门课来说，才刚刚开始…\nLecture 4: Automatic Differentiation 基本工具 计算图 计算图是自动微分中常用的一种工具。计算图是一张有向无环图，每个节点表示（中间结果）值，每条边表示输入输出变量。例如，y=f(x1,x2)=ln⁡(x1)+x1x2−sin⁡x2y=f(x1​,x2​)=ln(x1​)+x1​x2​−sinx2​对应的计算图为：按照拓扑序列遍历这张图，就可以得到对应表达式的值。 对自动微分方法的简单介绍 深度学习中，一个核心内容就是计算梯度。这里介绍集中计算梯度的方案：\n偏导数定义 梯度是由一个个偏导数组成的，可以直接根据偏导数的定义来计算梯度：\n∂f(θ)∂θi=lim⁡ϵ→0f(θ+ϵei)−f(θ)ϵ∂θi​∂f(θ)​=ϵ→0lim​ϵf(θ+ϵei​)−f(θ)​\n其中，eiei​是表示第ii个方向上的单位向量。\n数值求解 根据上述定义，我们可以选取一个很小的量代入ϵϵ，得到数值计算偏导的方法：\n∂f(θ)∂θi=f(θ+ϵei)−f(θ−ϵei)2ϵ+o(ϵ2)∂θi​∂f(θ)​=2ϵf(θ+ϵei​)−f(θ−ϵei​)​+o(ϵ2)\n这里并不是直接使用第一项的公式，即分子不是f(θ+ϵei)−f(θ)f(θ+ϵei​)−f(θ)，并且误差项是ϵ2ϵ2，这是由于泰勒展开：\nf(θ+δ)=f(θ)+f′(θ)δ+12f′′(θ)δ2+o(δ3)f(θ−δ)=f(θ)+f′(θ)δ−12f′′(θ)δ2+o(δ3)f(θ+δ)=f(θ)+f′(θ)δ+21​f′′(θ)δ2+o(δ3)f(θ−δ)=f(θ)+f′(θ)δ−21​f′′(θ)δ2+o(δ3)​\n上述两式作差，即可得到数值计算f′(θ)f′(θ)的方法。\n这个方法的问题在于存在误差，并且效率低下（这里要计算两次f），该方法常用于验证其它方法的具体实现是否出错。具体来说，验证如下等式是否成立：\nδT∇θf(θ)=f(θ+ϵδ)−f(θ−ϵδ)2ϵ+o(ϵ2)δT∇θ​f(θ)=2ϵf(θ+ϵδ)−f(θ−ϵδ)​+o(ϵ2)\n其中δδ是单位球上的某个向量，∇θf(θ)∇θ​f(θ)是使用其它方法计算得到的梯度。等式左边是其它方法计算的梯度在δδ上的投影，右侧是使用数值求解得到的梯度值，验证该等式是否成立就可以判断左侧梯度是否计算错误。\n符号微分 符号微分，就是根据微分的计算规则使用符号手动计算微分。部分规则为：\n∂(f(θ)+g(θ))∂θ=∂f(θ)∂θ+∂g(θ)∂θ∂(f(θ)g(θ))∂θ=g(θ)∂f(θ)∂θ+f(θ)∂g(θ)∂θ∂f(g(θ))∂θ=∂f(g(θ))∂g(θ)∂g(θ)∂θ​∂θ∂(f(θ)+g(θ))​=∂θ∂f(θ)​+∂θ∂g(θ)​∂θ∂(f(θ)g(θ))​=g(θ)∂θ∂f(θ)​+f(θ)∂θ∂g(θ)​∂θ∂f(g(θ))​=∂g(θ)∂f(g(θ))​∂θ∂g(θ)​​\n根据该公式，可以计算得到f(θ)=∏i=1nθif(θ)=∏i=1n​θi​的梯度表达式为：∂f(θ)∂θk=∏j≠knθj∂θk​∂f(θ)​=∏j=kn​θj​。如果我们根据该公式来计算梯度，会发现需要计算n(n−2)n(n−2)次乘法才能得到结果。这是因为在符号运算的过程中，我们忽略了可以反复利用的中间结果。\n正向模式自动微分 forward mode automatic differentiation 沿着计算图的拓扑序列，同样可以计算出输出关于输入的导数，还是以y=f(x1,x2)=ln⁡(x1)+x1x2−sin⁡x2y=f(x1​,x2​)=ln(x1​)+x1​x2​−sinx2​为例，其计算图为：\n整个梯度计算过程如下，在此过程中应用到了具体函数的求导公式：\nx1=2x2=5v˙1=1v˙2=0v˙3=v˙1/v1=0.5v˙4=v˙1v2+v˙2v1=1×5+0×2=5v˙5=v2˙cos⁡v2=0×cos⁡5=0v˙6=v˙3+v˙4=0.5+5=5.5v˙7=v6˙−v5˙=5.5−0=5.5​x1​=2x2​=5v˙1​=1v˙2​=0v˙3​=v˙1​/v1​=0.5v˙4​=v˙1​v2​+v˙2​v1​=1×5+0×2=5v˙5=v2​˙​cosv2​=0×cos5=0v˙6​=v˙3​+v˙4​=0.5+5=5.5v˙7​=v6​˙​−v5​˙​=5.5−0=5.5​\n对于f:Rn→Rkf:Rn→Rk，前向传播需要nn次前向计算才能得到关于每个输入的梯度，这就意味前向传播适合nn比较小、kk比较大的情况。但是在深度学习中，通常nn比较大、kk比较小。\n反向模式自动微分 定义adjoint:vi‾=∂y∂viadjoint:vi​​=∂vi​∂y​,其表示损失函数对于参数vivi​的偏导。 整个计算过程如下所示，需要注意的是v2‾v2​​的计算过程，其在计算图上延伸出了两个节点，因此梯度也由两部分相加：\nv7‾=∂y∂v7=1v6‾=v7‾∂v7∂v6=v7‾×1=1v5‾=v7‾∂v7∂v5=v7‾×(−1)=−1v4‾=v6‾∂v6∂v4=v6‾×1=1v3‾=v6‾∂v6∂v3=v6‾×1=1v2‾=v5‾∂v5∂v2+v4‾∂v4∂v2=v5‾×cos⁡v2+v4‾×v1v1‾=v4‾∂v4∂v1+v3‾∂v3∂v1=v4‾×v2+v3‾1v1=5+12=5.5​v7​​=∂v7​∂y​=1v6​​=v7​​∂v6​∂v7​​=v7​​×1=1v5​​=v7​​∂v5​∂v7​​=v7​​×(−1)=−1v4​​=v6​​∂v4​∂v6​​=v6​​×1=1v3​​=v6​​∂v3​∂v6​​=v6​​×1=1v2​​=v5​​∂v2​∂v5​​+v4​​∂v2​∂v4​​=v5​​×cosv2​+v4​​×v1​v1​​=v4​​∂v1​∂v4​​+v3​​∂v1​∂v3​​=v4​​×v2​+v3​​v1​1​=5+21​=5.5​\n接下来我们讨论一下为什么前文中v2‾v2​​由两部分组成。考虑如下一个计算图：\nyy可以被视作关于v2v2​和v3v3​的函数，即y=f(v2,v3)y=f(v2​,v3​)，那么：\nv1‾=∂y∂v1=∂f(v2,v3)∂v2∂v2∂v1+∂f(v2,v3)∂v3∂v3∂v1=v2‾∂v2∂v1+v3‾∂v3∂v1v1​​=∂v1​∂y​=∂v2​∂f(v2​,v3​)​∂v1​∂v2​​+∂v3​∂f(v2​,v3​)​∂v1​∂v3​​=v2​​∂v1​∂v2​​+v3​​∂v1​∂v3​​\n因此，定义partial adjoint vi→j‾=vj‾∂vj∂vivi→j​​=vj​​∂vi​∂vj​​，那么vi‾vi​​可以表示为：\nνi‾=∑j∈next(i)νi→j‾νi​​=j∈next(i)∑​νi→j​​\n反向模式微分算法的实现 基于以上分析，可以写出如下的实现反向模式微分算法的伪代码：\n其中node_to_grad是一个字典，保存着每个节点的partial adjoint值。由于是按照逆拓扑序列遍历的节点，因此可以保证当遍历到ii时，所有以ii为输入的节点（k节点所在的集合）都已被遍历完毕，即vk‾vk​​已经计算出来。\n那么partial adjoint值使用什么数据结构保存呢？一个常见的思路是使用邻接矩阵，但是这个矩阵中有大量元素是不存在了，空间浪费很大。我们可以在原有计算图的基础上进行拓展来保存partial adjoint和adjonitzhi之间的计算关系。\n如下图所示，黑色部分是原表达式的计算图，红色部分是将adjoint和partial adjount的计算图：\n使用计算图，除了能够节省内存外，还能清楚的看到正向计算的中间结果和反向计算之间的依赖关系，进而优化计算。\n反向模式ad和反向传播的区别 反向传播：\n在反向计算过程中使用与前向传播完全相同的计算图 应用于第一代深度学习框架 反向AD：\n为adjoint在计算图中创建独立的节点 被应用于现代深度学习框架 现代普遍应用反向AD的原因：\n某些损失函数是关于梯度的函数，这种情况下需要计算梯度的梯度，但反向传播就不能计算此类情况，而在反向AD中只要增加一个节点后在此计算梯度即可； 反向AD优化空间更大。 考虑Tensor的反向模式AD 前面都是在假设中间变量是标量的基础上讨论的，接下来我们将其推广到Tensor上。\n首先推广adjoint，定义对于一个TensorZZ，其adjointZ‾Z为：\n=[∂y∂Z1,1…∂y∂Z1,n………∂y∂Zm,1…∂y∂Zm,n]=​∂Z1,1​∂y​…∂Zm,1​∂y​​………​∂Z1,n​∂y​…∂Zm,n​∂y​​​\n鉴于\nZij=∑kXikWkjv=f(Z)Zij​v​=k∑​Xik​Wkj​=f(Z)​\n那么在计算Xi,k‾Xi,k​​时，需要将所有计算图上以Xi,kXi,k​为输入的节点都找出来，即ZZ的第ii行的每个元素。因此Xi,k‾Xi,k​​的计算公式为：\nXi,k‾=∑j∂Zi,j∂Xi,kZˉi,j=∑jWk,jZˉi,jXi,k​​=j∑​∂Xi,k​∂Zi,j​​Zˉi,j​=j∑​Wk,j​Zˉi,j​\n上述公式记为矩阵形式为：\nX‾=Z‾WTX=ZWT\nLecture 5: Automatic Differentiation Implementation 这讲主要介绍我们hw中要实现的needle的总体框架，项目中已给出了约1000行代码。\nautograd.py autograd保存与实现自动微分相关的代码。\nValue类对应计算图上的节点，其数据成员包括：\n1 2 3 4 5 6 7 8 9 10 class Value: \"\"\"A value in the computational graph.\"\"\" # trace of computational graph op: Optional[Op] inputs: List[\"Value\"] # The following fields are cached fields for # dynamic computation cached_data: NDArray requires_grad: bool op用于保存该节点的运算符，inputs保存该运算符的操作数，cached_data保存该节点的数值，其数据结构因平台不同而区别。\nops 本节主要介绍needle库的代码结构，笔记相当草率，建议看原视频。\nops文件夹（2023版本）或者op.py（2022）版本保存各种算子的实现。 Op类规定了两个必须要实现的接口：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 class Op: \"\"\"Operator definition.\"\"\" def compute(self, *args: Tuple[NDArray]): \"\"\"Calculate forward pass of operator. Parameters ---------- input: np.ndarray A list of input arrays to the function Returns ------- output: nd.array Array output of the operation \"\"\" raise NotImplementedError() def gradient( self, out_grad: \"Value\", node: \"Value\" ) -\u003e Union[\"Value\", Tuple[\"Value\"]]: \"\"\"Compute partial adjoint for each input value for a given output adjoint. Parameters ---------- out_grad: Value The adjoint wrt to the output value. node: Value The value node of forward evaluation. Returns ------- input_grads: Value or Tuple[Value] A list containing partial gradient adjoints to be propagated to each of the input node. \"\"\" raise NotImplementedError() compute接口用于描述该运算符实施的运算，gradient描述该运算符对应的梯度计算方式。\nLecture 6: Fully connected network, optimization, initialization 全连接网络 之前我们讨论的全连接网络都是不含偏执项的（为了方便进行手动微分），本章将介绍真正的MLP。其通过迭代的过程进行定义：\nzi+1=σi(WiTzi+bi), i=1,…,Lhθ(x)=zL+1z1=x​zi+1​=σi​(WiT​zi​+bi​), i=1,…,Lhθ​(x)=zL+1​z1​=x​\n上述模型中，可优化的参数集合为θ=W1:L,b1:Lθ=W1:L​,b1:L​。σi(x)σi​(x)是非线性的激活函数，特别的，最后一层没有激活函数，即σL(x)=xσL​(x)=x。\n迭代的表达式写成矩阵形式为：\nZi+1=σi(ZiWi+1biT)Zi+1​=σi​(Zi​Wi​+1biT​)​\n其中，11表示一个表示一个全1的列向量，用于将列向量biTbiT​广播到与矩阵ZiWiZi​Wi​相匹配的形状。\n在实际实现过程中，我们不用浪费空间去构造这样一个全1列向量，而是直接使用广播算子。在NumPy有许多自动的广播操作，但是在我们实现的needle库中，这一操作更加显式，例如对于(n×1)→(m×n)(n×1)→(m×n)，要执行的操作为A.reshape((1, n)).broadcast_to((m, n))。\n优化 对于有监督的深度学习任务，一般的优化目标为：\n$$θ f(θ)=1m∑i=1ml(hθ(x(i),y(i)))minimizeθ​ f(θ)=m1​i=1∑m​l(hθ​(x(i),y(i)))$$\n接下来将介绍几常用的优化算法。\n梯度下降 gradient desecent 梯度下降法之前几节课讲过了，这里直接给出其数学表达式： $$ θt+1=θt−α∇θf(θt)θt+1​=θt​−α∇θ​f(θt​)$$\n其中，tt表示迭代次数。\r学习率这一参数对于该方法格外重要，不同的学习率的表现相差很大很大：\n上图展示了大学习率和小学习率的迭代过程，如果目标函数再复杂一点，那么确定合适的学习率就会变得更加复杂。接下来将介绍一些不同的方法，它们各有其收敛行为。\n对于梯度下降法的改进，有两种方案：梯度计算的变种和随机的变种。首先介绍第一类。\n牛顿法 Newton’s Method 牛顿发使用二次曲面对一个高维函数做近似，因此其收敛速度显著快于一阶逼近的梯度下降法。其迭代公式为：\nθt+1=θt−α(∇θ2f(θt))−1∇θf(θt)θt+1​=θt​−α(∇θ2​f(θt​))−1∇θ​f(θt​)\n其中，(∇θ2f(θt))−1(∇θ2​f(θt​))−1是_Hessian_矩阵的逆矩阵。_Hessian_矩阵每个元素都是二阶导数，其具体定义为：\n$$∇θ2f(θt)=H=[∂2f∂x12∂2f∂x1∂x2⋯∂2f∂x1∂xn∂2f∂x2∂x1∂2f∂x22⋯∂2f∂x2∂xn⋱∂2f∂xn∂x1∂2f∂xn∂x2⋯∂2f∂xn2]∇θ2​f(θt​)=H=​∂x12​∂2f​∂x2​∂x1​∂2f​⋮∂xn​∂x1​∂2f​​∂x1​∂x2​∂2f​∂x22​∂2f​⋮∂xn​∂x2​∂2f​​⋯⋯⋱⋯​∂x1​∂xn​∂2f​∂x2​∂xn​∂2f​⋮∂xn2​∂$$2f​​​\n对于二次函数，牛顿法可以一次给出指向最优点的方向\n这一方法广泛用于传统凸优化领域，但是很少用于深度学习优化。有两个主要原因：1) Hessian矩阵是n×nn×n的，因此参数量稍微大一点其计算代码都非常非常恐怖；2) 对于非凸优化，二阶方法是否更有效还有待商榷。\n动量梯度下降法 Momentum 在普通梯度下降法中，如果学习率太大，就会出现来回横跳的情况，如果对前几次梯度取平均，则可能改善这一情况。 动量法正是对梯度取指数移动平均1的方案，具体来说有：\nut+1=βut+(1−β)∇θf(θt)θt+1=θt−αut+1​ut+1​=βut​+(1−β)∇θ​f(θt​)θt+1​=θt​−αut+1​​\n该方法可视化过程如下图所示，在较大学习率的情况下，其相比梯度下降法优化曲线更为平滑。\n无偏动量法 Unbiasing momentum 前一章节实际上有一个小瑕疵。如果u0u0​初始化为0，那么第一次进行更新是的梯度值是正常更新的(1−β)(1−β)倍，因此其前期的收敛过程会稍慢，随着迭代的进行，其效应会逐渐减弱。 为了修正其影响，我们可以在参数更新过程中对动量进行缩放，具体来说：\nθt+1=θt−αut+11−βt+1θt+1​=θt​−1−βt+1αut+1​​\n如下图所示，修正以后其前期的更新速度要快了不少。\nNesterov momentum Nesterov是梯度下降中一个非常有效的“trick”，其在传统momentum的基础上，将计算当前位置的梯度改为计算下一步位置的梯度。即：\nut+1=βut+(1−β)∇θf(θt−αut)ut+1​=βut​+(1−β)∇θ​f(θt​−αut​)\n关于其为啥有效，看到了两篇文章。第一篇2通过推导认为该方案对二阶导数进行了近似，因此其收敛速度更快；第二篇3认为其能够更好地感知未来位置的梯度，在未来梯度很大时放慢步子。\n不看广告看疗效，对比普通Momentum，该方法的收敛速度要快得多。据说其也更适合一个深度网络。 Adam Adam是一种自适应的梯度下降算法。不同参数其对应的梯度之间的大小差异可能很大，Adam对此的解决方案是提供一个缩放因子，梯度值小则将其缩放得大一点，即：\nut+1=β1ut+(1−β1)∇θf(θt)vt+1=β2vt+(1−β2)(∇θf(θt))2平方为逐元素运算θt+1=θt−αut+1vt+1+ϵ所有元素均为逐元素运算​ut+1​=β1​ut​+(1−β1​)∇θ​f(θt​)vt+1​=β2​vt​+(1−β2​)(∇θ​f(θt​))2θt+1​=θt​−vt+1​​+ϵαut+1​​​平方为逐元素运算所有元素均为逐元素运算​\nAdam在实践中得到了广泛应用，在特定任务上，其可能不是最佳的优化器（如下图），但在大部分任务上，其都能有不错的可以作为基线的表现。 接下来将介绍随机变种。随机变种是在优化过程中加入了随机变量（噪声），例如每次使用数据集的一个子集对参数进行更新。\n随机梯度下降 Stochastic gradient descent 随机梯度下降正是每次使用数据集的一个子集对参数进行更新，即：\nθt+1=θt−α∣B∣∑i∈B∇θl(hθ(x(i),yi))θt+1​=θt​−∣B∣α​i∈B∑​∇θ​l(hθ​(x(i),yi))\n看上去SGD的迭代次数比梯度下降要多得多，但是其每轮迭代的计算代价都要小的多，同时 尽管在凸优化上可视化训练过程给了很直观的感受，但需要注意的是，深度学习并不是凸优化或者二次函数，这些优化方法在深度学习上的应用与在凸优化上的效果可能完全不同。\n初始化 参数的初始值如何确定？这是个好问题。\n在凸优化中，尝尝将所有参数初始化为0，如果在神经网络中也这么做，那么每一层的输出都是0，求得的梯度也都是0🙁。全0是这个模型的一个不动点，模型将永远得不到更新。\n初始化参数对梯度的影响很大 一种自然的想法是对参数进行随机初始化，例如按照多元正态分布进行初始化。但是，分布中参数的选择对于梯度的影响可能会相当大，如下图所示：随着层数的增加，如果激活值范数变化的太剧烈，会导致梯度爆炸或者消失问题，如果梯度值过大或者过小，也会导致这些问题。\n权重的在训练过程的变化可能很小 可能存在这样一个误区：无论初始值如何选择，这些参数最终都会收敛到某个区域附近。事实并非如此，整个训练过程中权重的变化并非如此剧烈。\n为什么2/n在前面是个合适的初始化参数 这里直接使用gpt对这页ppt的解释\n考虑独立的随机变量 𝑥∼𝑁(0,1)x∼N(0,1) 和 𝑤∼𝑁(0,1𝑛)w∼N(0,n1​)，其中 𝑥x 是输入，𝑤w 是权重。\n期望和方差 𝐸[𝑥⋅𝑤𝑖]=0E[x⋅wi​]=0 Var[𝑥⋅𝑤𝑖]=1𝑛Var[x⋅wi​]=n1​ 因此，对于 𝑤𝑇𝑥wTx：\n𝐸[𝑤𝑇𝑥]=0E[wTx]=0 Var[𝑤𝑇𝑥]=1Var[wTx]=1（根据中心极限定理，𝑤𝑇𝑥wTx 服从 𝑁(0,1)N(0,1)） 激活值的方差 如果使用线性激活函数，并且 𝑧𝑖∼𝑁(0,𝐼)zi​∼N(0,I)，则 𝑊𝑖∼𝑁(0,1𝑛𝐼)Wi​∼N(0,n1​I)，那么：\n𝑧𝑖+1=𝑊𝑖𝑧𝑖zi+1​=Wi​zi​\nReLU 非线性 如果使用 ReLU 非线性激活函数，由于 ReLU 会将一半的 𝑧𝑖zi​ 分量设为零，因此为了达到相同的最终方差，需要将 𝑊𝑖Wi​ 的方差增加一倍。因此：\n𝑊𝑖∼𝑁(0,2𝑛𝐼)Wi​∼N(0,n2​I)\n这就是所谓的 Kaiming 正态初始化（He 初始化），它特别适用于 ReLU 激活函数。\nLecture 7: Neural Network Library Abstractions 这节课主要介绍如何使用我们的needle库来实现一些简单的深度学习模型，构造一些小组件。\n程序抽象 现代成熟的深度学习库提供了一些API，站在今天的视角，这些API都是都是恰到好处的。通过思考为什么要这样设计接口，可以让我们更好地理解深度学习库在进行程序抽象时的内部逻辑。\n首先几个经典的深度学习框架进行分析，包括Caffe、TensorFlow和PyTorch。\nCaffe 1.0 （2014） 在Caffe中，使用Layer这一概念来表示神经网络中的一个个小模块，通过拼接和替换Layer，可以实现快速构造和修改神经网络，并使用同一套代码进行训练。 Layer类提供了forward和backward两个接口：\n1 2 3 4 5 6 class Layer: def forward(bottom, top): pass def backward(top, propagate_down, bottom): pass forward负责将来自bottom的数据进行前向传播，然后将数据保存到top中。在backward接口中，top保存来自输出的梯度，propagate_down用以指示是否要对其求梯度，bottom用于存放梯度。\n在Caffe中，计算梯度是“就地”完成的，而非在计算图上新增额外的节点。作为第一代深度学习框架，直接计算梯度的思想是朴素但是符合直觉的。\nTensorFlow 1.0 （2015） 作为第二代深度学习框架，其在引入了计算图的概念。在计算图中，只要定义前向计算的计算方式，当需要计算梯度时，直接对计算图进行拓展即可。一个简短实例为： 1 2 3 4 5 6 7 8 9 import tensorflow as tf v1 = tf.Variable() v2 = tf.exp(v1) v3 = v2 + 1 v4 = v2 * v3 sess = tf.Session() value4 = sess.run(v4, feed_dict = {v1: numpy.array([1])}) 以上代码v1~4仅仅是占位符，用于构建计算图，在没有输入传入前并没有值。通过会话来获取某个输入的情况下输出的值。\n上述过程被称为声明式编程。即计算图在定义时并不会立即执行，而是等到会话（session）运行时才执行。这种方式的优点有：代码分区，可读性高；运行前计算图已知，可以针对性优化；通过会话便于实现分布式计算\nPyTorch (needle) PyTorch使用的是命令式编程，相比声明式编程，命令式编程在构建计算图时就已经指定其值。 1 2 3 4 5 6 import needle as ndl v1 = ndl.Tensor([1]) v2 = ndl.exp(v1) v3 = v2 + 1 v4 = v2 * v3 命令式编程可以很方便地与Python原生控制流语句结合在一起，例如：\n1 2 3 4 if v4.numpy() \u003e 0.5: v5 = v4 * 2 else: v5 = v4 tf1.0的效率更高，适合推理和部署。PyTorch1.0则更适合开发和debug。\n高级模块化库组件 如何使用深度学习库来实现深度学习呢？在hw1中我们使用一个个底层算子来搭建模型和实现训练过程，但这样开发太低效了。深度学习本身是很模块化的：由模型、损失函数和优化方法三部分组成。不但如此，模型本身也是高度模块化的。因此，我们在实现深度学习库时，必须精心设计好接口，以便支持该模块化的特性。\n在PyTorch中，有一类叫做nn.Module，对应的就是模型中一个个小的子模块，其特点是以Tensor同时作为输入和输出。损失函数也满足这一特性，其可以被视为一个模块。\n对于优化器，其作用是输入一个模型，对该模型中的参数按照某一规则进行更新。\n为了防止过拟合，有些模型还具有正则项，其有两种实现方式：\n作为损失函数的一部分进行实现 直接整合进优化器中 参数初始化同样很重要，其一般在构建nn.Module中指定。\n数据加载也是一个很重要的模块。数据加载中还经常对数据进行预处理和增强。\n各组件之间数据流图如下所示：\nLecture 8: Neural Network Implementation 修改Tensor的data域 在实现SGD时，由于存在多个batch，可能会在一个循环里对待学习参数进行更新，即：\n1 2 for _ in range(iterations): w -= lr * grad 正如在CMU 10-414 Assignments 实验笔记 \u003e SGD for a two-layer neural network踩过的坑那样，直接使用Tensor之间的算子进行参数更新会导致每次更新都会在计算图上增加一个新的节点w，这个节点具有Op和inputs，严重拖累反向传播速度。\n为了避免每次更新参数时都在计算图上留下一个需要求梯度的节点，needle库提供了Tensor.data()方法，用于创建一个与Tensor共享同一个底层data的节点，但其不存在Op和inputs，也不用对其进行求导。\n因此，可以使用Tensor.data方法，在不干扰计算图反向传播的前提下对参数进行正常的更新，即：\n1 w.data -= lr * grad.data 数值稳定性 每个数值在内存中的存储空间都是有限的，因此保存的数值的范围和精度都是有限的，计算过程中难免出现溢出或者精度丢失的情况，在实现算子时，必须考虑到数值稳定性的问题。\n例如，在softmax公式中，由于指数运算的存在，数值很有可能就上溢了，一个修正方式是在进行softmax运算前，每个元素都减去输入的最大值，以防止上溢。即：\nzi=softmax(xi)=exp⁡(xi−c)∑kexp⁡(xk−c)zi​=softmax(xi​)=∑k​exp(xk​−c)exp(xi​−c)​\n其中，c=max⁡(x)c=max(x)。\n类似的，其它算子也要考虑相应的稳定性问题。\nParameter 类 Parameter类用于表示可学习的参数，其是Tensor的子类。相比Tensor类，这个类不必再引入新的行为或者接口，因此其实现很简单：\n1 2 class Parameter(ndl.Tensor): \"\"\"parameter\"\"\" Module 类 Module类用于表示神经网络中一个个子模块。其具有如下接口：\nparameters：获取模块中所有可学习参数 __call__：进行前向传播 在实现时，定义了一个辅助函数_get_params用于提取一个模块中的所有可学习参数。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 def _get_params(value): if isinstance(value, Parameter): return [value] if isinstance(value, dict): params = [] for k, v in value.items(): params += _get_params(v) return params if isinstance(value, Module): return value.parameters() return [] class Module: def parameters(self): return _get_params(self.__dict__) def __call__(self, *args, **kwargs): return self.forward(*args, **kwargs) Optimizer 类 Optimizer类用于优化模型中可学习参数，其有两个关键接口：\nreset_grad：重置模型中可学习参数的grad字段 step：更新参数值 reset_grad实现比较简单，step方法则依赖于优化算法的具体实现： 1 2 3 4 5 6 7 8 9 10 class Optimizer: def __init__(self, params): self.params = params def reset_grad(self): for p in self.params: p.grad = None def step(self): raise NotImplemented() Lecture 9: Normalization and Regularization Normalization 在前面几讲提到过，参数初始值的选择对于模型的训练很重要，不恰当的初始值参数会导致梯度消失或者爆炸💥。更重要的是，当训练完成后，这些梯度和参数值大小仍有初始值差不多，这更强调了初始值的重要性。\n为了修复这一问题，引入了layer normalization。其思想就是对激活层的输出进行标准化，即将输出减去期望后除以标准差：\n$$z^i+1=σi(WiTzi+bi)zi+1=z^i+1−E(z^i+1)Var(z^i+1)+ϵ$$\n上述技巧目前已经得到广泛应用，但在实践中，应用layer norm会导致模型难以收敛到一个很小的loss值。\n另外一种技巧是batch norm。layer norm是对每一个sample（z的每一行）做归一化，而batch norm对每一列归一化。这一方法使得每个batch的所有样本都会对该batch中某个样本的推理结果有影响，因此在进行推理时，batch norm中的归一化的参数应该使用整个训练集上的参数，而非推理时输入样本的batch参数。\nRegularization 正则化用于对抗过拟合，所谓过拟合是指模型在训练集上性能非常好，但在测试机上泛化性能很差。正则化就是限制参数复杂度的过程，可以分为显式正则和隐式正则。\n隐式正则化是指现有算法或架构在不显式添加正则化项的情况下，自然地对函数类进行限制。具体来说，隐式正则化通过以下方式实现：\n算法的固有特性：例如，随机梯度下降（SGD）等优化算法在训练过程中自带某些正则化效果。虽然我们并没有显式地优化所有可能的神经网络，而是通过SGD优化那些在特定权重初始化下的神经网络。这种优化过程本身对模型的复杂度进行了限制。 架构的设计：某些网络架构设计本身就具有正则化效果。例如，卷积神经网络（CNN）的共享权重机制和局部连接特性，自然地减少了模型参数的数量，从而降低了模型复杂度。 显式正则化指的是通过显式得修改模型使其能够避免对训练集过拟合。\n一种最常见的应用于参数的正则化方案是l2正则化，即l2 regularization a.k.a weight decay。传统认为，模型参数值的大小可以在一定程度上指示出模型的复杂度，因此通过在优化目标中引入l2正则项来控制模型的大小。一般地，引入l2 regularization的机器学习优化问题可以表示为：\n$$minimize1m∑iml(hw1:L(x(i),y(i)))+λ2∑i=1L∣∣wi∣∣F2$$\n其中，∣∣wi∣∣F∣∣wi​∣∣F​是Frobenius范数，其表示矩阵每个元素的平方和的平方根。\n得益于这里的系数是1/21/2，在对wiwi​求导时正则项恰好为λwiλwi​。梯度更新的公式相应变为：\nWi:=(1−αλ)Wi−α∇1mlWi​:=(1−αλ)Wi​−α∇m1​l\n注意，引入l2正则化后，每轮迭代都会将参数缩小至原来的1−αλ1−αλ。很多地方不将l2正则化作为损失函数的一部分，而是将其作为优化器的一部分，即直接将参数进行缩小，这种方法被称为weight decay，显然二者是等价的。\n另外一种正则化方法是dropout，其思想是在训练过程中随机地将一些激活层的输出置为0，并对其它输出放大，以确保整层输出的数学期望不变，形式化表示为：\nz^i+1=σi(WiTzi)+bi(zi+1)j={((z^i+1)j)/(1−p)以概率1−p0以概率pz^i+1​(zi+1​)j​​=σi​(WiT​zi​)+bi​={((z^i+1​)j​)/(1−p)0​以概率1−p以概率p​​\n在推理时，则不需要进行dropout。\n直观地说，dropout能够提升模型在激活层部分缺失时进行推理的能力，但显然这一能力没什么卵用。另一种解释是dropout提升了模型训练过程中的随机性，类似SGD。\nLecture 10: Convolutional Networks Convolutional operators in deep networks 在hw2中，我们通过flatten操作将图片视作一个序列进行计算，这对于小尺寸的图片是可行的，但对于大尺寸的图片，例如256×256的图片，将会导致输入异常庞大，网络也随之变大。这种简单粗暴的处理方式不利于提取图片的内在特征，例如，如果对图片进行平移，其输入序列的变化相当大。\n卷积网络出于以下两个动机：\n层之间的激活以局部的方式发生，并且隐藏层的输出也被视为图像 在所有的空间位置共享权重 卷积网络有以下两个优点：\n使用的参数很少。参数量由卷积网络的大小决定，而和输入的shape无关； 能够很好地捕获图片的内在不变形。 卷积的计算示意如下图所示，卷积核在原图上滑动，从而产生一张新的图片。\n在深度学习中，输入和隐藏层都很少是一个1D的矩阵，一般而言，其是由多个通道的。例如，一张彩色图片由RGB三通道组成，而中间的隐藏层，通常会有比较大的通道数，如下图所示：\n记卷积层的输入x∈Rh×w×cinx∈Rh×w×cin​，输出z∈Rh×w×coutz∈Rh×w×cout​。从上图可以发现，卷积输出的某个通道，都是由输入在同一个局部的所有通道共同决定的，因此，卷积核W∈Rcin×cout×k×kW∈Rcin​×cout​×k×k，卷积过程可以形式化表示为：\n$$z[:,:,s]=∑r=1cinx[:,:,r]⋅W[r,s,:,:]$$\n关于多通道卷积，另外一种更符合直觉的理解是将相同位置的各通道的组合看作是一个向量，即下图中，xx每一格都是一个向量，WW每一格都是cout×cincout​×cin​的矩阵，卷积的输出由对应位置的zz和WW按矩阵乘法并求和得到。\nElements of practical convolutions 在实际的卷积操作中，通常还会应用一些别的技术。\nPadding 原始的卷积操作，会将输出的长宽变小k−1k−1个长度，通过在周围填充(k−1)/2(k−1)/2个0元可以保证输出的shape与输入一致。为了避免两侧填充不一致这个别扭的情况，我们一般选取卷积核大小为奇数。\nStrided Convolutions / Pooling 经过padding之后的卷积操作，不改变图片的shape，但在实际应用中，通常会对图片进行下采样。用两种解决方案：\n使用最大/平均池化来聚合信息，例如，使用一个2×2的核进行池化操作，每次移动的步长为2，就可以将整张图片长宽各放缩至原来一半； 卷积操作时，卷积核移动的步长大于1。 Grouped Convolutions 当输入和输出的通道数很大时，卷积核的参数量仍可能非常非常大。一种解决方案是，使用分组卷积，即将输入通道分为多个组，每个组独立进行卷积操作，如下图所示。如果分为G组，则参数量可减少为原来的1/G。 Dilations 传统卷积的感受野和卷积核一样大，扩张卷积的思路是在卷积区域中插入间隔，能够扩大卷积核的感受野。下图表示的很形象。 Differentiating convolutions 正如前文所提到的，我们可以通过一系列矩阵向量乘法和求和运算来实现卷积操作，但这么做效率太低了，我们的计算图上有很多中间节点，这些中间变量将消耗大量的内存空间。因此，我们不应该使用微分库中的算子来计算卷子，而是将其作为一个算子来实现，并手动计算其微分。\n首先定义卷积操作：\nz=conv⁡(x,W)z=conv(x,W)\nzz的梯度怎么与adjoints乘呢？这是个问题。zz的梯度有以下二者：∂z∂x∂x∂z​和∂z∂W∂W∂z​，从形式上看，他们是3阶张量初以四阶张量，相当复杂。\n首先考虑最简单的矩阵和向量相乘的情况，即：\nz=Wxz=Wx\n那么zz对xx的导数就是WW，即其与adjoint的乘法计算公式为：\nWTvˉWTvˉ\n也就是说如果在前向传播中我们计算一个矩阵和向量的乘积，那么在反向传播中，我们要计算这个矩阵的转置和adjoint的乘积。那对于卷积来说，它的“转置”是什么呢？\n将卷积视为矩阵运算I 以1d卷积为例，我们考虑如下的一个卷积运算，其中每个格子都是一个向量或者矩阵。 将上面这个矩阵运算展开，可以得到：\n[z1z2z3z4z5]=x∗w=[w2w3000w1w2w3000w1w2w3000w1w2w3000w1w2][x1x2x3x4x5]​z1​z2​z3​z4​z5​​​=x∗w=​w2​w1​000​w3​w2​w1​00​0w3​w2​w1​0​00w3​w2​w1​​000w3​w2​​​​x1​x2​x3​x4​x5​​​\n有了W^W^，我们可以很容易地写出W^TW^T,即：\nW^T=[w2w1000w3w2w1000w3w2w1000w3w2w1000w3w2]W^T=​w2​w3​000​w1​w2​w3​00​0w1​w2​w3​0​00w1​w2​w3​​000w1​w2​​​\n不难发现，这个算子实际上是[w3,w2,w1][w3​,w2​,w1​]这个卷积核，即原始卷积核翻转后的卷积核。也就是说，梯度和adjoint的乘积可以表示为：\nv^∂conv⁡(x,w)∂x=conv⁡(v^,flip⁡(w))v^∂x∂conv(x,w)​=conv(v^,flip(w))\n将卷积视为矩阵运算II 接下来我们考虑卷积对于参数ww的导数。同样，我们将矩阵运算展开，可以得到：\n[z1z2z3z4z5]=x∗w=[0x1x2x1x2x3x2x3x4x3x4x5x4x50][w1w2w3]​z1​z2​z3​z4​z5​​​=x∗w=​0x1​x2​x3​x4​​x1​x2​x3​x4​x5​​x2​x3​x4​x5​0​​​w1​w2​w3​​​\n相比矩阵运算I，我们构造出的X^X^矩阵是一个密集矩阵，在实现卷积算子时，我们常常采用这个方案来运算。这个X^X^矩阵被称为“im2col”矩阵（image to column）。\nLecture 11: Hardware acceleration General acceleration techniques 现代机器学习框架可以视为两层：上层是计算图，用于前向推理、自动微分和反向传播；下层是张量线性代数库，其负责底层的张量计算。在needle中，我们目前使用numpy作为线性代数库。本节我们将介绍一些常见的加速技术。\nVectorization 向量化 如果我们要将两个256长度的array相加，一种标量的处理方式是256个元素逐个相加，但是很多硬件都提供了批量从内存读取、向量运算指令，即优化为如下代码： 1 2 3 4 5 6 7 8 void vecadd(float* A, float* B, float* C){ for(int i=0; i\u003c64; i++){ float4 a = load_float4(A + i*4); float4 b = load_float4(B + i*4); float4 c = add_float4(a, b); store_float4(C + i*4, c); } } 这里要求ABC所在的内存块要是按照128 bit对齐的。\nData layout \u0026 strides 数据布局\u0026步幅 在内存中，数据是线性排列的，因此一个矩阵在内存中有两种布局方式：行优先和列优先。一些古老的语言使用列优先，现代的语言偏向使用行优先。 在许多库中，还引入了一种stride格式布局，即在保存张量时，额外保存一个数据，用于标识每个维度上需要移动的步长。在这种情况下，a[i, j] = a_data[i * strides[0] + j * strides[1]]\n这个方案可以在不用复制数据的情况下实现很多操作：通过改变offset和shape来实现切片；通过交换strides来实现转置；通过插入等于0的stride来实现广播。\n其缺点是访存操作可能不再连续，因此向量化技术不可用，很多库也需要先把他们拼接之后再使用。\nParallelization 并行化 使用openmp可以将计算分配给多个核并行处理： 1 2 3 4 5 6 7 8 9 void vecadd(float* A, float* B, float* C){ #pragma omp parallel for for(int i=0; i\u003c64; i++){ float4 a = load_float4(A + i*4); float4 b = load_float4(B + i*4); float4 c = add_float4(a, b); store_float4(C + i*4, c); } } Case study: matrix multiplication 本节我们将讨论如何优化矩阵乘法。\nVanilla matrix multiplication 朴素矩阵乘法 最朴素的想法是使用三重循环完成，其复杂度是O(n3)O(n3)，即如下代码： 1 2 3 4 5 6 7 8 9 10 float A[n][n], B[n][n], C[n][n]; for(int i=0; i\u003cn; i++){ for(int j=0; j\u003cn; j++){ c[i][j] = 0; for(int k=0; k\u003cn; k++){ c[i][j] += A[i][k] * B[k][j]; } } } 在现代存储器中，L1 cache的速度比DRAM快200倍，通过优化数据的读取就可以显著提升计算速度，考虑到这一点，我们可以将中间变量保存到寄存器中，即：\n1 2 3 4 5 6 7 8 9 10 11 12 13 dram float A[n][n], B[n][n], C[n][n]; for(int i=0; i\u003cn; i++){ for(int j=0; j\u003cn; j++){ register float c = 0; for(int k=0; k\u003cn; k++){ register float a = A[i][k]; register float b = B[k][j]; c += a*b; } C[i][j] = c; } } 上述代码中，从读取A、B到寄存器的操作分别进行了n3n3次，需要3个寄存器来完成该操作。\nRegister tiled matrix multiplication 寄存器分块矩阵乘法 该方案的思路是将结果进行分块，每次计算其中的一块，即： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 dram float A[n/v1][n/v3][v1][v3]; dram float B[n/v2][n/v3][v2][v3]; dram float C[n/v1][n/v2][v1][v2]; for (int i = 0; i \u003c n/v1; ++i) { for (int j = 0; j \u003c n/v2; ++j) { register float c[v1][v2] = 0; for (int k = 0; k \u003c n/v3; ++k) { register float a[v1][v3] = A[i][k]; register float b[v2][v3] = B[j][k]; c += dot(a, b.T); } C[i][j] = c; } } 上述代码中，要计算的矩阵C被分为v1×v2v1​×v2​的小矩阵，为了计算出每一块，每次必须从A中选出v1v1​行，从B中选出v2v2​列，这两组子矩阵可以按照长度v3v3​再次划分。在计算中，前两个循环依次遍历C中的一小块，然后初始化v1×v2v1​×v2​个寄存器用于保存该块内容，然后再根据v3v3​的大小二次划分，进行矩阵运算，将这些结果加到对应的寄存器上，第三个循环结束后就计算出C的一个子块。\nA的数据加载开销是n3/v2n3/v2​，B的数据加载开销是n3/v1n3/v1​，A的寄存器开销是v1×v3v1​×v3​，B的寄存器开销是v2×v3v2​×v3​，C的寄存器开销是v1×v2v1​×v2​。注意到v3v3​不影响数据加载的开销，因此可以取v3v3​为1，然后在满足寄存器总数约束的情况下，最大化v1v1​和v2v2​。\n之所以能够减小开销是因为在矩阵计算中，元素被重复使用，通过每次计算一个分块的方式，可以保证这个分块内用到的重复数据只要加载一次。\nCache line aware tiling 缓存行感知分块 前面我们使用寄存器来进行加速，本节我们考虑使用cache来加速。我们的实现代码为： 1 2 3 4 5 6 7 8 9 10 11 12 dram float A[n/b1][b1][n]; dram float B[n/b2][b2][n]; dram float C[n/b1][n/b2][b1][b2]; for (int i = 0; i \u003c n/b1; ++i) { l1cache float a[b1][n] = A[i]; for (int j = 0; j \u003c n/b2; ++j) { l1cache float b[b2][n] = B[j]; C[i][j] = dot(a, b.T); } } 上述代码中，结果矩阵C被分块为b1×b2b1​×b2​，A和B分别按行和按列分块，通过两层循环遍历计算C中的每个子块，计算子块的过程可以使用寄存器分块进行加速。\n上述代码中，A的加载开销是n2n2，B的加载开销是n3/b1n3/b1。有两个约束，一个是b1n+b2n","wordCount":"3670","inLanguage":"en","image":"http://localhost:13448/%3Cimage%20path/url%3E","datePublished":"2025-01-23T11:30:03Z","dateModified":"2025-01-23T11:30:03Z","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:13448/post/cmu10-414-lecture/"},"publisher":{"@type":"Organization","name":"Shihong Yuan's Course Note","logo":{"@type":"ImageObject","url":"http://localhost:13448/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:13448/ accesskey=h title="Shihong Yuan's CourseNotes (Alt + H)"><img src=http://localhost:13448/apple-touch-icon.png alt aria-label=logo height=35>Shihong Yuan's CourseNotes</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:13448/archive/ title=📦日期归档><span>📦日期归档</span></a></li><li><a href=http://localhost:13448/tags title=🏷️标签><span>🏷️标签</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:13448/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:13448/post/>Posts</a></div><h1 class="post-title entry-hint-parent">CMU10-414 lecture</h1><div class=post-description>Desc Text.</div><div class=post-meta><span title='2025-01-23 11:30:03 +0000 +0000'>January 23, 2025</span>&nbsp;·&nbsp;18 min&nbsp;·&nbsp;3670 words&nbsp;·&nbsp;Me&nbsp;|&nbsp;<a href=https://github.com/%3cpath_to_repo%3e/content/post/cmu10-414-lecture.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#cmu-10-414-deep-learning-system%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0 aria-label="《CMU 10-414 deep learning system》学习笔记">《CMU 10-414 deep learning system》学习笔记</a></li><li><a href=#%e5%86%99%e5%9c%a8%e6%9c%80%e5%89%8d%e9%9d%a2 aria-label=写在最前面>写在最前面</a></li><li><a href=#lecture-1-introduction-and-logistics aria-label="Lecture 1: Introduction and Logistics">Lecture 1: Introduction and Logistics</a><ul><li><a href=#%e8%af%be%e7%a8%8b%e7%9a%84%e7%9b%ae%e6%a0%87 aria-label=课程的目标>课程的目标</a></li><li><a href=#%e4%b8%ba%e4%bb%80%e4%b9%88%e5%ad%a6%e4%b9%a0%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e7%b3%bb%e7%bb%9f aria-label=为什么学习深度学习系统？>为什么学习深度学习系统？</a></li><li><a href=#%e9%a2%84%e5%a4%87%e7%9f%a5%e8%af%86 aria-label=预备知识>预备知识</a></li></ul></li><li><a href=#lecture-2-ml-refresher--softmax-regression aria-label="Lecture 2: ML Refresher & Softmax Regression">Lecture 2: ML Refresher & Softmax Regression</a><ul><li><a href=#%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e5%9f%ba%e7%a1%80 aria-label=机器学习基础>机器学习基础</a></li><li><a href=#softmax-%e5%9b%9e%e5%bd%92 aria-label="Softmax 回归">Softmax 回归</a></li></ul></li><li><a href=#lecture-3-manual-neural-networks aria-label="Lecture 3: Manual Neural Networks">Lecture 3: Manual Neural Networks</a><ul><li><a href=#%e4%bb%8e%e7%ba%bf%e6%80%a7%e6%a8%a1%e5%9e%8b%e8%bd%ac%e5%8f%98%e4%b8%ba%e9%9d%9e%e7%ba%bf%e6%80%a7%e6%a8%a1%e5%9e%8b aria-label=从线性模型转变为非线性模型>从线性模型转变为非线性模型</a></li><li><a href=#%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c aria-label=神经网络>神经网络</a></li><li><a href=#%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%e6%a2%af%e5%ba%a6%e8%ae%a1%e7%ae%97 aria-label=反向传播（梯度计算）>反向传播（梯度计算）</a></li></ul></li><li><a href=#lecture-4-automatic-differentiation aria-label="Lecture 4: Automatic Differentiation">Lecture 4: Automatic Differentiation</a><ul><li><a href=#%e5%9f%ba%e6%9c%ac%e5%b7%a5%e5%85%b7 aria-label=基本工具>基本工具</a></li><li><a href=#%e5%af%b9%e8%87%aa%e5%8a%a8%e5%be%ae%e5%88%86%e6%96%b9%e6%b3%95%e7%9a%84%e7%ae%80%e5%8d%95%e4%bb%8b%e7%bb%8d aria-label=对自动微分方法的简单介绍>对自动微分方法的简单介绍</a></li><li><a href=#%e5%8f%8d%e5%90%91%e6%a8%a1%e5%bc%8f%e5%be%ae%e5%88%86%e7%ae%97%e6%b3%95%e7%9a%84%e5%ae%9e%e7%8e%b0 aria-label=反向模式微分算法的实现>反向模式微分算法的实现</a></li><li><a href=#%e5%8f%8d%e5%90%91%e6%a8%a1%e5%bc%8fad%e5%92%8c%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad%e7%9a%84%e5%8c%ba%e5%88%ab aria-label=反向模式ad和反向传播的区别>反向模式ad和反向传播的区别</a></li><li><a href=#%e8%80%83%e8%99%91tensor%e7%9a%84%e5%8f%8d%e5%90%91%e6%a8%a1%e5%bc%8fad aria-label=考虑Tensor的反向模式AD>考虑Tensor的反向模式AD</a></li></ul></li><li><a href=#lecture-5-automatic-differentiation-implementation aria-label="Lecture 5: Automatic Differentiation Implementation">Lecture 5: Automatic Differentiation Implementation</a><ul><li><a href=#autogradpy aria-label=autograd.py>autograd.py</a></li><li><a href=#ops aria-label=ops>ops</a></li></ul></li><li><a href=#lecture-6-fully-connected-network-optimization-initialization aria-label="Lecture 6: Fully connected network, optimization, initialization">Lecture 6: Fully connected network, optimization, initialization</a><ul><li><a href=#%e5%85%a8%e8%bf%9e%e6%8e%a5%e7%bd%91%e7%bb%9c aria-label=全连接网络>全连接网络</a></li><li><a href=#%e4%bc%98%e5%8c%96 aria-label=优化>优化</a></li><li><a href=#%e5%88%9d%e5%a7%8b%e5%8c%96 aria-label=初始化>初始化</a><ul><ul><li><a href=#%e6%9c%9f%e6%9c%9b%e5%92%8c%e6%96%b9%e5%b7%ae aria-label=期望和方差>期望和方差</a></li></ul><li><a href=#%e6%bf%80%e6%b4%bb%e5%80%bc%e7%9a%84%e6%96%b9%e5%b7%ae aria-label=激活值的方差>激活值的方差</a></li><li><a href=#relu-%e9%9d%9e%e7%ba%bf%e6%80%a7 aria-label="ReLU 非线性">ReLU 非线性</a></li></ul></li></ul></li><li><a href=#lecture-7-neural-network-library-abstractions aria-label="Lecture 7: Neural Network Library Abstractions">Lecture 7: Neural Network Library Abstractions</a><ul><li><a href=#%e7%a8%8b%e5%ba%8f%e6%8a%bd%e8%b1%a1 aria-label=程序抽象>程序抽象</a></li><li><a href=#%e9%ab%98%e7%ba%a7%e6%a8%a1%e5%9d%97%e5%8c%96%e5%ba%93%e7%bb%84%e4%bb%b6 aria-label=高级模块化库组件>高级模块化库组件</a></li></ul></li><li><a href=#lecture-8-neural-network-implementation aria-label="Lecture 8: Neural Network Implementation">Lecture 8: Neural Network Implementation</a><ul><li><a href=#%e4%bf%ae%e6%94%b9tensor%e7%9a%84data%e5%9f%9f aria-label=修改Tensor的data域>修改Tensor的data域</a></li><li><a href=#%e6%95%b0%e5%80%bc%e7%a8%b3%e5%ae%9a%e6%80%a7 aria-label=数值稳定性>数值稳定性</a></li><li><a href=#parameter-%e7%b1%bb aria-label="Parameter 类">Parameter 类</a></li><li><a href=#module-%e7%b1%bb aria-label="Module 类">Module 类</a><ul><li><a href=#optimizer-%e7%b1%bb aria-label="Optimizer 类">Optimizer 类</a></li></ul></li></ul></li><li><a href=#lecture-9-normalization-and-regularization aria-label="Lecture 9: Normalization and Regularization">Lecture 9: Normalization and Regularization</a><ul><li><a href=#normalization aria-label=Normalization>Normalization</a></li><li><a href=#regularization aria-label=Regularization>Regularization</a></li></ul></li><li><a href=#lecture-10-convolutional-networks aria-label="Lecture 10: Convolutional Networks">Lecture 10: Convolutional Networks</a><ul><li><a href=#convolutional-operators-in-deep-networks aria-label="Convolutional operators in deep networks">Convolutional operators in deep networks</a></li><li><a href=#elements-of-practical-convolutions aria-label="Elements of practical convolutions">Elements of practical convolutions</a></li><li><a href=#differentiating-convolutions aria-label="Differentiating convolutions">Differentiating convolutions</a></li></ul></li><li><a href=#lecture-11-hardware-acceleration aria-label="Lecture 11: Hardware acceleration">Lecture 11: Hardware acceleration</a><ul><li><a href=#general-acceleration-techniques aria-label="General acceleration techniques">General acceleration techniques</a></li><li><a href=#case-study-matrix-multiplication aria-label="Case study: matrix multiplication">Case study: matrix multiplication</a></li></ul></li><li><a href=#lecture-12-gpu-acceleration aria-label="Lecture 12: GPU acceleration">Lecture 12: GPU acceleration</a><ul><li><a href=#gpu-programming aria-label="GPU programming">GPU programming</a></li><li><a href=#case-study-matrix-multiplication-on-gpu aria-label="Case study: matrix multiplication on GPU">Case study: matrix multiplication on GPU</a></li></ul></li><li><a href=#lecture-13-hardware-acceleration-implemetation aria-label="Lecture 13: Hardware Acceleration Implemetation">Lecture 13: Hardware Acceleration Implemetation</a></li><li><a href=#lecture-14-implementing-convolutions aria-label="Lecture 14: Implementing Convolutions">Lecture 14: Implementing Convolutions</a><ul><li><a href=#%e5%ad%98%e5%82%a8%e6%a0%bc%e5%bc%8f-storage-order aria-label="存储格式 Storage Order">存储格式 Storage Order</a></li><li><a href=#for%e5%be%aa%e7%8e%af%e5%ae%9e%e7%8e%b0%e5%8d%b7%e7%a7%af-convolutions-with-simple-loops aria-label="for循环实现卷积 Convolutions with simple loops">for循环实现卷积 Convolutions with simple loops</a></li><li><a href=#%e7%9f%a9%e9%98%b5%e4%b9%98%e6%b3%95%e5%ae%9e%e7%8e%b0%e5%8d%b7%e7%a7%af-convolutions-as-matrix-multiplications aria-label="矩阵乘法实现卷积 Convolutions as matrix multiplications">矩阵乘法实现卷积 Convolutions as matrix multiplications</a></li><li><a href=#%e9%80%9a%e8%bf%87strides%e6%9d%a5%e6%93%8d%e4%bd%9c%e7%9f%a9%e9%98%b5-manipulating-matrices-via-strides aria-label="通过strides来操作矩阵 Manipulating matrices via strides">通过strides来操作矩阵 Manipulating matrices via strides</a></li><li><a href=#%e9%80%9a%e8%bf%87-im2col-%e6%9d%a5%e5%ae%9e%e7%8e%b0%e5%8d%b7%e7%a7%af-convolutions-via-im2col aria-label="通过 im2col 来实现卷积 Convolutions via im2col">通过 im2col 来实现卷积 Convolutions via im2col</a></li><li><a href=#%e9%80%9a%e8%bf%87-im2col-%e6%9d%a5%e5%ae%9e%e7%8e%b0%e5%a4%9a%e9%80%9a%e9%81%93%e5%8d%b7%e7%a7%af aria-label="通过 im2col 来实现多通道卷积">通过 im2col 来实现多通道卷积</a></li></ul></li><li><a href=#lecture-15-training-large-models aria-label="Lecture 15: Training Large Models">Lecture 15: Training Large Models</a><ul><li><a href=#%e5%86%85%e5%ad%98%e8%8a%82%e7%9c%81%e6%8a%80%e6%9c%af-techniques-for-memory-saving aria-label="内存节省技术 Techniques for memory saving">内存节省技术 Techniques for memory saving</a></li><li><a href=#%e5%b9%b6%e8%a1%8c%e5%92%8c%e5%88%86%e5%b8%83%e5%bc%8f%e8%ae%ad%e7%bb%83-parallel-and-distributed-training aria-label="并行和分布式训练 Parallel and distributed training">并行和分布式训练 Parallel and distributed training</a></li></ul></li><li><a href=#lecture-16-generative-adversarial-network aria-label="Lecture 16 Generative Adversarial Network">Lecture 16 Generative Adversarial Network</a><ul><li><a href=#%e7%94%9f%e6%88%90%e5%af%b9%e6%8a%97%e8%ae%ad%e7%bb%83-generative-adversarial-training aria-label="生成对抗训练 Generative adversarial training">生成对抗训练 Generative adversarial training</a></li><li><a href=#%e5%b0%86%e5%af%b9%e6%8a%97%e8%ae%ad%e7%bb%83%e4%bd%9c%e4%b8%ba%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e4%b8%ad%e7%9a%84%e4%b8%80%e4%b8%aa%e6%a8%a1%e5%9d%97-adversarial-training-as-a-module-in-deep-learning-models aria-label="将对抗训练作为深度学习中的一个模块 Adversarial training as a module in deep learning models">将对抗训练作为深度学习中的一个模块 Adversarial training as a module in deep learning models</a></li></ul></li><li><a href=#lecture-17-generative-adversarial-networks-implementations aria-label="Lecture 17: Generative Adversarial Networks implementations">Lecture 17: Generative Adversarial Networks implementations</a></li><li><a href=#lecture-18-sequence-modeling-and-recurrent-networks aria-label="Lecture 18: Sequence Modeling and Recurrent Networks">Lecture 18: Sequence Modeling and Recurrent Networks</a><ul><li><a href=#%e5%ba%8f%e5%88%97%e5%bb%ba%e6%a8%a1-sequence-modeling aria-label="序列建模 Sequence modeling">序列建模 Sequence modeling</a></li><li><a href=#%e5%be%aa%e7%8e%af%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c-recurrent-neural-networks aria-label="循环神经网络 Recurrent neural networks">循环神经网络 Recurrent neural networks</a></li><li><a href=#lstms aria-label=LSTMs>LSTMs</a></li><li><a href=#beyond-simple-sequential-models aria-label="Beyond “simple” sequential models">Beyond “simple” sequential models</a></li></ul></li><li><a href=#lecture-19-lstm-implementation aria-label="Lecture 19: LSTM Implementation">Lecture 19: LSTM Implementation</a><ul><li><a href=#lstm-cell aria-label="LSTM cell">LSTM cell</a></li><li><a href=#full-sequence-lstm aria-label="Full sequence LSTM">Full sequence LSTM</a></li><li><a href=#batching-efficiently aria-label="Batching efficiently">Batching efficiently</a></li><li><a href=#training-lstms aria-label="Training LSTMs">Training LSTMs</a></li></ul></li><li><a href=#lecture-20-transformers-and-attention aria-label="Lecture 20: Transformers and Attention">Lecture 20: Transformers and Attention</a><ul><li><a href=#%e4%b8%a4%e7%a7%8d%e4%b8%ba%e6%97%b6%e9%97%b4%e5%ba%8f%e5%88%97%e5%bb%ba%e6%a8%a1%e7%9a%84%e6%96%b9%e6%b3%95-the-two-approaches-to-time-series-modeling aria-label="两种为时间序列建模的方法 The two approaches to time series modeling">两种为时间序列建模的方法 The two approaches to time series modeling</a></li><li><a href=#%e8%87%aa%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6%e5%92%8ctransformer-self-attention-and-transformers aria-label="自注意力机制和Transformer Self-attention and transformers">自注意力机制和Transformer Self-attention and transformers</a></li></ul></li><li><a href=#lecture-21-transformer-implementation aria-label="Lecture 21: Transformer Implementation">Lecture 21: Transformer Implementation</a><ul><li><a href=#%e8%87%aa%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6-self-attention aria-label="自注意力机制 Self-attention">自注意力机制 Self-attention</a></li><li><a href=#minibatching-with-batch-matrix-multiply aria-label="Minibatching with batch matrix multiply">Minibatching with batch matrix multiply</a></li><li><a href=#multihead-attention-%e5%a4%9a%e5%a4%b4%e6%b3%a8%e6%84%8f%e5%8a%9b aria-label="Multihead attention 多头注意力">Multihead attention 多头注意力</a></li><li><a href=#transformer-block aria-label="Transformer block">Transformer block</a></li></ul></li><li><a href=#lecture-23-moel-deployment aria-label="Lecture 23 Moel Deployment">Lecture 23 Moel Deployment</a><ul><li><a href=#%e6%a8%a1%e5%9e%8b%e9%83%a8%e7%bd%b2%e6%a6%82%e8%a7%88-model-deployment-overview aria-label="模型部署概览 Model deployment overview">模型部署概览 Model deployment overview</a></li><li><a href=#%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e7%bc%96%e8%af%91-machine-learning-compilation aria-label="机器学习编译 Machine learning compilation">机器学习编译 Machine learning compilation</a></li></ul></li><li><a href=#%e5%8f%82%e8%80%83%e6%96%87%e6%a1%a3 aria-label=参考文档>参考文档</a></li></ul></div></details></div></aside><script>let activeElement,elements;document.addEventListener("DOMContentLoaded",function(){if(checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),elements.length>0){activeElement=elements[0];const e=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${e}"]`).classList.add("active")}const t=document.getElementById("top-link");t&&t.addEventListener("click",e=>{e.preventDefault(),window.scrollTo({top:0,behavior:"smooth"})})},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{const e=window.pageYOffset||document.documentElement.scrollTop;if(e===0)return;elements&&elements.length>0&&(activeElement=Array.from(elements).find(t=>{if(getOffsetTop(t)-e>0&&getOffsetTop(t)-e<window.innerHeight/2)return t})||activeElement,elements.forEach(e=>{const n=encodeURI(e.getAttribute("id")).toLowerCase(),t=document.querySelector(`.inner ul li a[href="#${n}"]`);if(e===activeElement){t.classList.add("active");const e=document.querySelector(".toc .inner"),n=t.offsetTop,s=e.clientHeight,o=t.clientHeight,i=n-s/2+o/2;e.scrollTo({top:i,behavior:"smooth"})}else t.classList.remove("active")}))},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><h1 id=cmu-10-414-deep-learning-system学习笔记>《CMU 10-414 deep learning system》学习笔记<a hidden class=anchor aria-hidden=true href=#cmu-10-414-deep-learning-system学习笔记>#</a></h1><h1 id=写在最前面>写在最前面<a hidden class=anchor aria-hidden=true href=#写在最前面>#</a></h1><p>从 2024-04-28 到 2024-09-08，历时四个多月，总算把 DLSys 学完了。这门课的一些收获：</p><ul><li>自动微分理论知识和在实践过程中衍生的包括计算图等知识</li><li>系统学习了 ML 中几个基本模型和组件</li><li>Tensor 的 strides 相关内容</li><li>基础 CUDA 编程</li></ul><p>个人认为这门课一些没达到我预期的地方：</p><ul><li>CUDA 编程的内容太浅</li><li>后续讲 CNN、RNN、Transformer 的部分没必要，可以继续深入 CUDA 或者压缩课时</li></ul><p>本门课程的核心内容在 Lecture 0<del>15，对应的 homework 是 hw0</del>3，后面的内容没有时间可以跳过。</p><p>ps：全文章两万余字，Chrome 渲染图片时可能会很卡，建议使用 Microsoft Edge 浏览。</p><h1 id=lecture-1-introduction-and-logistics>Lecture 1: Introduction and Logistics<a hidden class=anchor aria-hidden=true href=#lecture-1-introduction-and-logistics>#</a></h1><h2 id=课程的目标>课程的目标<a hidden class=anchor aria-hidden=true href=#课程的目标>#</a></h2><p>本课程的目标是学习现代深度学习系统，了解包括自动微分、神经网络架构、优化以及 GPU 上的高效操作在内的技术的底层原理。作为实践，本课程将实现一个 needle（deep learning library）库，类似 PyTorch。</p><h2 id=为什么学习深度学习系统>为什么学习深度学习系统？<a hidden class=anchor aria-hidden=true href=#为什么学习深度学习系统>#</a></h2><p>为什么学习？深度学习这一概念很早就存在了，但直到 PyTorch、TensorFlow 此类现代深度学习框架发布，深度学习才开始迅速发展。简单易用的自动差分库是深度学习发展的最大动力。</p><p>除了使用这些库，我们为什么还要学习深度学习系统？</p><ul><li><p>为了构建深度学习系统<br>如果想要从事深度学习系统的开发，那毫无疑问得先学习它。目前深度学习框架并没完全成熟，还有很多开发新功能，乃至新的框架的机会。</p></li><li><p>为了能够更高效地使用现有系统<br>了解现有系统的内部实现，可以帮助我们写出更加高效的深度学习代码。如果想要提高自定义算子的效率，那必须先了解相关操作是如何实现的。</p></li><li><p>深度学习系统本身就很有趣<br>尽管这个系统看上去很复杂，但是其核心算法的原理确实相当简单的。两千行左右的代码，就可以写出一个深度学习库。</p></li></ul><h2 id=预备知识>预备知识<a hidden class=anchor aria-hidden=true href=#预备知识>#</a></h2><ul><li>systems programming</li><li>线性代数</li><li>其他数学知识：计算、概率、简单的证明</li><li>Python 和 C++ 经验</li><li>机器学习的相关经验</li></ul><h1 id=lecture-2-ml-refresher--softmax-regression>Lecture 2: ML Refresher & Softmax Regression<a hidden class=anchor aria-hidden=true href=#lecture-2-ml-refresher--softmax-regression>#</a></h1><h2 id=机器学习基础>机器学习基础<a hidden class=anchor aria-hidden=true href=#机器学习基础>#</a></h2><p>深度学习是由数据驱动的，所谓数据驱动，这意味着当我们想要写一个用于识别手写数字的模型时，我们关注的不是某个数字形状上有什么特点，如何通过编程识别该特点，而是直接将数据集喂给模型，模型自动训练并识别数字类别。</p><p>深度学习模型由三部分组成：</p><ul><li>假说模型：模型的结构，包括一系列参数，其描述了模型从输入到输出的映射关系；</li><li>损失函数：指定了对模型的评价，损失函数值越小，说明该模型在指定任务上完成得更好；</li><li>优化方法：用于对模型中参数进行优化，使得损失函数最小的方法。</li></ul><h2 id=softmax-回归>Softmax 回归<a hidden class=anchor aria-hidden=true href=#softmax-回归>#</a></h2><p>以经典的 softmax 回归模型为例，简单回顾一下 ML 模型。</p><p>考虑一个 k 分类任务，其中数据集为 x(i)∈Rn , y(i)∈1,…,k   i=1,…,mx(i)∈Rn , y(i)∈1,…,k   i=1,…,m，nn 标识输入数据集的维度，kk 标识标签类别数，mm 标识数据集样本数量。</p><p>一个假说模型就是将一个 nn 维的输入映射到一个 kk 维的输出，即：h:Rn→Rkh:Rn→Rk。注意，模型并不会直接输出类别的序号，而是通过输出一个 kk 维向量 h(x)h(x)，其中第 ii 个元素 hi(x)hi​(x) 表示是第 ii 个类别的概率。</p><p>对于线性模型来说，使用 θ∈Rn×kθ∈Rn×k 这个模型中的参数，那么 $hθ(x)=θTx$。</p><p>如果一次输入多个数据，那么输入数据就可以组织成一个矩阵，相比起多个向量操作，矩阵的操作通常效率更高，我们在代码实现中一般也是用矩阵操作。数据集可以表示为：</p><p>$$X∈Rm×n=[x(1)Tx(m)T],y∈{1,&mldr;,k}m=[y(1)y(m)]​​$$</p><p>数据集的矩阵是一个个样本转置后堆叠 stack 起来的。那么输出可以表示为：</p><p>$$hθ(X)=[hθ(x(1))Thθ(x(m))T]=[x(1)Tθx(m)Tθ]=Xθ$$</p><p>关于损失函数 lerrlerr​，一种朴素的想法是将模型预测错误的模型数据量作为损失函数，即如果模型预测的正确率最高的那个类别与真实类别不相同，则损失函数为 1，否则为 0：</p><p>$$lerr(h(x),y)={0 if argmaxihi(x)=y1 
otherwiselerr​(h(x),y)={0 1 ​if argmaxi​hi​(x)=yotherwise​$$</p><p>遗憾的是，这个符合直觉函数是不可微分的，难以对参数进行优化。更合适的做法是使用交叉熵损失函数。</p><p>在此之前，我们将先讲输出过一个 softmax 函数，使之的行为更像一个概率——各个类别的概率之和为 1：</p><p>$$zi=p(label=i)=exp⁡(hi(x))∑j=1kexp⁡(hj(x))​$$</p><p>那么交叉熵损失函数就可以定义为：</p><p>$$lce(h(x),y)=−log⁡p(label=y)=−hy(x)+log⁡∑j=1kexp⁡(hj(x))lce​(h(x),y)=−logp(label=y)=−hy​(x)+logj=1∑k​exp(hj​(x))$$</p><p>注意在计算交叉熵时，通过运算进行了化简，这使得我们可以省去计算 softmax 的过程，直接计算最终的结果。不但如此，交叉熵的计算中，如果 hi(x)hi​(x) 的值很小，那么取对数会出现很大的值，化简后的计算则避免了这种情况。</p><p>所有的深度学习问题，都可以归结为一下这个最优化问题：</p><p>$$θ  1m∑i=1ml(hθ(x(i)),y(i)))θminimize​  m1​i=1∑m​l(hθ​(x(i)),y(i)))$$</p><p>我们使用梯度下降法对该问题进行优化。在此之前，首先介绍一下关于梯度。我们的优化目标可以看作一个关于θ∈Rn×kθ∈Rn×k的函数ff，那么其在θ0θ0​处的梯度可以表示为：</p><p>$$∇θf(θ0)∈Rn×k=[∂f(θ0)∂θ11⋯∂f(θ0)∂θk1⋱∂f(θ0)∂θn1⋯∂f(θ0)∂θnk]∇θ​f(θ0​)∈Rn×k=​∂θ11​∂f(θ0​)​⋮∂θn1​∂f(θ0​)​​⋯⋱⋯​∂θk1​∂f(θ0​)​⋮∂θnk​∂f(θ0​)​​​$$</p><p>其中，第ii行第jj个元素表示除θijθij​之外的参数都被当作常数，对θijθij​求偏导。</p><p>梯度下降，就是沿着梯度方向不断进行迭代，以求找到最佳的θθ使得目标函数值最小。</p><p>$$θ:=θ0−α∇f(θ0)θ:=θ0​−α∇f(θ0​)$$</p><p>上式中，αα被称为学习率或者步长。</p><p>事实上，在现代深度学习中，并不是使用的传统梯度下降的方案，因为其无法将所有训练集一次性读入并计算梯度。现代使用的是随机梯度下降（Stochastic Gradient Descent，SGD）</p><p>首先将m个训练集样本划分一个个小batch，每个batch都有B条数据。那每一batch的数据表示为X∈RB×nX∈RB×n，更新参数θθ的公式变为：</p><p>θ:=θ0−αB∇f(θ0)θ:=θ0​−Bα​∇f(θ0​)</p><p>我们的梯度变成了每个小batch对全体样本梯度的估计。</p><p>那如何计算梯度表达式呢？梯度矩阵中每个元素都是一个偏导数，我们就先从计算偏导数开始。假设hh是个向量，我们来计算偏导数∂lce(h,y)∂hi∂hi​∂lce​(h,y)​：</p><p>$$∂lce(h,y)∂hi=∂∂hi(−hy+log⁡∑j=1kexp⁡hj)=−1{i=y}+exp⁡(hj)∑j=1kexp⁡hj=−1{i=y}+softmax(h)=z−ey∂hi​∂lce​(h,y)​​=∂hi​∂​(−hy​+logj=1∑k​exphj​)=−1{i=y}+∑j=1k​exphj​exp(hj​)​=−1{i=y}+softmax(h)=z−ey​​$$</p><p>如果hh是个向量，那么梯度∇hlce(h,y)∇h​lce​(h,y)就能够以向量的形式表示为：</p><p>$$∇hlce(h,y)=z−ey$$</p><p>这里我们将对hh进行softmax标准化记为zz，eyey​表示对应的单位向量。</p><p>事实上，我们要计算的梯度是关于θθ的，具体来说，表达式为∇θlce(θTx,y)∇θ​lce​(θTx,y)，其中，θθ是个矩阵。或许，可以使用链式法则进行求解，但是太麻烦了，这里还涉及矩阵对向量的求导。我们需要一种更加通用的求导方案。</p><p>有两个解决办法：</p><ul><li>正确且官方的做法：使用矩阵微分学、雅可比矩阵、克罗内克积和向量化等知识进行求解。</li><li>一个hacky、登不上台面、但大家都在用的方案：将所有的矩阵和向量当作标量，使用链式法则求解，并进行转置操作使得结果的size符合预期，最后检查数值上结果是否正确。</li></ul><p>按照第二个方法的逻辑，过程为：</p><p>∂∂θlce(θTx,y)=∂lce(θTx,y)∂θTx⋅∂θTx∂θ=[z−ey]k×1⋅xn×1=x⋅[z−ey]∂θ∂​lce​(θTx,y)​=∂θTx∂lce​(θTx,y)​⋅∂θ∂θTx​=[z−ey​]k×1​⋅xn×1​=x⋅[z−ey​]​</p><p>其中，z=softmax(θTx)z=softmax(θTx)。注意，倒数第二步求出的结果是两个列向量相乘，不能运算。又已知结果应该是n×kn×k的矩阵，调整向量之间的顺序即可。</p><p>照猫画虎，可以写出batch的情况，X∈RB×nX∈RB×n：</p><p>∂∂θlce(θTX,y)=∂lce(θTX,y)∂θTX⋅∂θTX∂θ=[Z−Ey]B×k⋅XB×n=XT⋅[Z−Ey]∂θ∂​lce​(θTX,y)​=∂θTX∂lce​(θTX,y)​⋅∂θ∂θTX​=[Z−Ey​]B×k​⋅XB×n​=XT⋅[Z−Ey​]​</p><h1 id=lecture-3-manual-neural-networks>Lecture 3: Manual Neural Networks<a hidden class=anchor aria-hidden=true href=#lecture-3-manual-neural-networks>#</a></h1><p>这节课，我们将人工实现全连接神经网络，之后的课程，将引入自动微分技术。</p><h2 id=从线性模型转变为非线性模型>从线性模型转变为非线性模型<a hidden class=anchor aria-hidden=true href=#从线性模型转变为非线性模型>#</a></h2><p><img alt=image.png|200 loading=lazy src="https://pics.zhouxin.space/202406071816708.png?x-oss-process=image/quality,q_90/format,webp"></p><p>如上图所示，线性模型本质上是将样本空间划分为线性的几个部分，这样的模型性能十分有限，因此很多不满足这样分布的实际问题就不能被解决。</p><p>一种解决方案是，在将样本输入到线性分类器前，先人工挑选出某些特征，即对XX应用一个函数ϕϕ，其将XX映射到ϕ(X)ϕ(X)上，映射后的空间可以被线性划分。一方面，它确实是早期实践中行之有效的方案；另一方面，人工提取特征的泛化性能有限，受限于具体问题和研究人员的对问题的洞察程度。</p><p>如果我们使用线性网络提取特征，并直接接上一个线性分类头，这两个线性层等效为一个线性层，并不能做到非线性化的要求（基础知识，此处不再解释）。</p><p>因此，在使用线性网络提取特征后，需要再接上一个非线性函数σσ，即ϕ=σ(WTX)ϕ=σ(WTX)。</p><h2 id=神经网络>神经网络<a hidden class=anchor aria-hidden=true href=#神经网络>#</a></h2><p>上文提到的使用非线性函数后的模型，就可以视作一种最简单的神经网络。所谓神经网络，值得是机器学习中某一类特定的假说模型，其由多层组成，每一层都有大量可以微分的参数。</p><p>神经网络最初的确起源于模拟人类神经元这一动机，但随着其发展，越来越多的神经网络模型出现，与人类大脑神经网络越来越不相关。</p><p>以双层神经网络为例，其形式化表示为hθ(x)=W2Tσ(W1Tx)hθ​(x)=W2T​σ(W1T​x)，所有可学习的参数使用θθ表示。以batch的矩阵形式表示为：</p><p>hθ(X)=σ(XW1)W2hθ​(X)=σ(XW1​)W2​</p><p>接下来给出L层多层感知机（a.k.a. MLP、前馈神经网络、全连接层）的形式化表达：</p><p>{Zi+1=σi(ZiWi),i=1,&mldr;,LZ1=Xhθ(X)=ZL+1[Zi∈Rm×ni,Wi∈Rni×ni+1]σi:R→R⎩⎨⎧​Zi+1​=σi​(Zi​Wi​),i=1,&mldr;,LZ1​=Xhθ​(X)=ZL+1​[Zi​∈Rm×ni​,Wi​∈Rni​×ni+1​]σi​:R→R​</p><p>每一层的输入为ZiZi​，输出为Zi+1Zi+1​。</p><p>为什么要是用深度网络而不是宽度网络？没有很完美的解释，但最好并且最现实的解释是：经验证明，当参数量固定时，深度网络性能优于宽度网络。</p><h2 id=反向传播梯度计算>反向传播（梯度计算）<a hidden class=anchor aria-hidden=true href=#反向传播梯度计算>#</a></h2><p>与Lecture 2一致，使用交叉熵作为损失函数，使用SGD作为优化算法，唯一的区别是，这次要对MLP网络求解梯度。</p><p>对于两层神经网络hθ(X)=σ(XW1)W2hθ​(X)=σ(XW1​)W2​，待求的梯度表达式为：</p><p>∇{W1,W2}lce(σ(XW1)W2,y)∇{W1​,W2​}​lce​(σ(XW1​)W2​,y)</p><p>对于W2W2​的梯度，其与Lecture 2的计算类似：</p><p>∂lce(σ(XW1)W2,y)∂W2=∂lce(σ(XW1)W2,y)∂σ(XW1)W2⋅∂σ(XW1)W2∂W2=(S−Iy)m×k⋅σ(XW1)m×d=σ(XW1)T⋅(S−Iy)[S=softmax(σ(XW1))]∂W2​∂lce​(σ(XW1​)W2​,y)​​=∂σ(XW1​)W2​∂lce​(σ(XW1​)W2​,y)​⋅∂W2​∂σ(XW1​)W2​​=(S−Iy​)m×k​⋅σ(XW1​)m×d​=σ(XW1​)T⋅(S−Iy​)[S=softmax(σ(XW1​))]​</p><p>对于W1W1​的梯度，其需要多次应用链式法则，但并不难计算：</p><p>∂lce(σ(XW1)W2,y)∂W1=∂lce(σ(XW1)W2,y)∂σ(XW1)W2⋅∂σ(XW1)W2∂σ(XW1)⋅∂σ(XW1)∂XW1⋅∂XW1∂X1=(S−Iy)m×k⋅[W2]d×k⋅σ′(XW1)m×d⋅Xm×n=XT⋅[σ′(XW1)⊙((S−Iy)⋅W2T)][S=softmax(σ(XW1))]∂W1​∂lce​(σ(XW1​)W2​,y)​​=∂σ(XW1​)W2​∂lce​(σ(XW1​)W2​,y)​⋅∂σ(XW1​)∂σ(XW1​)W2​​⋅∂XW1​∂σ(XW1​)​⋅∂X1​∂XW1​​=(S−Iy​)m×k​⋅[W2​]d×k​⋅σ′(XW1​)m×d​⋅Xm×n​=XT⋅[σ′(XW1​)⊙((S−Iy​)⋅W2T​)][S=softmax(σ(XW1​))]​</p><p>以上公式中⊙⊙表示逐元素乘法。至于为啥这么算，俺也不知道。</p><p>接下来将其推广到一般情况，即LL层的MLP中对WiWi​求导：</p><p>∂l(Zl+1,y)∂Wi=∂l∂Zl+1⋅∂Zl+1∂Zl⋅&mldr;⋅∂Zi+2∂Zi+1⋅∂Zi+1∂Wi=Gi+1⋅∂Zi+1∂Wi=∂l∂Zi+1⋅∂Zi+1Wi∂Wi​∂l(Zl+1​,y)​​=∂Zl+1​∂l​⋅∂Zl​∂Zl+1​​⋅&mldr;⋅∂Zi+1​∂Zi+2​​⋅∂Wi​∂Zi+1​​=Gi+1​⋅∂Wi​∂Zi+1​​=∂Zi+1​∂l​⋅Wi​∂Zi+1​​​</p><p>由上述公式，我们可以得到一个反向迭代计算的GiGi​，即：</p><p>Gi=Gi+1⋅Zi+1Zi=Gi+1⋅∂σ(ZiWi)∂ZiWi⋅∂ZiWiZi=Gi+1⋅σ′(ZiWi)⋅WiGi​​=Gi+1​⋅Zi​Zi+1​​=Gi+1​⋅∂Zi​Wi​∂σ(Zi​Wi​)​⋅Zi​∂Zi​Wi​​=Gi+1​⋅σ′(Zi​Wi​)⋅Wi​​</p><p>上面的计算都是将矩阵当作标量进行的，接下来我们考虑其维度。已知，Zi∈Rm×niZi​∈Rm×ni​是第ii层的输入，Gi=∂l∂ZiGi​=∂Zi​∂l​，其维度如何呢？GiGi​每个元素表示损失函数ll对第ii层输入的每一项求偏导，也可以记作是ll对ZiZi​求梯度，即∇Zil∇Zi​​l，其维度显然是m×nim×ni​，继续计算前文GiGi​：</p><p>Gi=[Gi+1]m×ni+1⋅σ′(ZiWi)m×ni+1⋅[Wi]ni×ni+1=[Gi+1⊙σ′(ZiWi)]WiTGi​​=[Gi+1​]m×ni+1​​⋅σ′(Zi​Wi​)m×ni+1​​⋅[Wi​]ni​×ni+1​​=[Gi+1​⊙σ′(Zi​Wi​)]WiT​​</p><p>有了GiGi​，就可以继续计算ll对WiWi​的偏导数了：</p><p>∂l(Zl+1,y)∂Wi=Gi+1⋅∂Zi+1∂Wi=Gi+1⋅∂σ(ZiWi)∂ZiWi⋅∂ZiWi∂Wi=[Gi+1]m×ni+1⋅σ′(ZiWi)m×ni+1⋅[Zi]m×ni=ZiT⋅[Gi+1⊙σ′(ZiWi)]∂Wi​∂l(Zl+1​,y)​​=Gi+1​⋅∂Wi​∂Zi+1​​=Gi+1​⋅∂Zi​Wi​∂σ(Zi​Wi​)​⋅∂Wi​∂Zi​Wi​​=[Gi+1​]m×ni+1​​⋅σ′(Zi​Wi​)m×ni+1​​⋅[Zi​]m×ni​​=ZiT​⋅[Gi+1​⊙σ′(Zi​Wi​)]​</p><p>至此，每个小组件都已制造完毕，让我们来把它装起来吧！</p><ul><li>前向传播<ul><li>初始化：Z1=XZ1​=X</li><li>迭代：Zi+1=σ(ZiWi)Zi+1​=σ(Zi​Wi​) 直至i=Li=L（注意，最后一层没有非线性部分，此处没有展示出来）</li></ul></li><li>反向传播<ul><li>初始化：GL+1=S−IyGL+1​=S−Iy​</li><li>迭代：Gi=[Gi+1⊙σ′(ZiWi)]WiTGi​=[Gi+1​⊙σ′(Zi​Wi​)]WiT​ 直至i=1i=1 值得注意的是，在反向传播中，需要用到前向传播的中间结果ZiZi​。为了更高效地计算梯度，不得不以牺牲内存空间为代价，即空间换时间。</li></ul></li></ul><blockquote><p>许多课程，讲到这里就结束了，但对我们这门课来说，才刚刚开始…</p></blockquote><h1 id=lecture-4-automatic-differentiation>Lecture 4: Automatic Differentiation<a hidden class=anchor aria-hidden=true href=#lecture-4-automatic-differentiation>#</a></h1><h2 id=基本工具>基本工具<a hidden class=anchor aria-hidden=true href=#基本工具>#</a></h2><ul><li>计算图 计算图是自动微分中常用的一种工具。计算图是一张有向无环图，每个节点表示（中间结果）值，每条边表示输入输出变量。例如，y=f(x1,x2)=ln⁡(x1)+x1x2−sin⁡x2y=f(x1​,x2​)=ln(x1​)+x1​x2​−sinx2​对应的计算图为：<img alt=|400 loading=lazy src="https://pics.zhouxin.space/202406071612073.webp?x-oss-process=image/quality,q_90/format,webp">按照拓扑序列遍历这张图，就可以得到对应表达式的值。</li></ul><h2 id=对自动微分方法的简单介绍>对自动微分方法的简单介绍<a hidden class=anchor aria-hidden=true href=#对自动微分方法的简单介绍>#</a></h2><p>深度学习中，一个核心内容就是计算梯度。这里介绍集中计算梯度的方案：</p><ul><li>偏导数定义</li></ul><p>梯度是由一个个偏导数组成的，可以直接根据偏导数的定义来计算梯度：</p><p>∂f(θ)∂θi=lim⁡ϵ→0f(θ+ϵei)−f(θ)ϵ∂θi​∂f(θ)​=ϵ→0lim​ϵf(θ+ϵei​)−f(θ)​</p><p>其中，eiei​是表示第ii个方向上的单位向量。</p><ul><li><p>数值求解 根据上述定义，我们可以选取一个很小的量代入ϵϵ，得到数值计算偏导的方法：</p><p>∂f(θ)∂θi=f(θ+ϵei)−f(θ−ϵei)2ϵ+o(ϵ2)∂θi​∂f(θ)​=2ϵf(θ+ϵei​)−f(θ−ϵei​)​+o(ϵ2)</p><p>这里并不是直接使用第一项的公式，即分子不是f(θ+ϵei)−f(θ)f(θ+ϵei​)−f(θ)，并且误差项是ϵ2ϵ2，这是由于泰勒展开：</p><p>f(θ+δ)=f(θ)+f′(θ)δ+12f′′(θ)δ2+o(δ3)f(θ−δ)=f(θ)+f′(θ)δ−12f′′(θ)δ2+o(δ3)f(θ+δ)=f(θ)+f′(θ)δ+21​f′′(θ)δ2+o(δ3)f(θ−δ)=f(θ)+f′(θ)δ−21​f′′(θ)δ2+o(δ3)​</p><p>上述两式作差，即可得到数值计算f′(θ)f′(θ)的方法。</p></li></ul><p>这个方法的问题在于存在误差，并且效率低下（这里要计算两次f），该方法常用于验证其它方法的具体实现是否出错。具体来说，验证如下等式是否成立：</p><p>δT∇θf(θ)=f(θ+ϵδ)−f(θ−ϵδ)2ϵ+o(ϵ2)δT∇θ​f(θ)=2ϵf(θ+ϵδ)−f(θ−ϵδ)​+o(ϵ2)</p><p>其中δδ是单位球上的某个向量，∇θf(θ)∇θ​f(θ)是使用其它方法计算得到的梯度。等式左边是其它方法计算的梯度在δδ上的投影，右侧是使用数值求解得到的梯度值，验证该等式是否成立就可以判断左侧梯度是否计算错误。</p><ul><li><p>符号微分 符号微分，就是根据微分的计算规则使用符号手动计算微分。部分规则为：</p><p>∂(f(θ)+g(θ))∂θ=∂f(θ)∂θ+∂g(θ)∂θ∂(f(θ)g(θ))∂θ=g(θ)∂f(θ)∂θ+f(θ)∂g(θ)∂θ∂f(g(θ))∂θ=∂f(g(θ))∂g(θ)∂g(θ)∂θ​∂θ∂(f(θ)+g(θ))​=∂θ∂f(θ)​+∂θ∂g(θ)​∂θ∂(f(θ)g(θ))​=g(θ)∂θ∂f(θ)​+f(θ)∂θ∂g(θ)​∂θ∂f(g(θ))​=∂g(θ)∂f(g(θ))​∂θ∂g(θ)​​</p><p>根据该公式，可以计算得到f(θ)=∏i=1nθif(θ)=∏i=1n​θi​的梯度表达式为：∂f(θ)∂θk=∏j≠knθj∂θk​∂f(θ)​=∏j=kn​θj​。如果我们根据该公式来计算梯度，会发现需要计算n(n−2)n(n−2)次乘法才能得到结果。这是因为在符号运算的过程中，我们忽略了可以反复利用的中间结果。</p></li><li><p>正向模式自动微分 forward mode automatic differentiation 沿着计算图的拓扑序列，同样可以计算出输出关于输入的导数，还是以y=f(x1,x2)=ln⁡(x1)+x1x2−sin⁡x2y=f(x1​,x2​)=ln(x1​)+x1​x2​−sinx2​为例，其计算图为：<img alt=image.png|400 loading=lazy src="https://pics.zhouxin.space/202406071612328.png?x-oss-process=image/quality,q_90/format,webp"></p></li></ul><p>整个梯度计算过程如下，在此过程中应用到了具体函数的求导公式：</p><p>x1=2x2=5v˙1=1v˙2=0v˙3=v˙1/v1=0.5v˙4=v˙1v2+v˙2v1=1×5+0×2=5v˙5=v2˙cos⁡v2=0×cos⁡5=0v˙6=v˙3+v˙4=0.5+5=5.5v˙7=v6˙−v5˙=5.5−0=5.5​x1​=2x2​=5v˙1​=1v˙2​=0v˙3​=v˙1​/v1​=0.5v˙4​=v˙1​v2​+v˙2​v1​=1×5+0×2=5v˙5=v2​˙​cosv2​=0×cos5=0v˙6​=v˙3​+v˙4​=0.5+5=5.5v˙7​=v6​˙​−v5​˙​=5.5−0=5.5​</p><p>对于f:Rn→Rkf:Rn→Rk，前向传播需要nn次前向计算才能得到关于每个输入的梯度，这就意味前向传播适合nn比较小、kk比较大的情况。但是在深度学习中，通常nn比较大、kk比较小。</p><ul><li><p>反向模式自动微分 定义adjoint:vi‾=∂y∂viadjoint:vi​​=∂vi​∂y​,其表示损失函数对于参数vivi​的偏导。 整个计算过程如下所示，需要注意的是v2‾v2​​的计算过程，其在计算图上延伸出了两个节点，因此梯度也由两部分相加：</p><p>v7‾=∂y∂v7=1v6‾=v7‾∂v7∂v6=v7‾×1=1v5‾=v7‾∂v7∂v5=v7‾×(−1)=−1v4‾=v6‾∂v6∂v4=v6‾×1=1v3‾=v6‾∂v6∂v3=v6‾×1=1v2‾=v5‾∂v5∂v2+v4‾∂v4∂v2=v5‾×cos⁡v2+v4‾×v1v1‾=v4‾∂v4∂v1+v3‾∂v3∂v1=v4‾×v2+v3‾1v1=5+12=5.5​v7​​=∂v7​∂y​=1v6​​=v7​​∂v6​∂v7​​=v7​​×1=1v5​​=v7​​∂v5​∂v7​​=v7​​×(−1)=−1v4​​=v6​​∂v4​∂v6​​=v6​​×1=1v3​​=v6​​∂v3​∂v6​​=v6​​×1=1v2​​=v5​​∂v2​∂v5​​+v4​​∂v2​∂v4​​=v5​​×cosv2​+v4​​×v1​v1​​=v4​​∂v1​∂v4​​+v3​​∂v1​∂v3​​=v4​​×v2​+v3​​v1​1​=5+21​=5.5​</p></li></ul><p>接下来我们讨论一下为什么前文中v2‾v2​​由两部分组成。考虑如下一个计算图：<img alt=image.png|400 loading=lazy src="https://pics.zhouxin.space/202406071612078.png?x-oss-process=image/quality,q_90/format,webp"></p><p>yy可以被视作关于v2v2​和v3v3​的函数，即y=f(v2,v3)y=f(v2​,v3​)，那么：</p><p>v1‾=∂y∂v1=∂f(v2,v3)∂v2∂v2∂v1+∂f(v2,v3)∂v3∂v3∂v1=v2‾∂v2∂v1+v3‾∂v3∂v1v1​​=∂v1​∂y​=∂v2​∂f(v2​,v3​)​∂v1​∂v2​​+∂v3​∂f(v2​,v3​)​∂v1​∂v3​​=v2​​∂v1​∂v2​​+v3​​∂v1​∂v3​​</p><p>因此，定义partial adjoint vi→j‾=vj‾∂vj∂vivi→j​​=vj​​∂vi​∂vj​​，那么vi‾vi​​可以表示为：</p><p>νi‾=∑j∈next(i)νi→j‾νi​​=j∈next(i)∑​νi→j​​</p><h2 id=反向模式微分算法的实现>反向模式微分算法的实现<a hidden class=anchor aria-hidden=true href=#反向模式微分算法的实现>#</a></h2><p>基于以上分析，可以写出如下的实现反向模式微分算法的伪代码：<img alt=image.png|400 loading=lazy src="https://pics.zhouxin.space/202406071612188.png?x-oss-process=image/quality,q_90/format,webp"></p><p>其中<code>node_to_grad</code>是一个字典，保存着每个节点的partial adjoint值。由于是按照逆拓扑序列遍历的节点，因此可以保证当遍历到ii时，所有以ii为输入的节点（k节点所在的集合）都已被遍历完毕，即vk‾vk​​已经计算出来。</p><p>那么partial adjoint值使用什么数据结构保存呢？一个常见的思路是使用邻接矩阵，但是这个矩阵中有大量元素是不存在了，空间浪费很大。我们可以在原有计算图的基础上进行拓展来保存partial adjoint和adjonitzhi之间的计算关系。</p><p>如下图所示，黑色部分是原表达式的计算图，红色部分是将adjoint和partial adjount的计算图：<img alt=image.png|400 loading=lazy src="https://pics.zhouxin.space/202406071611419.png?x-oss-process=image/quality,q_90/format,webp"></p><p>使用计算图，除了能够节省内存外，还能清楚的看到正向计算的中间结果和反向计算之间的依赖关系，进而优化计算。</p><h2 id=反向模式ad和反向传播的区别>反向模式ad和反向传播的区别<a hidden class=anchor aria-hidden=true href=#反向模式ad和反向传播的区别>#</a></h2><p><img alt=image.png|500 loading=lazy src="https://pics.zhouxin.space/202406071817738.png?x-oss-process=image/quality,q_90/format,webp"></p><p>反向传播：</p><ul><li>在反向计算过程中使用与前向传播完全相同的计算图</li><li>应用于第一代深度学习框架</li></ul><p>反向AD：</p><ul><li>为adjoint在计算图中创建独立的节点</li><li>被应用于现代深度学习框架</li></ul><p>现代普遍应用反向AD的原因：</p><ul><li>某些损失函数是关于梯度的函数，这种情况下需要计算梯度的梯度，但反向传播就不能计算此类情况，而在反向AD中只要增加一个节点后在此计算梯度即可；</li><li>反向AD优化空间更大。</li></ul><h2 id=考虑tensor的反向模式ad>考虑Tensor的反向模式AD<a hidden class=anchor aria-hidden=true href=#考虑tensor的反向模式ad>#</a></h2><p>前面都是在假设中间变量是标量的基础上讨论的，接下来我们将其推广到Tensor上。</p><p>首先推广adjoint，定义对于一个TensorZZ，其adjointZ‾Z为：</p><p>=[∂y∂Z1,1&mldr;∂y∂Z1,n&mldr;&mldr;&mldr;∂y∂Zm,1&mldr;∂y∂Zm,n]=​∂Z1,1​∂y​&mldr;∂Zm,1​∂y​​&mldr;&mldr;&mldr;​∂Z1,n​∂y​&mldr;∂Zm,n​∂y​​​</p><p>鉴于</p><p>Zij=∑kXikWkjv=f(Z)Zij​v​=k∑​Xik​Wkj​=f(Z)​</p><p>那么在计算Xi,k‾Xi,k​​时，需要将所有计算图上以Xi,kXi,k​为输入的节点都找出来，即ZZ的第ii行的每个元素。因此Xi,k‾Xi,k​​的计算公式为：</p><p>Xi,k‾=∑j∂Zi,j∂Xi,kZˉi,j=∑jWk,jZˉi,jXi,k​​=j∑​∂Xi,k​∂Zi,j​​Zˉi,j​=j∑​Wk,j​Zˉi,j​</p><p>上述公式记为矩阵形式为：</p><p>X‾=Z‾WTX=ZWT</p><h1 id=lecture-5-automatic-differentiation-implementation>Lecture 5: Automatic Differentiation Implementation<a hidden class=anchor aria-hidden=true href=#lecture-5-automatic-differentiation-implementation>#</a></h1><p>这讲主要介绍我们hw中要实现的needle的总体框架，项目中已给出了约1000行代码。</p><h2 id=autogradpy>autograd.py<a hidden class=anchor aria-hidden=true href=#autogradpy>#</a></h2><p>autograd保存与实现自动微分相关的代码。</p><p><code>Value</code>类对应计算图上的节点，其数据成员包括：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>Value</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;A value in the computational graph.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># trace of computational graph</span>
</span></span><span class=line><span class=cl>    <span class=n>op</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>Op</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>inputs</span><span class=p>:</span> <span class=n>List</span><span class=p>[</span><span class=s2>&#34;Value&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=c1># The following fields are cached fields for</span>
</span></span><span class=line><span class=cl>    <span class=c1># dynamic computation</span>
</span></span><span class=line><span class=cl>    <span class=n>cached_data</span><span class=p>:</span> <span class=n>NDArray</span>
</span></span><span class=line><span class=cl>    <span class=n>requires_grad</span><span class=p>:</span> <span class=nb>bool</span>
</span></span></code></pre></td></tr></table></div></div><p><code>op</code>用于保存该节点的运算符，<code>inputs</code>保存该运算符的操作数，<code>cached_data</code>保存该节点的数值，其数据结构因平台不同而区别。</p><h2 id=ops>ops<a hidden class=anchor aria-hidden=true href=#ops>#</a></h2><p>本节主要介绍needle库的代码结构，笔记相当草率，建议看原视频。</p><p>ops文件夹（2023版本）或者op.py（2022）版本保存各种算子的实现。 <code>Op</code>类规定了两个必须要实现的接口：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>Op</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Operator definition.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>compute</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=o>*</span><span class=n>args</span><span class=p>:</span> <span class=n>Tuple</span><span class=p>[</span><span class=n>NDArray</span><span class=p>]):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;Calculate forward pass of operator.
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>        Parameters
</span></span></span><span class=line><span class=cl><span class=s2>        ----------
</span></span></span><span class=line><span class=cl><span class=s2>        input: np.ndarray
</span></span></span><span class=line><span class=cl><span class=s2>            A list of input arrays to the function
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>        Returns
</span></span></span><span class=line><span class=cl><span class=s2>        -------
</span></span></span><span class=line><span class=cl><span class=s2>        output: nd.array
</span></span></span><span class=line><span class=cl><span class=s2>            Array output of the operation
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>        &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=k>raise</span> <span class=ne>NotImplementedError</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>gradient</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=p>,</span> <span class=n>out_grad</span><span class=p>:</span> <span class=s2>&#34;Value&#34;</span><span class=p>,</span> <span class=n>node</span><span class=p>:</span> <span class=s2>&#34;Value&#34;</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Union</span><span class=p>[</span><span class=s2>&#34;Value&#34;</span><span class=p>,</span> <span class=n>Tuple</span><span class=p>[</span><span class=s2>&#34;Value&#34;</span><span class=p>]]:</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;Compute partial adjoint for each input value for a given output adjoint.
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>        Parameters
</span></span></span><span class=line><span class=cl><span class=s2>        ----------
</span></span></span><span class=line><span class=cl><span class=s2>        out_grad: Value
</span></span></span><span class=line><span class=cl><span class=s2>            The adjoint wrt to the output value.
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>        node: Value
</span></span></span><span class=line><span class=cl><span class=s2>            The value node of forward evaluation.
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>        Returns
</span></span></span><span class=line><span class=cl><span class=s2>        -------
</span></span></span><span class=line><span class=cl><span class=s2>        input_grads: Value or Tuple[Value]
</span></span></span><span class=line><span class=cl><span class=s2>            A list containing partial gradient adjoints to be propagated to
</span></span></span><span class=line><span class=cl><span class=s2>            each of the input node.
</span></span></span><span class=line><span class=cl><span class=s2>        &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=k>raise</span> <span class=ne>NotImplementedError</span><span class=p>()</span>
</span></span></code></pre></td></tr></table></div></div><p><code>compute</code>接口用于描述该运算符实施的运算，<code>gradient</code>描述该运算符对应的梯度计算方式。</p><h1 id=lecture-6-fully-connected-network-optimization-initialization>Lecture 6: Fully connected network, optimization, initialization<a hidden class=anchor aria-hidden=true href=#lecture-6-fully-connected-network-optimization-initialization>#</a></h1><h2 id=全连接网络>全连接网络<a hidden class=anchor aria-hidden=true href=#全连接网络>#</a></h2><p>之前我们讨论的全连接网络都是不含偏执项的（为了方便进行手动微分），本章将介绍真正的MLP。其通过迭代的过程进行定义：</p><p>zi+1=σi(WiTzi+bi),   i=1,&mldr;,Lhθ(x)=zL+1z1=x​zi+1​=σi​(WiT​zi​+bi​),   i=1,&mldr;,Lhθ​(x)=zL+1​z1​=x​</p><p>上述模型中，可优化的参数集合为θ=W1:L,b1:Lθ=W1:L​,b1:L​。σi(x)σi​(x)是非线性的激活函数，特别的，最后一层没有激活函数，即σL(x)=xσL​(x)=x。</p><p>迭代的表达式写成矩阵形式为：</p><p>Zi+1=σi(ZiWi+1biT)Zi+1​=σi​(Zi​Wi​+1biT​)​</p><p>其中，11表示一个表示一个全1的列向量，用于将列向量biTbiT​广播到与矩阵ZiWiZi​Wi​相匹配的形状。</p><p>在实际实现过程中，我们不用浪费空间去构造这样一个全1列向量，而是直接使用广播算子。在NumPy有许多自动的广播操作，但是在我们实现的needle库中，这一操作更加显式，例如对于(n×1)→(m×n)(n×1)→(m×n)，要执行的操作为<code>A.reshape((1, n)).broadcast_to((m, n))</code>。</p><h2 id=优化>优化<a hidden class=anchor aria-hidden=true href=#优化>#</a></h2><p>对于有监督的深度学习任务，一般的优化目标为：</p><p>$$θ  f(θ)=1m∑i=1ml(hθ(x(i),y(i)))minimizeθ​  f(θ)=m1​i=1∑m​l(hθ​(x(i),y(i)))$$</p><p>接下来将介绍几常用的优化算法。</p><ul><li>梯度下降 gradient desecent 梯度下降法之前几节课讲过了，这里直接给出其数学表达式：</li></ul><p>$$ θt+1=θt−α∇θf(θt)θt+1​=θt​−α∇θ​f(θt​)$$</p><pre><code>其中，tt表示迭代次数。
</code></pre><p>学习率这一参数对于该方法格外重要，不同的学习率的表现相差很大很大：<img alt=image.png|600 loading=lazy src="https://pics.zhouxin.space/202406161006752.png?x-oss-process=image/quality,q_90/format,webp"></p><p>上图展示了大学习率和小学习率的迭代过程，如果目标函数再复杂一点，那么确定合适的学习率就会变得更加复杂。接下来将介绍一些不同的方法，它们各有其收敛行为。</p><p>对于梯度下降法的改进，有两种方案：梯度计算的变种和随机的变种。首先介绍第一类。</p><ul><li><p>牛顿法 Newton’s Method 牛顿发使用二次曲面对一个高维函数做近似，因此其收敛速度显著快于一阶逼近的梯度下降法。其迭代公式为：</p><p>θt+1=θt−α(∇θ2f(θt))−1∇θf(θt)θt+1​=θt​−α(∇θ2​f(θt​))−1∇θ​f(θt​)</p><p>其中，(∇θ2f(θt))−1(∇θ2​f(θt​))−1是_Hessian_矩阵的逆矩阵。_Hessian_矩阵每个元素都是二阶导数，其具体定义为：</p><p>$$∇θ2f(θt)=H=[∂2f∂x12∂2f∂x1∂x2⋯∂2f∂x1∂xn∂2f∂x2∂x1∂2f∂x22⋯∂2f∂x2∂xn⋱∂2f∂xn∂x1∂2f∂xn∂x2⋯∂2f∂xn2]∇θ2​f(θt​)=H=​∂x12​∂2f​∂x2​∂x1​∂2f​⋮∂xn​∂x1​∂2f​​∂x1​∂x2​∂2f​∂x22​∂2f​⋮∂xn​∂x2​∂2f​​⋯⋯⋱⋯​∂x1​∂xn​∂2f​∂x2​∂xn​∂2f​⋮∂xn2​∂$$2f​​​</p><p>对于二次函数，牛顿法可以一次给出指向最优点的方向</p></li></ul><p>这一方法广泛用于传统凸优化领域，但是很少用于深度学习优化。有两个主要原因：1) Hessian矩阵是n×nn×n的，因此参数量稍微大一点其计算代码都非常非常恐怖；2) 对于非凸优化，二阶方法是否更有效还有待商榷。</p><ul><li>动量梯度下降法 Momentum 在普通梯度下降法中，如果学习率太大，就会出现来回横跳的情况，如果对前几次梯度取平均，则可能改善这一情况。</li></ul><p>动量法正是对梯度取指数移动平均<a href=https://www.zhouxin.space/notes/notes-on-cmu-10-414-deep-learning-system/#fn:1>1</a>的方案，具体来说有：</p><p>ut+1=βut+(1−β)∇θf(θt)θt+1=θt−αut+1​ut+1​=βut​+(1−β)∇θ​f(θt​)θt+1​=θt​−αut+1​​</p><p>该方法可视化过程如下图所示，在较大学习率的情况下，其相比梯度下降法优化曲线更为平滑。<img alt=image.png loading=lazy src="https://pics.zhouxin.space/202406161114979.png?x-oss-process=image/quality,q_90/format,webp"></p><ul><li>无偏动量法 Unbiasing momentum 前一章节实际上有一个小瑕疵。如果u0u0​初始化为0，那么第一次进行更新是的梯度值是正常更新的(1−β)(1−β)倍，因此其前期的收敛过程会稍慢，随着迭代的进行，其效应会逐渐减弱。</li></ul><p>为了修正其影响，我们可以在参数更新过程中对动量进行缩放，具体来说：</p><p>θt+1=θt−αut+11−βt+1θt+1​=θt​−1−βt+1αut+1​​</p><p>如下图所示，修正以后其前期的更新速度要快了不少。<img alt=image.png loading=lazy src="https://pics.zhouxin.space/202406161128045.png?x-oss-process=image/quality,q_90/format,webp"></p><ul><li><p>Nesterov momentum Nesterov是梯度下降中一个非常有效的“trick”，其在传统momentum的基础上，将计算当前位置的梯度改为计算下一步位置的梯度。即：</p><p>ut+1=βut+(1−β)∇θf(θt−αut)ut+1​=βut​+(1−β)∇θ​f(θt​−αut​)</p><p>关于其为啥有效，看到了两篇文章。第一篇<a href=https://www.zhouxin.space/notes/notes-on-cmu-10-414-deep-learning-system/#fn:2>2</a>通过推导认为该方案对二阶导数进行了近似，因此其收敛速度更快；第二篇<a href=https://www.zhouxin.space/notes/notes-on-cmu-10-414-deep-learning-system/#fn:3>3</a>认为其能够更好地感知未来位置的梯度，在未来梯度很大时放慢步子。</p></li></ul><p>不看广告看疗效，对比普通Momentum，该方法的收敛速度要快得多。据说其也更适合一个深度网络。
<img alt=image.png loading=lazy src="https://pics.zhouxin.space/202406161306619.png?x-oss-process=image/quality,q_90/format,webp"></p><ul><li><p>Adam Adam是一种自适应的梯度下降算法。不同参数其对应的梯度之间的大小差异可能很大，Adam对此的解决方案是提供一个缩放因子，梯度值小则将其缩放得大一点，即：</p><p>ut+1=β1ut+(1−β1)∇θf(θt)vt+1=β2vt+(1−β2)(∇θf(θt))2平方为逐元素运算θt+1=θt−αut+1vt+1+ϵ所有元素均为逐元素运算​ut+1​=β1​ut​+(1−β1​)∇θ​f(θt​)vt+1​=β2​vt​+(1−β2​)(∇θ​f(θt​))2θt+1​=θt​−vt+1​​+ϵαut+1​​​平方为逐元素运算所有元素均为逐元素运算​</p><p>Adam在实践中得到了广泛应用，在特定任务上，其可能不是最佳的优化器（如下图），但在大部分任务上，其都能有不错的可以作为基线的表现。
<img alt=image.png|600 loading=lazy src="https://pics.zhouxin.space/202406161602224.png?x-oss-process=image/quality,q_90/format,webp"></p></li></ul><p>接下来将介绍随机变种。随机变种是在优化过程中加入了随机变量（噪声），例如每次使用数据集的一个子集对参数进行更新。</p><ul><li><p>随机梯度下降 Stochastic gradient descent 随机梯度下降正是每次使用数据集的一个子集对参数进行更新，即：</p><p>θt+1=θt−α∣B∣∑i∈B∇θl(hθ(x(i),yi))θt+1​=θt​−∣B∣α​i∈B∑​∇θ​l(hθ​(x(i),yi))</p></li></ul><p>看上去SGD的迭代次数比梯度下降要多得多，但是其每轮迭代的计算代价都要小的多，同时
<img alt=image.png|750 loading=lazy src="https://pics.zhouxin.space/202406161624584.png?x-oss-process=image/quality,q_90/format,webp"></p><p>尽管在凸优化上可视化训练过程给了很直观的感受，但需要注意的是，深度学习并不是凸优化或者二次函数，这些优化方法在深度学习上的应用与在凸优化上的效果可能完全不同。</p><h2 id=初始化>初始化<a hidden class=anchor aria-hidden=true href=#初始化>#</a></h2><p>参数的初始值如何确定？这是个好问题。</p><p>在凸优化中，尝尝将所有参数初始化为0，如果在神经网络中也这么做，那么每一层的输出都是0，求得的梯度也都是0🙁。全0是这个模型的一个不动点，模型将永远得不到更新。</p><ul><li><p>初始化参数对梯度的影响很大 一种自然的想法是对参数进行随机初始化，例如按照多元正态分布进行初始化。但是，分布中参数的选择对于梯度的影响可能会相当大，如下图所示：<img alt=image.png|600 loading=lazy src="https://pics.zhouxin.space/202406161659652.png?x-oss-process=image/quality,q_90/format,webp">随着层数的增加，如果激活值范数变化的太剧烈，会导致梯度爆炸或者消失问题，如果梯度值过大或者过小，也会导致这些问题。</p></li><li><p>权重的在训练过程的变化可能很小 可能存在这样一个误区：无论初始值如何选择，这些参数最终都会收敛到某个区域附近。事实并非如此，整个训练过程中权重的变化并非如此剧烈。</p></li><li><p>为什么2/n在前面是个合适的初始化参数 这里直接使用gpt对这页ppt的解释</p></li></ul><blockquote><p>考虑独立的随机变量 𝑥∼𝑁(0,1)x∼N(0,1) 和 𝑤∼𝑁(0,1𝑛)w∼N(0,n1​)，其中 𝑥x 是输入，𝑤w 是权重。</p><h4 id=期望和方差>期望和方差<a hidden class=anchor aria-hidden=true href=#期望和方差>#</a></h4><ul><li>𝐸[𝑥⋅𝑤𝑖]=0E[x⋅wi​]=0</li><li>Var[𝑥⋅𝑤𝑖]=1𝑛Var[x⋅wi​]=n1​</li></ul><p>因此，对于 𝑤𝑇𝑥wTx：</p><ul><li>𝐸[𝑤𝑇𝑥]=0E[wTx]=0</li><li>Var[𝑤𝑇𝑥]=1Var[wTx]=1（根据中心极限定理，𝑤𝑇𝑥wTx 服从 𝑁(0,1)N(0,1)）</li></ul><h3 id=激活值的方差>激活值的方差<a hidden class=anchor aria-hidden=true href=#激活值的方差>#</a></h3><p>如果使用线性激活函数，并且 𝑧𝑖∼𝑁(0,𝐼)zi​∼N(0,I)，则 𝑊𝑖∼𝑁(0,1𝑛𝐼)Wi​∼N(0,n1​I)，那么：</p><p>𝑧𝑖+1=𝑊𝑖𝑧𝑖zi+1​=Wi​zi​</p><h3 id=relu-非线性>ReLU 非线性<a hidden class=anchor aria-hidden=true href=#relu-非线性>#</a></h3><p>如果使用 ReLU 非线性激活函数，由于 ReLU 会将一半的 𝑧𝑖zi​ 分量设为零，因此为了达到相同的最终方差，需要将 𝑊𝑖Wi​ 的方差增加一倍。因此：</p><p>𝑊𝑖∼𝑁(0,2𝑛𝐼)Wi​∼N(0,n2​I)</p><p>这就是所谓的 Kaiming 正态初始化（He 初始化），它特别适用于 ReLU 激活函数。</p></blockquote><h1 id=lecture-7-neural-network-library-abstractions>Lecture 7: Neural Network Library Abstractions<a hidden class=anchor aria-hidden=true href=#lecture-7-neural-network-library-abstractions>#</a></h1><p>这节课主要介绍如何使用我们的needle库来实现一些简单的深度学习模型，构造一些小组件。</p><h2 id=程序抽象>程序抽象<a hidden class=anchor aria-hidden=true href=#程序抽象>#</a></h2><p>现代成熟的深度学习库提供了一些API，站在今天的视角，这些API都是都是恰到好处的。通过思考为什么要这样设计接口，可以让我们更好地理解深度学习库在进行程序抽象时的内部逻辑。</p><p>首先几个经典的深度学习框架进行分析，包括Caffe、TensorFlow和PyTorch。</p><ul><li>Caffe 1.0 （2014） 在Caffe中，使用Layer这一概念来表示神经网络中的一个个小模块，通过拼接和替换Layer，可以实现快速构造和修改神经网络，并使用同一套代码进行训练。</li></ul><p>Layer类提供了<code>forward</code>和<code>backward</code>两个接口：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>Layer</span><span class=p>:</span>
</span></span><span class=line><span class=cl>	<span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=n>bottom</span><span class=p>,</span> <span class=n>top</span><span class=p>):</span>
</span></span><span class=line><span class=cl>		<span class=k>pass</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>	<span class=k>def</span> <span class=nf>backward</span><span class=p>(</span><span class=n>top</span><span class=p>,</span> <span class=n>propagate_down</span><span class=p>,</span> <span class=n>bottom</span><span class=p>):</span>
</span></span><span class=line><span class=cl>		<span class=k>pass</span>
</span></span></code></pre></td></tr></table></div></div><p><code>forward</code>负责将来自bottom的数据进行前向传播，然后将数据保存到top中。在<code>backward</code>接口中，top保存来自输出的梯度，<code>propagate_down</code>用以指示是否要对其求梯度，bottom用于存放梯度。</p><p>在Caffe中，计算梯度是“就地”完成的，而非在计算图上新增额外的节点。作为第一代深度学习框架，直接计算梯度的思想是朴素但是符合直觉的。</p><ul><li>TensorFlow 1.0 （2015） 作为第二代深度学习框架，其在引入了计算图的概念。在计算图中，只要定义前向计算的计算方式，当需要计算梯度时，直接对计算图进行拓展即可。一个简短实例为：</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span><span class=lnt>9
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>tensorflow</span> <span class=k>as</span> <span class=nn>tf</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>v1</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>Variable</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>v2</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=n>v1</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>v3</span> <span class=o>=</span> <span class=n>v2</span> <span class=o>+</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl><span class=n>v4</span> <span class=o>=</span> <span class=n>v2</span> <span class=o>*</span> <span class=n>v3</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>sess</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>Session</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>value4</span> <span class=o>=</span> <span class=n>sess</span><span class=o>.</span><span class=n>run</span><span class=p>(</span><span class=n>v4</span><span class=p>,</span> <span class=n>feed_dict</span> <span class=o>=</span> <span class=p>{</span><span class=n>v1</span><span class=p>:</span> <span class=n>numpy</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=mi>1</span><span class=p>])})</span>
</span></span></code></pre></td></tr></table></div></div><p>以上代码<code>v1~4</code>仅仅是占位符，用于构建计算图，在没有输入传入前并没有值。通过会话来获取某个输入的情况下输出的值。</p><p>上述过程被称为声明式编程。即计算图在定义时并不会立即执行，而是等到会话（session）运行时才执行。这种方式的优点有：代码分区，可读性高；运行前计算图已知，可以针对性优化；通过会话便于实现分布式计算</p><ul><li>PyTorch (needle) PyTorch使用的是命令式编程，相比声明式编程，命令式编程在构建计算图时就已经指定其值。</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>needle</span> <span class=k>as</span> <span class=nn>ndl</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>v1</span> <span class=o>=</span> <span class=n>ndl</span><span class=o>.</span><span class=n>Tensor</span><span class=p>([</span><span class=mi>1</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>v2</span> <span class=o>=</span> <span class=n>ndl</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=n>v1</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>v3</span> <span class=o>=</span> <span class=n>v2</span> <span class=o>+</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl><span class=n>v4</span> <span class=o>=</span> <span class=n>v2</span> <span class=o>*</span> <span class=n>v3</span>
</span></span></code></pre></td></tr></table></div></div><p>命令式编程可以很方便地与Python原生控制流语句结合在一起，例如：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>if</span> <span class=n>v4</span><span class=o>.</span><span class=n>numpy</span><span class=p>()</span> <span class=o>&gt;</span> <span class=mf>0.5</span><span class=p>:</span>
</span></span><span class=line><span class=cl>	<span class=n>v5</span> <span class=o>=</span> <span class=n>v4</span> <span class=o>*</span> <span class=mi>2</span>
</span></span><span class=line><span class=cl><span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>	<span class=n>v5</span> <span class=o>=</span> <span class=n>v4</span>
</span></span></code></pre></td></tr></table></div></div><p>tf1.0的效率更高，适合推理和部署。PyTorch1.0则更适合开发和debug。</p><h2 id=高级模块化库组件>高级模块化库组件<a hidden class=anchor aria-hidden=true href=#高级模块化库组件>#</a></h2><p>如何使用深度学习库来实现深度学习呢？在hw1中我们使用一个个底层算子来搭建模型和实现训练过程，但这样开发太低效了。深度学习本身是很模块化的：由模型、损失函数和优化方法三部分组成。不但如此，模型本身也是高度模块化的。因此，我们在实现深度学习库时，必须精心设计好接口，以便支持该模块化的特性。</p><p>在PyTorch中，有一类叫做<code>nn.Module</code>，对应的就是模型中一个个小的子模块，其特点是以Tensor同时作为输入和输出。损失函数也满足这一特性，其可以被视为一个模块。</p><p>对于优化器，其作用是输入一个模型，对该模型中的参数按照某一规则进行更新。</p><p>为了防止过拟合，有些模型还具有正则项，其有两种实现方式：</p><ul><li>作为损失函数的一部分进行实现</li><li>直接整合进优化器中</li></ul><p>参数初始化同样很重要，其一般在构建<code>nn.Module</code>中指定。</p><p>数据加载也是一个很重要的模块。数据加载中还经常对数据进行预处理和增强。</p><p>各组件之间数据流图如下所示：<img alt=image.png loading=lazy src="https://pics.zhouxin.space/202406200916559.png?x-oss-process=image/quality,q_90/format,webp"></p><h1 id=lecture-8-neural-network-implementation>Lecture 8: Neural Network Implementation<a hidden class=anchor aria-hidden=true href=#lecture-8-neural-network-implementation>#</a></h1><h2 id=修改tensor的data域>修改Tensor的data域<a hidden class=anchor aria-hidden=true href=#修改tensor的data域>#</a></h2><p>在实现SGD时，由于存在多个batch，可能会在一个循环里对待学习参数进行更新，即：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>iterations</span><span class=p>):</span>
</span></span><span class=line><span class=cl>	<span class=n>w</span> <span class=o>-=</span> <span class=n>lr</span> <span class=o>*</span> <span class=n>grad</span>
</span></span></code></pre></td></tr></table></div></div><p>正如在<a href=https://www.zhouxin.space/notes/notes-on-cmu-10-414-assignments/#sgd-for-a-two-layer-neural-network>CMU 10-414 Assignments 实验笔记 > SGD for a two-layer neural network</a>踩过的坑那样，直接使用Tensor之间的算子进行参数更新会导致每次更新都会在计算图上增加一个新的节点w，这个节点具有Op和inputs，严重拖累反向传播速度。</p><p>为了避免每次更新参数时都在计算图上留下一个需要求梯度的节点，needle库提供了<code>Tensor.data()</code>方法，用于创建一个与<code>Tensor</code>共享同一个底层data的节点，但其不存在Op和inputs，也不用对其进行求导。</p><p>因此，可以使用<code>Tensor.data</code>方法，在不干扰计算图反向传播的前提下对参数进行正常的更新，即：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>w</span><span class=o>.</span><span class=n>data</span> <span class=o>-=</span> <span class=n>lr</span> <span class=o>*</span> <span class=n>grad</span><span class=o>.</span><span class=n>data</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=数值稳定性>数值稳定性<a hidden class=anchor aria-hidden=true href=#数值稳定性>#</a></h2><p>每个数值在内存中的存储空间都是有限的，因此保存的数值的范围和精度都是有限的，计算过程中难免出现溢出或者精度丢失的情况，在实现算子时，必须考虑到数值稳定性的问题。</p><p>例如，在softmax公式中，由于指数运算的存在，数值很有可能就上溢了，一个修正方式是在进行softmax运算前，每个元素都减去输入的最大值，以防止上溢。即：</p><p>zi=softmax(xi)=exp⁡(xi−c)∑kexp⁡(xk−c)zi​=softmax(xi​)=∑k​exp(xk​−c)exp(xi​−c)​</p><p>其中，c=max⁡(x)c=max(x)。</p><p>类似的，其它算子也要考虑相应的稳定性问题。</p><h2 id=parameter-类>Parameter 类<a hidden class=anchor aria-hidden=true href=#parameter-类>#</a></h2><p><code>Parameter</code>类用于表示可学习的参数，其是<code>Tensor</code>的子类。相比<code>Tensor</code>类，这个类不必再引入新的行为或者接口，因此其实现很简单：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>Parameter</span><span class=p>(</span><span class=n>ndl</span><span class=o>.</span><span class=n>Tensor</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;parameter&#34;&#34;&#34;</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=module-类>Module 类<a hidden class=anchor aria-hidden=true href=#module-类>#</a></h2><p><code>Module</code>类用于表示神经网络中一个个子模块。其具有如下接口：</p><ul><li><code>parameters</code>：获取模块中所有可学习参数</li><li><code>__call__</code>：进行前向传播 在实现时，定义了一个辅助函数<code>_get_params</code>用于提取一个模块中的所有可学习参数。</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>_get_params</span><span class=p>(</span><span class=n>value</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=nb>isinstance</span><span class=p>(</span><span class=n>value</span><span class=p>,</span> <span class=n>Parameter</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=p>[</span><span class=n>value</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=nb>isinstance</span><span class=p>(</span><span class=n>value</span><span class=p>,</span> <span class=nb>dict</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>params</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>k</span><span class=p>,</span> <span class=n>v</span> <span class=ow>in</span> <span class=n>value</span><span class=o>.</span><span class=n>items</span><span class=p>():</span>
</span></span><span class=line><span class=cl>            <span class=n>params</span> <span class=o>+=</span> <span class=n>_get_params</span><span class=p>(</span><span class=n>v</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>params</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=nb>isinstance</span><span class=p>(</span><span class=n>value</span><span class=p>,</span> <span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>value</span><span class=o>.</span><span class=n>parameters</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>Module</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>parameters</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>_get_params</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=vm>__dict__</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__call__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=o>*</span><span class=n>args</span><span class=p>,</span> <span class=o>**</span><span class=n>kwargs</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>forward</span><span class=p>(</span><span class=o>*</span><span class=n>args</span><span class=p>,</span> <span class=o>**</span><span class=n>kwargs</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=optimizer-类>Optimizer 类<a hidden class=anchor aria-hidden=true href=#optimizer-类>#</a></h3><p><code>Optimizer</code>类用于优化模型中可学习参数，其有两个关键接口：</p><ul><li><code>reset_grad</code>：重置模型中可学习参数的grad字段</li><li><code>step</code>：更新参数值 <code>reset_grad</code>实现比较简单，<code>step</code>方法则依赖于优化算法的具体实现：</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>Optimizer</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>params</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>params</span> <span class=o>=</span> <span class=n>params</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>reset_grad</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>p</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>params</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>p</span><span class=o>.</span><span class=n>grad</span> <span class=o>=</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>step</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>raise</span> <span class=bp>NotImplemented</span><span class=p>()</span>
</span></span></code></pre></td></tr></table></div></div><h1 id=lecture-9-normalization-and-regularization>Lecture 9: Normalization and Regularization<a hidden class=anchor aria-hidden=true href=#lecture-9-normalization-and-regularization>#</a></h1><h2 id=normalization>Normalization<a hidden class=anchor aria-hidden=true href=#normalization>#</a></h2><p>在前面几讲提到过，参数初始值的选择对于模型的训练很重要，不恰当的初始值参数会导致梯度消失或者爆炸💥。更重要的是，当训练完成后，这些梯度和参数值大小仍有初始值差不多，这更强调了初始值的重要性。<img alt=image.png|600 loading=lazy src="https://pics.zhouxin.space/202407051309612.png?x-oss-process=image/quality,q_90/format,webp"></p><p>为了修复这一问题，引入了layer normalization。其思想就是对激活层的输出进行标准化，即将输出减去期望后除以标准差：</p><p>$$z^i+1=σi(WiTzi+bi)zi+1=z^i+1−E(z^i+1)Var(z^i+1)+ϵ$$</p><p>上述技巧目前已经得到广泛应用，但在实践中，应用layer norm会导致模型难以收敛到一个很小的loss值。</p><p>另外一种技巧是batch norm。layer norm是对每一个sample（z的每一行）做归一化，而batch norm对每一列归一化。这一方法使得每个batch的所有样本都会对该batch中某个样本的推理结果有影响，因此在进行推理时，batch norm中的归一化的参数应该使用整个训练集上的参数，而非推理时输入样本的batch参数。</p><h2 id=regularization>Regularization<a hidden class=anchor aria-hidden=true href=#regularization>#</a></h2><p>正则化用于对抗过拟合，所谓过拟合是指模型在训练集上性能非常好，但在测试机上泛化性能很差。正则化就是限制参数复杂度的过程，可以分为显式正则和隐式正则。</p><p>隐式正则化是指现有算法或架构在不显式添加正则化项的情况下，自然地对函数类进行限制。具体来说，隐式正则化通过以下方式实现：</p><ul><li><strong>算法的固有特性</strong>：例如，随机梯度下降（SGD）等优化算法在训练过程中自带某些正则化效果。虽然我们并没有显式地优化所有可能的神经网络，而是通过SGD优化那些在特定权重初始化下的神经网络。这种优化过程本身对模型的复杂度进行了限制。</li><li><strong>架构的设计</strong>：某些网络架构设计本身就具有正则化效果。例如，卷积神经网络（CNN）的共享权重机制和局部连接特性，自然地减少了模型参数的数量，从而降低了模型复杂度。</li></ul><p>显式正则化指的是通过显式得修改模型使其能够避免对训练集过拟合。</p><p>一种最常见的应用于参数的正则化方案是l2正则化，即l2 regularization a.k.a weight decay。传统认为，模型参数值的大小可以在一定程度上指示出模型的复杂度，因此通过在优化目标中引入l2正则项来控制模型的大小。一般地，引入l2 regularization的机器学习优化问题可以表示为：</p><p>$$minimize1m∑iml(hw1:L(x(i),y(i)))+λ2∑i=1L∣∣wi∣∣F2$$</p><p>其中，∣∣wi∣∣F∣∣wi​∣∣F​是Frobenius范数，其表示矩阵每个元素的平方和的平方根。</p><p>得益于这里的系数是1/21/2，在对wiwi​求导时正则项恰好为λwiλwi​。梯度更新的公式相应变为：</p><p>Wi:=(1−αλ)Wi−α∇1mlWi​:=(1−αλ)Wi​−α∇m1​l</p><p>注意，引入l2正则化后，每轮迭代都会将参数缩小至原来的1−αλ1−αλ。很多地方不将l2正则化作为损失函数的一部分，而是将其作为优化器的一部分，即直接将参数进行缩小，这种方法被称为weight decay，显然二者是等价的。</p><p>另外一种正则化方法是dropout，其思想是在训练过程中随机地将一些激活层的输出置为0，并对其它输出放大，以确保整层输出的数学期望不变，形式化表示为：</p><p>z^i+1=σi(WiTzi)+bi(zi+1)j={((z^i+1)j)/(1−p)以概率1−p0以概率pz^i+1​(zi+1​)j​​=σi​(WiT​zi​)+bi​={((z^i+1​)j​)/(1−p)0​以概率1−p以概率p​​</p><p>在推理时，则不需要进行dropout。</p><p>直观地说，dropout能够提升模型在激活层部分缺失时进行推理的能力，但显然这一能力没什么卵用。另一种解释是dropout提升了模型训练过程中的随机性，类似SGD。</p><h1 id=lecture-10-convolutional-networks>Lecture 10: Convolutional Networks<a hidden class=anchor aria-hidden=true href=#lecture-10-convolutional-networks>#</a></h1><h2 id=convolutional-operators-in-deep-networks>Convolutional operators in deep networks<a hidden class=anchor aria-hidden=true href=#convolutional-operators-in-deep-networks>#</a></h2><p>在hw2中，我们通过flatten操作将图片视作一个序列进行计算，这对于小尺寸的图片是可行的，但对于大尺寸的图片，例如256×256的图片，将会导致输入异常庞大，网络也随之变大。这种简单粗暴的处理方式不利于提取图片的内在特征，例如，如果对图片进行平移，其输入序列的变化相当大。</p><p>卷积网络出于以下两个动机：</p><ul><li>层之间的激活以局部的方式发生，并且隐藏层的输出也被视为图像</li><li>在所有的空间位置共享权重</li></ul><p>卷积网络有以下两个优点：</p><ul><li>使用的参数很少。参数量由卷积网络的大小决定，而和输入的shape无关；</li><li>能够很好地捕获图片的内在不变形。</li></ul><p>卷积的计算示意如下图所示，卷积核在原图上滑动，从而产生一张新的图片。</p><p><img alt=image.png|550 loading=lazy src="https://pics.zhouxin.space/202407250959153.png?x-oss-process=image/quality,q_90/format,webp"></p><p>在深度学习中，输入和隐藏层都很少是一个1D的矩阵，一般而言，其是由多个通道的。例如，一张彩色图片由RGB三通道组成，而中间的隐藏层，通常会有比较大的通道数，如下图所示：</p><p><img alt=image.png|300 loading=lazy src="https://pics.zhouxin.space/202407251015471.png?x-oss-process=image/quality,q_90/format,webp"></p><p>记卷积层的输入x∈Rh×w×cinx∈Rh×w×cin​，输出z∈Rh×w×coutz∈Rh×w×cout​。从上图可以发现，卷积输出的某个通道，都是由输入在同一个局部的所有通道共同决定的，因此，卷积核W∈Rcin×cout×k×kW∈Rcin​×cout​×k×k，卷积过程可以形式化表示为：</p><p>$$z[:,:,s]=∑r=1cinx[:,:,r]⋅W[r,s,:,:]$$</p><p>关于多通道卷积，另外一种更符合直觉的理解是将相同位置的各通道的组合看作是一个向量，即下图中，xx每一格都是一个向量，WW每一格都是cout×cincout​×cin​的矩阵，卷积的输出由对应位置的zz和WW按矩阵乘法并求和得到。</p><p><img alt=image.png|650 loading=lazy src="https://pics.zhouxin.space/202407251027480.png?x-oss-process=image/quality,q_90/format,webp"></p><h2 id=elements-of-practical-convolutions>Elements of practical convolutions<a hidden class=anchor aria-hidden=true href=#elements-of-practical-convolutions>#</a></h2><p>在实际的卷积操作中，通常还会应用一些别的技术。</p><ul><li><p>Padding 原始的卷积操作，会将输出的长宽变小k−1k−1个长度，通过在周围填充(k−1)/2(k−1)/2个0元可以保证输出的shape与输入一致。为了避免两侧填充不一致这个别扭的情况，我们一般选取卷积核大小为奇数。</p></li><li><p>Strided Convolutions / Pooling 经过padding之后的卷积操作，不改变图片的shape，但在实际应用中，通常会对图片进行下采样。用两种解决方案：</p></li></ul><ol><li>使用最大/平均池化来聚合信息，例如，使用一个2×2的核进行池化操作，每次移动的步长为2，就可以将整张图片长宽各放缩至原来一半；</li><li>卷积操作时，卷积核移动的步长大于1。</li></ol><ul><li><p>Grouped Convolutions 当输入和输出的通道数很大时，卷积核的参数量仍可能非常非常大。一种解决方案是，使用分组卷积，即将输入通道分为多个组，每个组独立进行卷积操作，如下图所示。如果分为G组，则参数量可减少为原来的1/G。
<img alt=image.png|400 loading=lazy src="https://pics.zhouxin.space/202407251311275.png?x-oss-process=image/quality,q_90/format,webp"></p></li><li><p>Dilations 传统卷积的感受野和卷积核一样大，扩张卷积的思路是在卷积区域中插入间隔，能够扩大卷积核的感受野。下图表示的很形象。
<img alt=image.png|400 loading=lazy src="https://pics.zhouxin.space/202407251316286.png?x-oss-process=image/quality,q_90/format,webp"></p></li></ul><h2 id=differentiating-convolutions>Differentiating convolutions<a hidden class=anchor aria-hidden=true href=#differentiating-convolutions>#</a></h2><p>正如前文所提到的，我们可以通过一系列矩阵向量乘法和求和运算来实现卷积操作，但这么做效率太低了，我们的计算图上有很多中间节点，这些中间变量将消耗大量的内存空间。因此，我们不应该使用微分库中的算子来计算卷子，而是将其作为一个算子来实现，并手动计算其微分。</p><p>首先定义卷积操作：</p><p>z=conv⁡(x,W)z=conv(x,W)</p><p>zz的梯度怎么与adjoints乘呢？这是个问题。zz的梯度有以下二者：∂z∂x∂x∂z​和∂z∂W∂W∂z​，从形式上看，他们是3阶张量初以四阶张量，相当复杂。</p><p>首先考虑最简单的矩阵和向量相乘的情况，即：</p><p>z=Wxz=Wx</p><p>那么zz对xx的导数就是WW，即其与adjoint的乘法计算公式为：</p><p>WTvˉWTvˉ</p><p>也就是说如果在前向传播中我们计算一个矩阵和向量的乘积，那么在反向传播中，我们要计算这个矩阵的转置和adjoint的乘积。那对于卷积来说，它的“转置”是什么呢？</p><ul><li><p>将卷积视为矩阵运算I 以1d卷积为例，我们考虑如下的一个卷积运算，其中每个格子都是一个向量或者矩阵。
<img alt=image.png loading=lazy src="https://pics.zhouxin.space/202407251428228.png?x-oss-process=image/quality,q_90/format,webp">将上面这个矩阵运算展开，可以得到：</p><p>[z1z2z3z4z5]=x∗w=[w2w3000w1w2w3000w1w2w3000w1w2w3000w1w2][x1x2x3x4x5]​z1​z2​z3​z4​z5​​​=x∗w=​w2​w1​000​w3​w2​w1​00​0w3​w2​w1​0​00w3​w2​w1​​000w3​w2​​​​x1​x2​x3​x4​x5​​​</p><p>有了W^W^，我们可以很容易地写出W^TW^T,即：</p><p>W^T=[w2w1000w3w2w1000w3w2w1000w3w2w1000w3w2]W^T=​w2​w3​000​w1​w2​w3​00​0w1​w2​w3​0​00w1​w2​w3​​000w1​w2​​​</p><p>不难发现，这个算子实际上是[w3,w2,w1][w3​,w2​,w1​]这个卷积核，即原始卷积核翻转后的卷积核。也就是说，梯度和adjoint的乘积可以表示为：</p><p>v^∂conv⁡(x,w)∂x=conv⁡(v^,flip⁡(w))v^∂x∂conv(x,w)​=conv(v^,flip(w))</p></li><li><p>将卷积视为矩阵运算II 接下来我们考虑卷积对于参数ww的导数。同样，我们将矩阵运算展开，可以得到：</p><p>[z1z2z3z4z5]=x∗w=[0x1x2x1x2x3x2x3x4x3x4x5x4x50][w1w2w3]​z1​z2​z3​z4​z5​​​=x∗w=​0x1​x2​x3​x4​​x1​x2​x3​x4​x5​​x2​x3​x4​x5​0​​​w1​w2​w3​​​</p><p>相比矩阵运算I，我们构造出的X^X^矩阵是一个密集矩阵，在实现卷积算子时，我们常常采用这个方案来运算。这个X^X^矩阵被称为“im2col”矩阵（image to column）。</p></li></ul><h1 id=lecture-11-hardware-acceleration>Lecture 11: Hardware acceleration<a hidden class=anchor aria-hidden=true href=#lecture-11-hardware-acceleration>#</a></h1><h2 id=general-acceleration-techniques>General acceleration techniques<a hidden class=anchor aria-hidden=true href=#general-acceleration-techniques>#</a></h2><p>现代机器学习框架可以视为两层：上层是计算图，用于前向推理、自动微分和反向传播；下层是张量线性代数库，其负责底层的张量计算。在needle中，我们目前使用numpy作为线性代数库。本节我们将介绍一些常见的加速技术。</p><ul><li>Vectorization 向量化 如果我们要将两个256长度的array相加，一种标量的处理方式是256个元素逐个相加，但是很多硬件都提供了批量从内存读取、向量运算指令，即优化为如下代码：</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-C data-lang=C><span class=line><span class=cl><span class=kt>void</span> <span class=nf>vecadd</span><span class=p>(</span><span class=kt>float</span><span class=o>*</span> <span class=n>A</span><span class=p>,</span> <span class=kt>float</span><span class=o>*</span> <span class=n>B</span><span class=p>,</span> <span class=kt>float</span><span class=o>*</span> <span class=n>C</span><span class=p>){</span>
</span></span><span class=line><span class=cl>	<span class=k>for</span><span class=p>(</span><span class=kt>int</span> <span class=n>i</span><span class=o>=</span><span class=mi>0</span><span class=p>;</span> <span class=n>i</span><span class=o>&lt;</span><span class=mi>64</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>){</span>
</span></span><span class=line><span class=cl>		<span class=n>float4</span> <span class=n>a</span> <span class=o>=</span> <span class=nf>load_float4</span><span class=p>(</span><span class=n>A</span> <span class=o>+</span> <span class=n>i</span><span class=o>*</span><span class=mi>4</span><span class=p>);</span>
</span></span><span class=line><span class=cl>		<span class=n>float4</span> <span class=n>b</span> <span class=o>=</span> <span class=nf>load_float4</span><span class=p>(</span><span class=n>B</span> <span class=o>+</span> <span class=n>i</span><span class=o>*</span><span class=mi>4</span><span class=p>);</span>
</span></span><span class=line><span class=cl>		<span class=n>float4</span> <span class=n>c</span> <span class=o>=</span> <span class=nf>add_float4</span><span class=p>(</span><span class=n>a</span><span class=p>,</span> <span class=n>b</span><span class=p>);</span>
</span></span><span class=line><span class=cl>		<span class=nf>store_float4</span><span class=p>(</span><span class=n>C</span> <span class=o>+</span> <span class=n>i</span><span class=o>*</span><span class=mi>4</span><span class=p>,</span> <span class=n>c</span><span class=p>);</span>
</span></span><span class=line><span class=cl>	<span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><p>这里要求ABC所在的内存块要是按照128 bit对齐的。</p><ul><li>Data layout & strides 数据布局&步幅 在内存中，数据是线性排列的，因此一个矩阵在内存中有两种布局方式：行优先和列优先。一些古老的语言使用列优先，现代的语言偏向使用行优先。</li></ul><p>在许多库中，还引入了一种stride格式布局，即在保存张量时，额外保存一个数据，用于标识每个维度上需要移动的步长。在这种情况下，<code>a[i, j] = a_data[i * strides[0] + j * strides[1]]</code></p><p>这个方案可以在不用复制数据的情况下实现很多操作：通过改变offset和shape来实现切片；通过交换strides来实现转置；通过插入等于0的stride来实现广播。</p><p>其缺点是访存操作可能不再连续，因此向量化技术不可用，很多库也需要先把他们拼接之后再使用。</p><ul><li>Parallelization 并行化 使用openmp可以将计算分配给多个核并行处理：</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span><span class=lnt>9
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-C data-lang=C><span class=line><span class=cl><span class=kt>void</span> <span class=nf>vecadd</span><span class=p>(</span><span class=kt>float</span><span class=o>*</span> <span class=n>A</span><span class=p>,</span> <span class=kt>float</span><span class=o>*</span> <span class=n>B</span><span class=p>,</span> <span class=kt>float</span><span class=o>*</span> <span class=n>C</span><span class=p>){</span>
</span></span><span class=line><span class=cl>	<span class=cp>#pragma omp parallel for
</span></span></span><span class=line><span class=cl><span class=cp></span>	<span class=k>for</span><span class=p>(</span><span class=kt>int</span> <span class=n>i</span><span class=o>=</span><span class=mi>0</span><span class=p>;</span> <span class=n>i</span><span class=o>&lt;</span><span class=mi>64</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>){</span>
</span></span><span class=line><span class=cl>		<span class=n>float4</span> <span class=n>a</span> <span class=o>=</span> <span class=nf>load_float4</span><span class=p>(</span><span class=n>A</span> <span class=o>+</span> <span class=n>i</span><span class=o>*</span><span class=mi>4</span><span class=p>);</span>
</span></span><span class=line><span class=cl>		<span class=n>float4</span> <span class=n>b</span> <span class=o>=</span> <span class=nf>load_float4</span><span class=p>(</span><span class=n>B</span> <span class=o>+</span> <span class=n>i</span><span class=o>*</span><span class=mi>4</span><span class=p>);</span>
</span></span><span class=line><span class=cl>		<span class=n>float4</span> <span class=n>c</span> <span class=o>=</span> <span class=nf>add_float4</span><span class=p>(</span><span class=n>a</span><span class=p>,</span> <span class=n>b</span><span class=p>);</span>
</span></span><span class=line><span class=cl>		<span class=nf>store_float4</span><span class=p>(</span><span class=n>C</span> <span class=o>+</span> <span class=n>i</span><span class=o>*</span><span class=mi>4</span><span class=p>,</span> <span class=n>c</span><span class=p>);</span>
</span></span><span class=line><span class=cl>	<span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=case-study-matrix-multiplication>Case study: matrix multiplication<a hidden class=anchor aria-hidden=true href=#case-study-matrix-multiplication>#</a></h2><p>本节我们将讨论如何优化矩阵乘法。</p><ul><li>Vanilla matrix multiplication 朴素矩阵乘法 最朴素的想法是使用三重循环完成，其复杂度是O(n3)O(n3)，即如下代码：</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-c data-lang=c><span class=line><span class=cl><span class=kt>float</span> <span class=n>A</span><span class=p>[</span><span class=n>n</span><span class=p>][</span><span class=n>n</span><span class=p>],</span> <span class=n>B</span><span class=p>[</span><span class=n>n</span><span class=p>][</span><span class=n>n</span><span class=p>],</span> <span class=n>C</span><span class=p>[</span><span class=n>n</span><span class=p>][</span><span class=n>n</span><span class=p>];</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span><span class=p>(</span><span class=kt>int</span> <span class=n>i</span><span class=o>=</span><span class=mi>0</span><span class=p>;</span> <span class=n>i</span><span class=o>&lt;</span><span class=n>n</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>){</span>
</span></span><span class=line><span class=cl>	<span class=k>for</span><span class=p>(</span><span class=kt>int</span> <span class=n>j</span><span class=o>=</span><span class=mi>0</span><span class=p>;</span> <span class=n>j</span><span class=o>&lt;</span><span class=n>n</span><span class=p>;</span> <span class=n>j</span><span class=o>++</span><span class=p>){</span>
</span></span><span class=line><span class=cl>		<span class=n>c</span><span class=p>[</span><span class=n>i</span><span class=p>][</span><span class=n>j</span><span class=p>]</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span>
</span></span><span class=line><span class=cl>		<span class=k>for</span><span class=p>(</span><span class=kt>int</span> <span class=n>k</span><span class=o>=</span><span class=mi>0</span><span class=p>;</span> <span class=n>k</span><span class=o>&lt;</span><span class=n>n</span><span class=p>;</span> <span class=n>k</span><span class=o>++</span><span class=p>){</span>
</span></span><span class=line><span class=cl>		<span class=n>c</span><span class=p>[</span><span class=n>i</span><span class=p>][</span><span class=n>j</span><span class=p>]</span> <span class=o>+=</span> <span class=n>A</span><span class=p>[</span><span class=n>i</span><span class=p>][</span><span class=n>k</span><span class=p>]</span> <span class=o>*</span> <span class=n>B</span><span class=p>[</span><span class=n>k</span><span class=p>][</span><span class=n>j</span><span class=p>];</span>
</span></span><span class=line><span class=cl>		<span class=p>}</span>
</span></span><span class=line><span class=cl>	<span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><p>在现代存储器中，L1 cache的速度比DRAM快200倍，通过优化数据的读取就可以显著提升计算速度，考虑到这一点，我们可以将中间变量保存到寄存器中，即：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-c data-lang=c><span class=line><span class=cl><span class=n>dram</span> <span class=kt>float</span> <span class=n>A</span><span class=p>[</span><span class=n>n</span><span class=p>][</span><span class=n>n</span><span class=p>],</span> <span class=n>B</span><span class=p>[</span><span class=n>n</span><span class=p>][</span><span class=n>n</span><span class=p>],</span> <span class=n>C</span><span class=p>[</span><span class=n>n</span><span class=p>][</span><span class=n>n</span><span class=p>];</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span><span class=p>(</span><span class=kt>int</span> <span class=n>i</span><span class=o>=</span><span class=mi>0</span><span class=p>;</span> <span class=n>i</span><span class=o>&lt;</span><span class=n>n</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>){</span>
</span></span><span class=line><span class=cl>	<span class=k>for</span><span class=p>(</span><span class=kt>int</span> <span class=n>j</span><span class=o>=</span><span class=mi>0</span><span class=p>;</span> <span class=n>j</span><span class=o>&lt;</span><span class=n>n</span><span class=p>;</span> <span class=n>j</span><span class=o>++</span><span class=p>){</span>
</span></span><span class=line><span class=cl>		<span class=k>register</span> <span class=kt>float</span> <span class=n>c</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span>
</span></span><span class=line><span class=cl>		<span class=k>for</span><span class=p>(</span><span class=kt>int</span> <span class=n>k</span><span class=o>=</span><span class=mi>0</span><span class=p>;</span> <span class=n>k</span><span class=o>&lt;</span><span class=n>n</span><span class=p>;</span> <span class=n>k</span><span class=o>++</span><span class=p>){</span>
</span></span><span class=line><span class=cl>		<span class=k>register</span> <span class=kt>float</span> <span class=n>a</span> <span class=o>=</span> <span class=n>A</span><span class=p>[</span><span class=n>i</span><span class=p>][</span><span class=n>k</span><span class=p>];</span>
</span></span><span class=line><span class=cl>		<span class=k>register</span> <span class=kt>float</span> <span class=n>b</span> <span class=o>=</span> <span class=n>B</span><span class=p>[</span><span class=n>k</span><span class=p>][</span><span class=n>j</span><span class=p>];</span>
</span></span><span class=line><span class=cl>		<span class=n>c</span> <span class=o>+=</span> <span class=n>a</span><span class=o>*</span><span class=n>b</span><span class=p>;</span>
</span></span><span class=line><span class=cl>		<span class=p>}</span>
</span></span><span class=line><span class=cl>		<span class=n>C</span><span class=p>[</span><span class=n>i</span><span class=p>][</span><span class=n>j</span><span class=p>]</span> <span class=o>=</span> <span class=n>c</span><span class=p>;</span>
</span></span><span class=line><span class=cl>	<span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><p>上述代码中，从读取A、B到寄存器的操作分别进行了n3n3次，需要3个寄存器来完成该操作。</p><ul><li>Register tiled matrix multiplication 寄存器分块矩阵乘法 该方案的思路是将结果进行分块，每次计算其中的一块，即：</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-c data-lang=c><span class=line><span class=cl><span class=n>dram</span> <span class=kt>float</span> <span class=n>A</span><span class=p>[</span><span class=n>n</span><span class=o>/</span><span class=n>v1</span><span class=p>][</span><span class=n>n</span><span class=o>/</span><span class=n>v3</span><span class=p>][</span><span class=n>v1</span><span class=p>][</span><span class=n>v3</span><span class=p>];</span>
</span></span><span class=line><span class=cl><span class=n>dram</span> <span class=kt>float</span> <span class=n>B</span><span class=p>[</span><span class=n>n</span><span class=o>/</span><span class=n>v2</span><span class=p>][</span><span class=n>n</span><span class=o>/</span><span class=n>v3</span><span class=p>][</span><span class=n>v2</span><span class=p>][</span><span class=n>v3</span><span class=p>];</span>
</span></span><span class=line><span class=cl><span class=n>dram</span> <span class=kt>float</span> <span class=n>C</span><span class=p>[</span><span class=n>n</span><span class=o>/</span><span class=n>v1</span><span class=p>][</span><span class=n>n</span><span class=o>/</span><span class=n>v2</span><span class=p>][</span><span class=n>v1</span><span class=p>][</span><span class=n>v2</span><span class=p>];</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>n</span><span class=o>/</span><span class=n>v1</span><span class=p>;</span> <span class=o>++</span><span class=n>i</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>j</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>j</span> <span class=o>&lt;</span> <span class=n>n</span><span class=o>/</span><span class=n>v2</span><span class=p>;</span> <span class=o>++</span><span class=n>j</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=k>register</span> <span class=kt>float</span> <span class=n>c</span><span class=p>[</span><span class=n>v1</span><span class=p>][</span><span class=n>v2</span><span class=p>]</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>k</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>k</span> <span class=o>&lt;</span> <span class=n>n</span><span class=o>/</span><span class=n>v3</span><span class=p>;</span> <span class=o>++</span><span class=n>k</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=k>register</span> <span class=kt>float</span> <span class=n>a</span><span class=p>[</span><span class=n>v1</span><span class=p>][</span><span class=n>v3</span><span class=p>]</span> <span class=o>=</span> <span class=n>A</span><span class=p>[</span><span class=n>i</span><span class=p>][</span><span class=n>k</span><span class=p>];</span>
</span></span><span class=line><span class=cl>            <span class=k>register</span> <span class=kt>float</span> <span class=n>b</span><span class=p>[</span><span class=n>v2</span><span class=p>][</span><span class=n>v3</span><span class=p>]</span> <span class=o>=</span> <span class=n>B</span><span class=p>[</span><span class=n>j</span><span class=p>][</span><span class=n>k</span><span class=p>];</span>
</span></span><span class=line><span class=cl>            <span class=n>c</span> <span class=o>+=</span> <span class=nf>dot</span><span class=p>(</span><span class=n>a</span><span class=p>,</span> <span class=n>b</span><span class=p>.</span><span class=n>T</span><span class=p>);</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>        <span class=n>C</span><span class=p>[</span><span class=n>i</span><span class=p>][</span><span class=n>j</span><span class=p>]</span> <span class=o>=</span> <span class=n>c</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><p>上述代码中，要计算的矩阵C被分为v1×v2v1​×v2​的小矩阵，为了计算出每一块，每次必须从A中选出v1v1​行，从B中选出v2v2​列，这两组子矩阵可以按照长度v3v3​再次划分。在计算中，前两个循环依次遍历C中的一小块，然后初始化v1×v2v1​×v2​个寄存器用于保存该块内容，然后再根据v3v3​的大小二次划分，进行矩阵运算，将这些结果加到对应的寄存器上，第三个循环结束后就计算出C的一个子块。</p><p>A的数据加载开销是n3/v2n3/v2​，B的数据加载开销是n3/v1n3/v1​，A的寄存器开销是v1×v3v1​×v3​，B的寄存器开销是v2×v3v2​×v3​，C的寄存器开销是v1×v2v1​×v2​。注意到v3v3​不影响数据加载的开销，因此可以取v3v3​为1，然后在满足寄存器总数约束的情况下，最大化v1v1​和v2v2​。</p><p>之所以能够减小开销是因为在矩阵计算中，元素被重复使用，通过每次计算一个分块的方式，可以保证这个分块内用到的重复数据只要加载一次。</p><ul><li>Cache line aware tiling 缓存行感知分块 前面我们使用寄存器来进行加速，本节我们考虑使用cache来加速。我们的实现代码为：</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-c data-lang=c><span class=line><span class=cl><span class=n>dram</span> <span class=kt>float</span> <span class=n>A</span><span class=p>[</span><span class=n>n</span><span class=o>/</span><span class=n>b1</span><span class=p>][</span><span class=n>b1</span><span class=p>][</span><span class=n>n</span><span class=p>];</span>
</span></span><span class=line><span class=cl><span class=n>dram</span> <span class=kt>float</span> <span class=n>B</span><span class=p>[</span><span class=n>n</span><span class=o>/</span><span class=n>b2</span><span class=p>][</span><span class=n>b2</span><span class=p>][</span><span class=n>n</span><span class=p>];</span>
</span></span><span class=line><span class=cl><span class=n>dram</span> <span class=kt>float</span> <span class=n>C</span><span class=p>[</span><span class=n>n</span><span class=o>/</span><span class=n>b1</span><span class=p>][</span><span class=n>n</span><span class=o>/</span><span class=n>b2</span><span class=p>][</span><span class=n>b1</span><span class=p>][</span><span class=n>b2</span><span class=p>];</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>n</span><span class=o>/</span><span class=n>b1</span><span class=p>;</span> <span class=o>++</span><span class=n>i</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>l1cache</span> <span class=kt>float</span> <span class=n>a</span><span class=p>[</span><span class=n>b1</span><span class=p>][</span><span class=n>n</span><span class=p>]</span> <span class=o>=</span> <span class=n>A</span><span class=p>[</span><span class=n>i</span><span class=p>];</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>j</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>j</span> <span class=o>&lt;</span> <span class=n>n</span><span class=o>/</span><span class=n>b2</span><span class=p>;</span> <span class=o>++</span><span class=n>j</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=n>l1cache</span> <span class=kt>float</span> <span class=n>b</span><span class=p>[</span><span class=n>b2</span><span class=p>][</span><span class=n>n</span><span class=p>]</span> <span class=o>=</span> <span class=n>B</span><span class=p>[</span><span class=n>j</span><span class=p>];</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=n>C</span><span class=p>[</span><span class=n>i</span><span class=p>][</span><span class=n>j</span><span class=p>]</span> <span class=o>=</span> <span class=nf>dot</span><span class=p>(</span><span class=n>a</span><span class=p>,</span> <span class=n>b</span><span class=p>.</span><span class=n>T</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><p>上述代码中，结果矩阵C被分块为b1×b2b1​×b2​，A和B分别按行和按列分块，通过两层循环遍历计算C中的每个子块，计算子块的过程可以使用寄存器分块进行加速。</p><p>上述代码中，A的加载开销是n2n2，B的加载开销是n3/b1n3/b1。有两个约束，一个是b1n+b2n&lt;l1 chche sizeb1​n+b2​n&lt;l1 chche size，另一个是b1b1​。</p><ul><li>Put it together 将缓存版本的<code>dot</code>运算使用寄存器版本展开，可以得到最终的分块乘法实现：</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-c data-lang=c><span class=line><span class=cl><span class=n>dram</span> <span class=kt>float</span> <span class=n>A</span><span class=p>[</span><span class=n>n</span><span class=o>/</span><span class=n>b1</span><span class=p>][</span><span class=n>b1</span><span class=o>/</span><span class=n>v1</span><span class=p>][</span><span class=n>n</span><span class=p>][</span><span class=n>v1</span><span class=p>];</span>
</span></span><span class=line><span class=cl><span class=n>dram</span> <span class=kt>float</span> <span class=n>B</span><span class=p>[</span><span class=n>n</span><span class=o>/</span><span class=n>b2</span><span class=p>][</span><span class=n>b2</span><span class=o>/</span><span class=n>v2</span><span class=p>][</span><span class=n>n</span><span class=p>][</span><span class=n>v2</span><span class=p>];</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>n</span><span class=o>/</span><span class=n>b1</span><span class=p>;</span> <span class=o>++</span><span class=n>i</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>l1cache</span> <span class=kt>float</span> <span class=n>a</span><span class=p>[</span><span class=n>b1</span><span class=o>/</span><span class=n>v1</span><span class=p>][</span><span class=n>n</span><span class=p>][</span><span class=n>v1</span><span class=p>]</span> <span class=o>=</span> <span class=n>A</span><span class=p>[</span><span class=n>i</span><span class=p>];</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>j</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>j</span> <span class=o>&lt;</span> <span class=n>n</span><span class=o>/</span><span class=n>b2</span><span class=p>;</span> <span class=o>++</span><span class=n>j</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=n>l1cache</span> <span class=n>b</span><span class=p>[</span><span class=n>b2</span><span class=o>/</span><span class=n>v2</span><span class=p>][</span><span class=n>n</span><span class=p>][</span><span class=n>v2</span><span class=p>]</span> <span class=o>=</span> <span class=n>B</span><span class=p>[</span><span class=n>j</span><span class=p>];</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>x</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>x</span> <span class=o>&lt;</span> <span class=n>b1</span><span class=o>/</span><span class=n>v1</span><span class=p>;</span> <span class=o>++</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>y</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>y</span> <span class=o>&lt;</span> <span class=n>b2</span><span class=o>/</span><span class=n>v2</span><span class=p>;</span> <span class=o>++</span><span class=n>y</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>                <span class=k>register</span> <span class=kt>float</span> <span class=n>c</span><span class=p>[</span><span class=n>v1</span><span class=p>][</span><span class=n>v2</span><span class=p>]</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span>
</span></span><span class=line><span class=cl>                <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>k</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>k</span> <span class=o>&lt;</span> <span class=n>n</span><span class=p>;</span> <span class=o>++</span><span class=n>k</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>                    <span class=k>register</span> <span class=kt>float</span> <span class=n>ar</span><span class=p>[</span><span class=n>v1</span><span class=p>]</span> <span class=o>=</span> <span class=n>a</span><span class=p>[</span><span class=n>x</span><span class=p>][</span><span class=n>k</span><span class=p>][</span><span class=o>:</span><span class=p>];</span>
</span></span><span class=line><span class=cl>                    <span class=k>register</span> <span class=kt>float</span> <span class=n>br</span><span class=p>[</span><span class=n>v2</span><span class=p>]</span> <span class=o>=</span> <span class=n>b</span><span class=p>[</span><span class=n>y</span><span class=p>][</span><span class=n>k</span><span class=p>][</span><span class=o>:</span><span class=p>];</span>
</span></span><span class=line><span class=cl>                    <span class=n>C</span> <span class=o>+=</span> <span class=nf>dot</span><span class=p>(</span><span class=n>ar</span><span class=p>,</span> <span class=n>br</span><span class=p>.</span><span class=n>T</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=p>}</span>
</span></span><span class=line><span class=cl>            <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><p>上述代码的数据加载开销是：</p><p>$$speedl1⋅(n3v2+n3v1)+speeddram⋅(n2+n3b1)$$</p><h1 id=lecture-12-gpu-acceleration>Lecture 12: GPU acceleration<a hidden class=anchor aria-hidden=true href=#lecture-12-gpu-acceleration>#</a></h1><h2 id=gpu-programming>GPU programming<a hidden class=anchor aria-hidden=true href=#gpu-programming>#</a></h2><p>如下图所示，CPU是一种通用处理器，其可以灵活地处理不同的任务，每个核都有独立的控制器。但在某些任务，例如图形渲染中，可能存在大量的重复工作，例如给每个像素都进行相同的处理。GPU正是擅长处理此类任务，其有大量的执行单元，可以批量执行同一指令。将GPU应用于深度学习，可以带来10X ~ 100X的加速倍率。
<img alt=image.png|450 loading=lazy src="https://pics.zhouxin.space/202407260953795.png?x-oss-process=image/quality,q_90/format,webp"></p><ul><li>GPU programming model: SIMT 在本章节，我们将使用CUDA中的术语，但是在别的模型中，通常也有对应的概念。</li></ul><p>SIMT中所有的线程都执行相同的指令，但是具有不同的数据通路。线程被分组为block，每个block共享内存。block被分组为launch grid，当启动一个kernel时，实际上就是在一个grid上执行。</p><ul><li>Example: vector add 以下代码演示了在CPU和GPU上执行向量加法的过程：</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-c data-lang=c><span class=line><span class=cl><span class=kt>void</span> <span class=nf>VecAddCPU</span><span class=p>(</span><span class=kt>float</span><span class=o>*</span> <span class=n>A</span><span class=p>,</span> <span class=kt>float</span> <span class=o>*</span><span class=n>B</span><span class=p>,</span> <span class=kt>float</span><span class=o>*</span> <span class=n>C</span><span class=p>,</span> <span class=kt>int</span> <span class=n>n</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>n</span><span class=p>;</span> <span class=o>++</span><span class=n>i</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=n>C</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>=</span> <span class=n>A</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>+</span> <span class=n>B</span><span class=p>[</span><span class=n>i</span><span class=p>];</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>__global__</span> <span class=kt>void</span> <span class=nf>VecAddKernel</span><span class=p>(</span><span class=kt>float</span><span class=o>*</span> <span class=n>A</span><span class=p>,</span> <span class=kt>float</span> <span class=o>*</span><span class=n>B</span><span class=p>,</span> <span class=kt>float</span><span class=o>*</span> <span class=n>C</span><span class=p>,</span> <span class=kt>int</span> <span class=n>n</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=n>blockDim</span><span class=p>.</span><span class=n>x</span> <span class=o>*</span> <span class=n>blockIdx</span><span class=p>.</span><span class=n>x</span> <span class=o>+</span> <span class=n>threadIdx</span><span class=p>.</span><span class=n>x</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=p>(</span><span class=n>i</span> <span class=o>&lt;</span> <span class=n>n</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=n>C</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>=</span> <span class=n>A</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>+</span> <span class=n>B</span><span class=p>[</span><span class=n>i</span><span class=p>];</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><p>从GPU版本我们可以看到，每个线程执行的指令都是相同，不同的是每个线程具有不同的环境变量。</p><p>为了执行上述GPU代码，在主机端要执行以下内容：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-c data-lang=c><span class=line><span class=cl><span class=kt>void</span> <span class=nf>VecAddCUDA</span><span class=p>(</span><span class=kt>float</span> <span class=o>*</span><span class=n>Acpu</span><span class=p>,</span> <span class=kt>float</span> <span class=o>*</span><span class=n>Bcpu</span><span class=p>,</span> <span class=kt>float</span> <span class=o>*</span><span class=n>Ccpu</span><span class=p>,</span> <span class=kt>int</span> <span class=n>n</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=kt>float</span> <span class=o>*</span><span class=n>dA</span><span class=p>,</span> <span class=o>*</span><span class=n>dB</span><span class=p>,</span> <span class=o>*</span><span class=n>dC</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=nf>cudaMalloc</span><span class=p>(</span><span class=o>&amp;</span><span class=n>dA</span><span class=p>,</span> <span class=n>n</span> <span class=o>*</span> <span class=k>sizeof</span><span class=p>(</span><span class=kt>float</span><span class=p>));</span>
</span></span><span class=line><span class=cl>    <span class=nf>cudaMalloc</span><span class=p>(</span><span class=o>&amp;</span><span class=n>dB</span><span class=p>,</span> <span class=n>n</span> <span class=o>*</span> <span class=k>sizeof</span><span class=p>(</span><span class=kt>float</span><span class=p>));</span>
</span></span><span class=line><span class=cl>    <span class=nf>cudaMalloc</span><span class=p>(</span><span class=o>&amp;</span><span class=n>dC</span><span class=p>,</span> <span class=n>n</span> <span class=o>*</span> <span class=k>sizeof</span><span class=p>(</span><span class=kt>float</span><span class=p>));</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nf>cudaMemcpy</span><span class=p>(</span><span class=n>dA</span><span class=p>,</span> <span class=n>Acpu</span><span class=p>,</span> <span class=n>n</span> <span class=o>*</span> <span class=k>sizeof</span><span class=p>(</span><span class=kt>float</span><span class=p>),</span> <span class=n>cudaMemcpyHostToDevice</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=nf>cudaMemcpy</span><span class=p>(</span><span class=n>dB</span><span class=p>,</span> <span class=n>Bcpu</span><span class=p>,</span> <span class=n>n</span> <span class=o>*</span> <span class=k>sizeof</span><span class=p>(</span><span class=kt>float</span><span class=p>),</span> <span class=n>cudaMemcpyHostToDevice</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>threads_per_block</span> <span class=o>=</span> <span class=mi>512</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>nblocks</span> <span class=o>=</span> <span class=p>(</span><span class=n>n</span> <span class=o>+</span> <span class=n>threads_per_block</span> <span class=o>-</span> <span class=mi>1</span><span class=p>)</span> <span class=o>/</span> <span class=n>threads_per_block</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=n>VecAddKernel</span><span class=o>&lt;&lt;&lt;</span><span class=n>nblocks</span><span class=p>,</span> <span class=n>threads_per_block</span><span class=o>&gt;&gt;&gt;</span><span class=p>(</span><span class=n>dA</span><span class=p>,</span> <span class=n>dB</span><span class=p>,</span> <span class=n>dC</span><span class=p>,</span> <span class=n>n</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nf>cudaMemcpy</span><span class=p>(</span><span class=n>Ccpu</span><span class=p>,</span> <span class=n>dC</span><span class=p>,</span> <span class=n>n</span> <span class=o>*</span> <span class=k>sizeof</span><span class=p>(</span><span class=kt>float</span><span class=p>),</span> <span class=n>cudaMemcpyDeviceToHost</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nf>cudaFree</span><span class=p>(</span><span class=n>dA</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=nf>cudaFree</span><span class=p>(</span><span class=n>dB</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=nf>cudaFree</span><span class=p>(</span><span class=n>dC</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><p>函数的输入是来自cpu内存上的三个数组，在GPU上分配出对应大小的显存，然后将两个加数拷贝到设备中。根据数据的规模确定要启用的block数量，然后执行GPU代码，最后将结果拷贝会CPU内存并释放相应显存。</p><p>在实际中，内存拷贝是一个非常耗时的过程，因此我们希望将数据一直保留在显存中进行计算，而非频繁地来回拷贝。</p><ul><li>Example: window sum window sum是一种权重全为1的卷积，一种朴素的想法是这么些的：</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-c data-lang=c><span class=line><span class=cl><span class=cp>#define RADIUS 2
</span></span></span><span class=line><span class=cl><span class=cp></span>
</span></span><span class=line><span class=cl><span class=n>__global__</span> <span class=kt>void</span> <span class=nf>WindowSumSimpleKernel</span><span class=p>(</span><span class=kt>float</span><span class=o>*</span> <span class=n>A</span><span class=p>,</span> <span class=kt>float</span> <span class=o>*</span><span class=n>B</span><span class=p>,</span> <span class=kt>int</span> <span class=n>n</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>out_idx</span> <span class=o>=</span> <span class=n>blockDim</span><span class=p>.</span><span class=n>x</span> <span class=o>*</span> <span class=n>blockIdx</span><span class=p>.</span><span class=n>x</span> <span class=o>+</span> <span class=n>threadIdx</span><span class=p>.</span><span class=n>x</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=p>(</span><span class=n>out_idx</span> <span class=o>&lt;</span> <span class=n>n</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=kt>float</span> <span class=n>sum</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>dx</span> <span class=o>=</span> <span class=o>-</span><span class=n>RADIUS</span><span class=p>;</span> <span class=n>dx</span> <span class=o>&lt;=</span> <span class=n>RADIUS</span><span class=p>;</span> <span class=o>++</span><span class=n>dx</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=n>sum</span> <span class=o>+=</span> <span class=n>A</span><span class=p>[</span><span class=n>dx</span> <span class=o>+</span> <span class=n>out_idx</span> <span class=o>+</span> <span class=n>RADIUS</span><span class=p>];</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>        <span class=n>B</span><span class=p>[</span><span class=n>out_idx</span><span class=p>]</span> <span class=o>=</span> <span class=n>sum</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><p>但显然，这个算法并不高效，将重复访问数据，要加载5n5n次数据。</p><p>这时候可以引入共享内存进行优化，将一个block内要要用到的数据全部读取到共享内存中。数据加载的任务可以分给每个线程并行完成，显著降低了内存加载时间开销。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-C data-lang=C><span class=line><span class=cl><span class=n>__global__</span> <span class=kt>void</span> <span class=nf>WindowSumSharedKernel</span><span class=p>(</span><span class=kt>float</span><span class=o>*</span> <span class=n>A</span><span class=p>,</span> <span class=kt>float</span><span class=o>*</span> <span class=n>B</span><span class=p>,</span> <span class=kt>int</span> <span class=n>n</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>__shared__</span> <span class=kt>float</span> <span class=n>temp</span><span class=p>[</span><span class=n>THREADS_PER_BLOCK</span> <span class=o>+</span> <span class=mi>2</span> <span class=o>*</span> <span class=n>RADIUS</span><span class=p>];</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>base</span> <span class=o>=</span> <span class=n>blockDim</span><span class=p>.</span><span class=n>x</span> <span class=o>*</span> <span class=n>blockIdx</span><span class=p>.</span><span class=n>x</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>out_idx</span> <span class=o>=</span> <span class=n>base</span> <span class=o>+</span> <span class=n>threadIdx</span><span class=p>.</span><span class=n>x</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=p>(</span><span class=n>base</span> <span class=o>+</span> <span class=n>threadIdx</span><span class=p>.</span><span class=n>x</span> <span class=o>&lt;</span> <span class=n>n</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=n>temp</span><span class=p>[</span><span class=n>threadIdx</span><span class=p>.</span><span class=n>x</span><span class=p>]</span> <span class=o>=</span> <span class=n>A</span><span class=p>[</span><span class=n>base</span> <span class=o>+</span> <span class=n>threadIdx</span><span class=p>.</span><span class=n>x</span><span class=p>];</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=p>(</span><span class=n>threadIdx</span><span class=p>.</span><span class=n>x</span> <span class=o>&lt;</span> <span class=mi>2</span> <span class=o>*</span> <span class=n>RADIUS</span> <span class=o>&amp;&amp;</span> <span class=n>base</span> <span class=o>+</span> <span class=n>THREADS_PER_BLOCK</span> <span class=o>+</span> <span class=n>threadIdx</span><span class=p>.</span><span class=n>x</span> <span class=o>&lt;</span> <span class=n>n</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=n>temp</span><span class=p>[</span><span class=n>threadIdx</span><span class=p>.</span><span class=n>x</span> <span class=o>+</span> <span class=n>THREADS_PER_BLOCK</span><span class=p>]</span> <span class=o>=</span> <span class=n>A</span><span class=p>[</span><span class=n>base</span> <span class=o>+</span> <span class=n>THREADS_PER_BLOCK</span> <span class=o>+</span> <span class=n>threadIdx</span><span class=p>.</span><span class=n>x</span><span class=p>];</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=nf>__syncthreads</span><span class=p>();</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=p>(</span><span class=n>out_idx</span> <span class=o>&lt;</span> <span class=n>n</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=kt>float</span> <span class=n>sum</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>dx</span> <span class=o>=</span> <span class=o>-</span><span class=n>RADIUS</span><span class=p>;</span> <span class=n>dx</span> <span class=o>&lt;=</span> <span class=n>RADIUS</span><span class=p>;</span> <span class=o>++</span><span class=n>dx</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=n>sum</span> <span class=o>+=</span> <span class=n>temp</span><span class=p>[</span><span class=n>threadIdx</span><span class=p>.</span><span class=n>x</span> <span class=o>+</span> <span class=n>dx</span> <span class=o>+</span> <span class=n>RADIUS</span><span class=p>];</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>        <span class=n>B</span><span class=p>[</span><span class=n>out_idx</span><span class=p>]</span> <span class=o>=</span> <span class=n>sum</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><p>通过<code>__syncthreads</code>同步，确保所有线程都将数据加载完毕，然后再计算window sum。</p><h2 id=case-study-matrix-multiplication-on-gpu>Case study: matrix multiplication on GPU<a hidden class=anchor aria-hidden=true href=#case-study-matrix-multiplication-on-gpu>#</a></h2><p>从线程的细粒度来说，我们可以在GPU上实现一个寄存器分块版本的矩阵乘法：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-c data-lang=c><span class=line><span class=cl><span class=n>__global__</span> <span class=kt>void</span> <span class=nf>mm</span><span class=p>(</span><span class=kt>float</span> <span class=n>A</span><span class=p>[</span><span class=n>N</span><span class=p>][</span><span class=n>N</span><span class=p>],</span> <span class=kt>float</span> <span class=n>B</span><span class=p>[</span><span class=n>N</span><span class=p>][</span><span class=n>N</span><span class=p>],</span> <span class=kt>float</span> <span class=n>C</span><span class=p>[</span><span class=n>N</span><span class=p>][</span><span class=n>N</span><span class=p>])</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>ybase</span> <span class=o>=</span> <span class=n>blockIdx</span><span class=p>.</span><span class=n>y</span> <span class=o>*</span> <span class=n>blockDim</span><span class=p>.</span><span class=n>y</span> <span class=o>+</span> <span class=n>threadIdx</span><span class=p>.</span><span class=n>y</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>xbase</span> <span class=o>=</span> <span class=n>blockIdx</span><span class=p>.</span><span class=n>x</span> <span class=o>*</span> <span class=n>blockDim</span><span class=p>.</span><span class=n>x</span> <span class=o>+</span> <span class=n>threadIdx</span><span class=p>.</span><span class=n>x</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=kt>float</span> <span class=n>c</span><span class=p>[</span><span class=n>V</span><span class=p>][</span><span class=n>V</span><span class=p>]</span> <span class=o>=</span> <span class=p>{</span><span class=mi>0</span><span class=p>};</span>
</span></span><span class=line><span class=cl>    <span class=kt>float</span> <span class=n>a</span><span class=p>[</span><span class=n>V</span><span class=p>],</span> <span class=n>b</span><span class=p>[</span><span class=n>V</span><span class=p>];</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>k</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>k</span> <span class=o>&lt;</span> <span class=n>N</span><span class=p>;</span> <span class=o>++</span><span class=n>k</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=n>a</span><span class=p>[</span><span class=o>:</span><span class=p>]</span> <span class=o>=</span> <span class=n>A</span><span class=p>[</span><span class=n>k</span><span class=p>,</span> <span class=n>ybase</span><span class=o>*</span><span class=nl>V</span> <span class=p>:</span> <span class=n>ybase</span><span class=o>*</span><span class=n>V</span> <span class=o>+</span> <span class=n>V</span><span class=p>];</span>
</span></span><span class=line><span class=cl>        <span class=n>b</span><span class=p>[</span><span class=o>:</span><span class=p>]</span> <span class=o>=</span> <span class=n>B</span><span class=p>[</span><span class=n>k</span><span class=p>,</span> <span class=n>xbase</span><span class=o>*</span><span class=nl>V</span> <span class=p>:</span> <span class=n>xbase</span><span class=o>*</span><span class=n>V</span> <span class=o>+</span> <span class=n>V</span><span class=p>];</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>y</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>y</span> <span class=o>&lt;</span> <span class=n>V</span><span class=p>;</span> <span class=o>++</span><span class=n>y</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>x</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>x</span> <span class=o>&lt;</span> <span class=n>V</span><span class=p>;</span> <span class=o>++</span><span class=n>x</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>                <span class=n>c</span><span class=p>[</span><span class=n>y</span><span class=p>][</span><span class=n>x</span><span class=p>]</span> <span class=o>+=</span> <span class=n>a</span><span class=p>[</span><span class=n>y</span><span class=p>]</span> <span class=o>*</span> <span class=n>b</span><span class=p>[</span><span class=n>x</span><span class=p>];</span>
</span></span><span class=line><span class=cl>            <span class=p>}</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=n>C</span><span class=p>[</span><span class=n>ybase</span> <span class=o>*</span> <span class=nl>V</span> <span class=p>:</span> <span class=n>ybase</span> <span class=o>*</span> <span class=n>V</span> <span class=o>+</span> <span class=n>V</span><span class=p>,</span> <span class=n>xbase</span> <span class=o>*</span> <span class=nl>V</span> <span class=p>:</span> <span class=n>xbase</span> <span class=o>*</span> <span class=n>V</span> <span class=o>+</span> <span class=n>V</span><span class=p>]</span> <span class=o>=</span> <span class=n>c</span><span class=p>[</span><span class=o>:</span><span class=p>,</span><span class=o>:</span><span class=p>];</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><p>每个线程负责计算一个分块的结果，即每次计算下图中的一块。
<img alt=image.png|350 loading=lazy src="https://pics.zhouxin.space/202407261324561.png?x-oss-process=image/quality,q_90/format,webp">
还可以将计算一块的任务交给一个block，这样就可以使用共享内存技术有block内的线程共同加载要用到的数据。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-c data-lang=c><span class=line><span class=cl><span class=n>__global__</span> <span class=kt>void</span> <span class=nf>mm</span><span class=p>(</span><span class=kt>float</span> <span class=n>A</span><span class=p>[</span><span class=n>N</span><span class=p>][</span><span class=n>N</span><span class=p>],</span> <span class=kt>float</span> <span class=n>B</span><span class=p>[</span><span class=n>N</span><span class=p>][</span><span class=n>N</span><span class=p>],</span> <span class=kt>float</span> <span class=n>C</span><span class=p>[</span><span class=n>N</span><span class=p>][</span><span class=n>N</span><span class=p>])</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>__shared__</span> <span class=kt>float</span> <span class=n>sA</span><span class=p>[</span><span class=n>S</span><span class=p>][</span><span class=n>L</span><span class=p>],</span> <span class=n>sB</span><span class=p>[</span><span class=n>S</span><span class=p>][</span><span class=n>L</span><span class=p>];</span>
</span></span><span class=line><span class=cl>    <span class=kt>float</span> <span class=n>c</span><span class=p>[</span><span class=n>V</span><span class=p>][</span><span class=n>V</span><span class=p>]</span> <span class=o>=</span> <span class=p>{</span><span class=mi>0</span><span class=p>};</span>
</span></span><span class=line><span class=cl>    <span class=kt>float</span> <span class=n>a</span><span class=p>[</span><span class=n>V</span><span class=p>],</span> <span class=n>b</span><span class=p>[</span><span class=n>V</span><span class=p>];</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>yblock</span> <span class=o>=</span> <span class=n>blockIdx</span><span class=p>.</span><span class=n>y</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>xblock</span> <span class=o>=</span> <span class=n>blockIdx</span><span class=p>.</span><span class=n>x</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>ko</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>ko</span> <span class=o>&lt;</span> <span class=n>N</span><span class=p>;</span> <span class=n>ko</span> <span class=o>+=</span> <span class=n>S</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=nf>__syncthreads</span><span class=p>();</span>
</span></span><span class=line><span class=cl>        <span class=c1>// needs to be implemented by thread cooperative fetching
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=n>sA</span><span class=p>[</span><span class=o>:</span><span class=p>,</span> <span class=o>:</span><span class=p>]</span> <span class=o>=</span> <span class=n>A</span><span class=p>[</span><span class=n>ko</span> <span class=o>+</span> <span class=n>S</span><span class=p>,</span> <span class=n>yblock</span> <span class=o>*</span> <span class=nl>L</span> <span class=p>:</span> <span class=n>yblock</span> <span class=o>*</span> <span class=n>L</span> <span class=o>+</span> <span class=n>L</span><span class=p>];</span>
</span></span><span class=line><span class=cl>        <span class=n>sB</span><span class=p>[</span><span class=o>:</span><span class=p>,</span> <span class=o>:</span><span class=p>]</span> <span class=o>=</span> <span class=n>B</span><span class=p>[</span><span class=n>ko</span> <span class=o>+</span> <span class=n>S</span><span class=p>,</span> <span class=n>xblock</span> <span class=o>*</span> <span class=nl>L</span> <span class=p>:</span> <span class=n>xblock</span> <span class=o>*</span> <span class=n>L</span> <span class=o>+</span> <span class=n>L</span><span class=p>];</span>
</span></span><span class=line><span class=cl>        <span class=nf>__syncthreads</span><span class=p>();</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>ki</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>ki</span> <span class=o>&lt;</span> <span class=n>S</span><span class=p>;</span> <span class=o>++</span><span class=n>ki</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=n>a</span><span class=p>[</span><span class=o>:</span><span class=p>]</span> <span class=o>=</span> <span class=n>sA</span><span class=p>[</span><span class=n>ki</span><span class=p>,</span> <span class=n>threadIdx</span><span class=p>.</span><span class=n>x</span> <span class=o>*</span> <span class=n>V</span> <span class=o>+</span> <span class=n>V</span><span class=p>];</span>
</span></span><span class=line><span class=cl>            <span class=n>b</span><span class=p>[</span><span class=o>:</span><span class=p>]</span> <span class=o>=</span> <span class=n>sB</span><span class=p>[</span><span class=n>ki</span><span class=p>,</span> <span class=n>threadIdx</span><span class=p>.</span><span class=n>x</span> <span class=o>*</span> <span class=n>V</span> <span class=o>+</span> <span class=n>V</span><span class=p>];</span>
</span></span><span class=line><span class=cl>            <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>y</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>y</span> <span class=o>&lt;</span> <span class=n>V</span><span class=p>;</span> <span class=o>++</span><span class=n>y</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>                <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>x</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>x</span> <span class=o>&lt;</span> <span class=n>V</span><span class=p>;</span> <span class=o>++</span><span class=n>x</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>                    <span class=n>c</span><span class=p>[</span><span class=n>y</span><span class=p>][</span><span class=n>x</span><span class=p>]</span> <span class=o>+=</span> <span class=n>a</span><span class=p>[</span><span class=n>y</span><span class=p>]</span> <span class=o>*</span> <span class=n>b</span><span class=p>[</span><span class=n>x</span><span class=p>];</span>
</span></span><span class=line><span class=cl>                <span class=p>}</span>
</span></span><span class=line><span class=cl>            <span class=p>}</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>ybase</span> <span class=o>=</span> <span class=n>blockIdx</span><span class=p>.</span><span class=n>y</span> <span class=o>*</span> <span class=n>blockDim</span><span class=p>.</span><span class=n>y</span> <span class=o>+</span> <span class=n>threadIdx</span><span class=p>.</span><span class=n>y</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>xbase</span> <span class=o>=</span> <span class=n>blockIdx</span><span class=p>.</span><span class=n>x</span> <span class=o>*</span> <span class=n>blockDim</span><span class=p>.</span><span class=n>x</span> <span class=o>+</span> <span class=n>threadIdx</span><span class=p>.</span><span class=n>x</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=n>C</span><span class=p>[</span><span class=n>ybase</span> <span class=o>*</span> <span class=nl>V</span> <span class=p>:</span> <span class=n>ybase</span> <span class=o>*</span> <span class=n>V</span> <span class=o>+</span> <span class=n>V</span><span class=p>,</span> <span class=n>xbase</span> <span class=o>*</span> <span class=nl>V</span> <span class=p>:</span> <span class=n>xbase</span> <span class=o>*</span> <span class=n>V</span> <span class=o>+</span> <span class=n>V</span><span class=p>]</span> <span class=o>=</span> <span class=n>c</span><span class=p>[</span><span class=o>:</span><span class=p>,</span> <span class=o>:</span><span class=p>];</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><p>上述代码从全部内存到共享内存的加载过程被复用L次（计算每个分块矩阵都要读取L次AB的行列向量），从共享内存到寄存器被复用V次（在分块矩阵中按照长度V进行了二次分块计算）<img alt=image.png|400 loading=lazy src="https://pics.zhouxin.space/202407261448550.png?x-oss-process=image/quality,q_90/format,webp">
各线程读取数据到共享内存的过程为：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-c data-lang=c><span class=line><span class=cl><span class=n>sA</span><span class=p>[</span><span class=o>:</span><span class=p>,</span> <span class=o>:</span><span class=p>]</span> <span class=o>=</span> <span class=n>A</span><span class=p>[</span><span class=nl>k</span> <span class=p>:</span> <span class=n>k</span> <span class=o>+</span> <span class=n>S</span><span class=p>,</span> <span class=n>yblock</span> <span class=o>*</span> <span class=nl>L</span> <span class=p>:</span> <span class=n>yblock</span> <span class=o>*</span> <span class=n>L</span> <span class=o>+</span> <span class=n>L</span><span class=p>];</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kt>int</span> <span class=n>nthreads</span> <span class=o>=</span> <span class=n>blockDim</span><span class=p>.</span><span class=n>y</span> <span class=o>*</span> <span class=n>blockDim</span><span class=p>.</span><span class=n>x</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=kt>int</span> <span class=n>tid</span> <span class=o>=</span> <span class=n>threadIdx</span><span class=p>.</span><span class=n>y</span> <span class=o>*</span> <span class=n>blockDim</span><span class=p>.</span><span class=n>x</span> <span class=o>+</span> <span class=n>threadIdx</span><span class=p>.</span><span class=n>x</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=k>for</span><span class=p>(</span><span class=kt>int</span> <span class=n>j</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>j</span> <span class=o>&lt;</span> <span class=n>L</span> <span class=o>*</span> <span class=n>S</span> <span class=o>/</span> <span class=n>nthreads</span><span class=p>;</span> <span class=o>++</span><span class=n>j</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>y</span> <span class=o>=</span> <span class=p>(</span><span class=n>j</span> <span class=o>*</span> <span class=n>nthreads</span> <span class=o>+</span> <span class=n>tid</span><span class=p>)</span> <span class=o>/</span> <span class=n>L</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>x</span> <span class=o>=</span> <span class=p>(</span><span class=n>j</span> <span class=o>*</span> <span class=n>nthreads</span> <span class=o>+</span> <span class=n>tid</span><span class=p>)</span> <span class=o>%</span> <span class=n>L</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=n>s</span><span class=p>[</span><span class=n>y</span><span class=p>,</span> <span class=n>x</span><span class=p>]</span> <span class=o>=</span> <span class=n>A</span><span class=p>[</span><span class=n>k</span> <span class=o>+</span> <span class=n>y</span><span class=p>,</span> <span class=n>yblock</span> <span class=o>*</span> <span class=n>L</span> <span class=o>+</span> <span class=n>x</span><span class=p>];</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><h1 id=lecture-13-hardware-acceleration-implemetation>Lecture 13: Hardware Acceleration Implemetation<a hidden class=anchor aria-hidden=true href=#lecture-13-hardware-acceleration-implemetation>#</a></h1><p>这节是实验课，在这节课中，我们将学习needle库中CPU和GPU底端具体实现的代码骨架。</p><p>这节课不做笔记，本节课内容可通过完成hw3学习。</p><h1 id=lecture-14-implementing-convolutions>Lecture 14: Implementing Convolutions<a hidden class=anchor aria-hidden=true href=#lecture-14-implementing-convolutions>#</a></h1><p>本节课将学习卷积算子的具体实现。</p><h2 id=存储格式-storage-order>存储格式 Storage Order<a hidden class=anchor aria-hidden=true href=#存储格式-storage-order>#</a></h2><p>对于图片数据或者隐藏层，我们需要存储<code>batch_size*channel*height*width</code>即<code>B*C*H*W</code>个元素，本课程中，我们选取的存储格式为：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=kt>float</span> <span class=n>Z</span><span class=p>[</span><span class=n>BATCHES</span><span class=p>][</span><span class=n>HEIGHT</span><span class=p>][</span><span class=n>WIDTH</span><span class=p>][</span><span class=n>CHANNELS</span><span class=p>];</span>
</span></span></code></pre></td></tr></table></div></div><p>上述格式被称为NHWC格式（N代表number）。PyTorch默认格式为NCHW，其在后期版本也支持NHWC。不同的格式会影响操作的性能：卷积在NHWC上更快，Batch Norm在NCHW上更快。</p><p>对于卷积核，其需要存储<code>k*k*C_in*C_out</code>个元素，本课程我们选取的存储格式为：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=kt>float</span> <span class=n>weights</span><span class=p>[</span><span class=n>KERNEL_SIZE</span><span class=p>][</span><span class=n>KERNEL_SIZE</span><span class=p>][</span><span class=n>IN_CHANNELS</span><span class=p>][</span><span class=n>OUT_CHANNELS</span><span class=p>];</span>
</span></span></code></pre></td></tr></table></div></div><p>PyTorch选择的格式为<code>(C_out, C_in, k, k)</code>。</p><h2 id=for循环实现卷积-convolutions-with-simple-loops>for循环实现卷积 Convolutions with simple loops<a hidden class=anchor aria-hidden=true href=#for循环实现卷积-convolutions-with-simple-loops>#</a></h2><p>通过循环来实现卷积操作的过程，从外到内，循环迭代的参数依次为：batch、channel_in、channel_out、out_row、out_column，还有两个循环用于实现卷积，共七个循环：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>conv_naive</span><span class=p>(</span><span class=n>Z</span><span class=p>,</span> <span class=n>weight</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>N</span><span class=p>,</span><span class=n>H</span><span class=p>,</span><span class=n>W</span><span class=p>,</span><span class=n>C_in</span> <span class=o>=</span> <span class=n>Z</span><span class=o>.</span><span class=n>shape</span>
</span></span><span class=line><span class=cl>    <span class=n>K</span><span class=p>,</span><span class=n>_</span><span class=p>,</span><span class=n>_</span><span class=p>,</span><span class=n>C_out</span> <span class=o>=</span> <span class=n>weight</span><span class=o>.</span><span class=n>shape</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=n>out</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>zeros</span><span class=p>((</span><span class=n>N</span><span class=p>,</span><span class=n>H</span><span class=o>-</span><span class=n>K</span><span class=o>+</span><span class=mi>1</span><span class=p>,</span><span class=n>W</span><span class=o>-</span><span class=n>K</span><span class=o>+</span><span class=mi>1</span><span class=p>,</span><span class=n>C_out</span><span class=p>));</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>n</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>N</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>c_in</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>C_in</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=k>for</span> <span class=n>c_out</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>C_out</span><span class=p>):</span>
</span></span><span class=line><span class=cl>                <span class=k>for</span> <span class=n>y</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>H</span><span class=o>-</span><span class=n>K</span><span class=o>+</span><span class=mi>1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>                    <span class=k>for</span> <span class=n>x</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>W</span><span class=o>-</span><span class=n>K</span><span class=o>+</span><span class=mi>1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>                        <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>K</span><span class=p>):</span>
</span></span><span class=line><span class=cl>                            <span class=k>for</span> <span class=n>j</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>K</span><span class=p>):</span>
</span></span><span class=line><span class=cl>                                <span class=n>out</span><span class=p>[</span><span class=n>n</span><span class=p>,</span><span class=n>y</span><span class=p>,</span><span class=n>x</span><span class=p>,</span><span class=n>c_out</span><span class=p>]</span> <span class=o>+=</span> <span class=n>Z</span><span class=p>[</span><span class=n>n</span><span class=p>,</span><span class=n>y</span><span class=o>+</span><span class=n>i</span><span class=p>,</span><span class=n>x</span><span class=o>+</span><span class=n>j</span><span class=p>,</span><span class=n>c_in</span><span class=p>]</span> <span class=o>*</span> <span class=n>weight</span><span class=p>[</span><span class=n>i</span><span class=p>,</span><span class=n>j</span><span class=p>,</span><span class=n>c_in</span><span class=p>,</span><span class=n>c_out</span><span class=p>]</span>
</span></span></code></pre></td></tr></table></div></div><p>该七重循环实现的卷积耗时3秒，而PyTorch仅需1.2毫秒，约2500倍的性能差距。</p><h2 id=矩阵乘法实现卷积-convolutions-as-matrix-multiplications>矩阵乘法实现卷积 Convolutions as matrix multiplications<a hidden class=anchor aria-hidden=true href=#矩阵乘法实现卷积-convolutions-as-matrix-multiplications>#</a></h2><p>卷积核中任意一个元素[ i, j, :, : ]都是一个shape为(c_in, c_out)的矩阵，当其作用在输入图片的某个元素(p,q,m,:)即作用在一个长度为c_in的向量上时，这个过程就是一个矩阵乘法运算。</p><p>特别的，对于卷积核大小为1×1的情况，整个卷积过程可以直接用一个矩阵乘法来表示：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>W1</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span><span class=mi>1</span><span class=p>,</span><span class=mi>8</span><span class=p>,</span><span class=mi>16</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>out</span> <span class=o>=</span> <span class=n>conv_reference</span><span class=p>(</span><span class=n>Z</span><span class=p>,</span><span class=n>W1</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>怎么将1×1的卷积核推广到一般情况呢？可以把卷积核看成由一个个1×1的小卷积核组成的，不断迭代这些卷积核即可。需要注意的是，每个小卷积核在图片上作用的范围都不一样，要做好切片：</p><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td><code>&lt;br>1&lt;br>2&lt;br>3&lt;br>4&lt;br>5&lt;br>6&lt;br>7&lt;br>8&lt;br>9&lt;br></code></td><td><code>python&lt;br>def conv_matrix_mult(Z, weight):&lt;br> N,H,W,C_in = Z.shape&lt;br> K,_,_,C_out = weight.shape&lt;br> out = np.zeros((N,H-K+1,W-K+1,C_out))&lt;br> &lt;br> for i in range(K):&lt;br> for j in range(K):&lt;br> out += Z[:,i:i+H-K+1,j:j+W-K+1,:] @ weight[i,j]&lt;br> return out&lt;br></code></td></tr></tbody></table><p>该版本卷积耗时17毫秒，相比PyTorch1.2毫秒，约14倍性能差距。</p><h2 id=通过strides来操作矩阵-manipulating-matrices-via-strides>通过strides来操作矩阵 Manipulating matrices via strides<a hidden class=anchor aria-hidden=true href=#通过strides来操作矩阵-manipulating-matrices-via-strides>#</a></h2><p>在内存中，通常将矩阵按照二维数组的形式在内存中存储：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=kt>float</span> <span class=n>A</span><span class=p>[</span><span class=n>M</span><span class=p>][</span><span class=n>N</span><span class=p>];</span>
</span></span></code></pre></td></tr></table></div></div><p>但是，我们在实现一些高效算子时，经常会把矩阵分块，如果将其分块存储，那么这些算子将会具有更好的空间局部性：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=kt>float</span> <span class=n>A</span><span class=p>[</span><span class=n>M</span><span class=o>/</span><span class=n>TILE</span><span class=p>][</span><span class=n>N</span><span class=o>/</span><span class=n>TILE</span><span class=p>][</span><span class=n>TILE</span><span class=p>][</span><span class=n>TILE</span><span class=p>]</span>
</span></span></code></pre></td></tr></table></div></div><p>NumPy提供了一个函数用于实现从二维数组转变为分块矩阵的格式：<code>np.lib.stride_tricks.as_strided</code><a href=https://www.zhouxin.space/notes/notes-on-cmu-10-414-deep-learning-system/#fn:4>4</a>。</p><p>具体来说，<code>as_strided</code>这个函数用于创建一个具有不同shape和strides，但与原array具有相同底层数据的视图（view）。</p><p>举个例子，如下图所示，一个6×6的矩阵，对于按照2×2进行分块，我们从strides[3]倒着写出其值。<img alt=image.png loading=lazy src="https://pics.zhouxin.space/202408252356974.webp?x-oss-process=image/quality,q_90/format,webp"></p><ul><li>strides [3]表示在子矩阵内部移动到下一列的元素的步长，即从0移动到1的步长，数据在内存中是按行连续排列的，因此其值为1；</li><li>strides [2]表示在子矩阵中移动到下一行元素的步长，即从0移动到6的所需步长，观察图片可以看到该步步长等于矩阵的列数N，即6；</li><li>strides [1]表示从一个子矩阵移动到同行下一个子矩阵的对应位置的步长，即从0移动到2的步长，可以看到移动的步长等于分块的列长度TILE，即2；</li><li>strides [0]表示从一个子矩阵移动到同列下一个子矩阵对应位置的步长，即从0移动到12的步长，可以看到移动的步长等于TILE*N，即12。</li></ul><p>确定了strides之后，就可以使用<code>as_strided</code>为原矩阵创建一个分块矩阵的视图，并使用<code>np.ascontiguousarray</code>创建一个内存连续版本的副本：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl><span class=n>n</span> <span class=o>=</span> <span class=mi>6</span>
</span></span><span class=line><span class=cl><span class=n>A</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=n>n</span><span class=o>**</span><span class=mi>2</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>np</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>n</span><span class=p>,</span><span class=n>n</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>B</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>lib</span><span class=o>.</span><span class=n>stride_tricks</span><span class=o>.</span><span class=n>as_strided</span><span class=p>(</span><span class=n>A</span><span class=p>,</span> <span class=n>shape</span><span class=o>=</span><span class=p>(</span><span class=mi>3</span><span class=p>,</span><span class=mi>3</span><span class=p>,</span><span class=mi>2</span><span class=p>,</span><span class=mi>2</span><span class=p>),</span> <span class=n>strides</span><span class=o>=</span><span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>((</span><span class=mi>12</span><span class=p>,</span><span class=mi>2</span><span class=p>,</span><span class=mi>6</span><span class=p>,</span><span class=mi>1</span><span class=p>))</span><span class=o>*</span><span class=mi>4</span><span class=p>)</span>  <span class=c1>#numpy中strides以字节为单位</span>
</span></span><span class=line><span class=cl><span class=n>C</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>ascontiguousarray</span><span class=p>(</span><span class=n>B</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>—————-以下非课程内容—————- 这里插一嘴，这里实现分块的方式非常不优雅，毕竟numpy并不建议使用这么底层的API来直接修改数据，我问了下GPT，他提供了一种更优雅的方案。</p><p>我们首先可以将原矩阵(M, N)reshape为(M//TILE, TILE, N//TILE, TILE)，这一步相当于将原矩阵在行和列上进行分块，并且(p,m,q,n)表示第p行第q列的子矩阵中第m行第n列个元素。然后使用<code>transpose(0, 2, 1, 3)</code>重新排列维度即可。</p><p>至于为什么reshape那一步后索引仍是正确的，我略微理解的，但难以表达出来，有点只可意会的意思：reshape那个操作可以分成两步完成，分别是在行和列上进行切片，这两个步骤又不冲突，合并后的结果就是如下的代码：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>block_matrix</span><span class=p>(</span><span class=n>A</span><span class=p>,</span> <span class=n>TILE</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>M</span><span class=p>,</span> <span class=n>N</span> <span class=o>=</span> <span class=n>A</span><span class=o>.</span><span class=n>shape</span>
</span></span><span class=line><span class=cl>    <span class=k>assert</span> <span class=n>M</span> <span class=o>%</span> <span class=n>TILE</span> <span class=o>==</span> <span class=mi>0</span> <span class=ow>and</span> <span class=n>N</span> <span class=o>%</span> <span class=n>TILE</span> <span class=o>==</span> <span class=mi>0</span><span class=p>,</span> <span class=s2>&#34;矩阵维度必须能被TILE整除&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>A_reshaped</span> <span class=o>=</span> <span class=n>A</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>M</span><span class=o>//</span><span class=n>TILE</span><span class=p>,</span> <span class=n>TILE</span><span class=p>,</span> <span class=n>N</span><span class=o>//</span><span class=n>TILE</span><span class=p>,</span> <span class=n>TILE</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>A_blocked</span> <span class=o>=</span> <span class=n>A_reshaped</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>3</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>np</span><span class=o>.</span><span class=n>ascontiguousarray</span><span class=p>(</span><span class=n>A_blocked</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>—————-以上非课程内容—————-</p><h2 id=通过-im2col-来实现卷积-convolutions-via-im2col>通过 im2col 来实现卷积 Convolutions via im2col<a hidden class=anchor aria-hidden=true href=#通过-im2col-来实现卷积-convolutions-via-im2col>#</a></h2><p>在Lecture 10中提到，我们可以使用im2col技术，将一维卷积运算转换为矩阵运算：</p><p>[z1z2z3z4z5]=x∗w=[0x1x2x1x2x3x2x3x4x3x4x5x4x50][w1w2w3]​z1​z2​z3​z4​z5​​​=x∗w=​0x1​x2​x3​x4​​x1​x2​x3​x4​x5​​x2​x3​x4​x5​0​​​w1​w2​w3​​​</p><p>对于二维卷积来说，同样也是可以的。以卷积核大小为3×3为例，对6×6的矩阵进行卷积，其结果矩阵为4×4。首先，我们找出每次运算的感受野，将其单独拿出来，那么所有这些感受野就组成了一个4×4×3×3的Tensor。</p><p>如下图所示，第[0,0]个感受野就是[0,1,2;6,7,8;12,13,14]。怎么将原始矩阵转变为Tensor呢？这里就可以用到上节提到的<code>as_strided</code>方法。strides[0]表示到同列下一个感受野的相同位置的元素的步长，为列长6；strides[1]表示到同行下一个感受野的步长，为1；strides[2]表示同一个感受野内部同列下一个元素的步长，为原始列长6；strides[3]表示同一个感受野内部同行下一个元素的步长，为1。即，使用<code>B = np.lib.stride_tricks.as_strided(A, shape=(4,4,3,3), strides=4*(np.array((6,1,6,1))))</code>可以将原始待卷积矩阵A转变为感受野张量B。<img alt=image.png loading=lazy src="https://pics.zhouxin.space/202408300948305.png?x-oss-process=image/quality,q_90/format,webp"></p><p>下一步，通过reshape操作将单个感受野和卷积核都转变为向量，通过内积运算计算卷积值：</p><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td><code>&lt;br>1&lt;br></code></td><td><code>python&lt;br>(B.reshape(16,9) @ W.reshape(9)).reshape(4,4)&lt;br></code></td></tr></tbody></table><p>需要注意的是，B的reshape的操作并不是free的，无法通过原始的A的数据来表示reshape后的B，该reshape操作会分配出一块O(K2)O(K2)的内存空间，当K比较大时，这个操作将相当耗费内存。因此，在现代版本中，常常会使用lazy技术或者其它技术，但这不在本课程讨论范围之内。</p><h2 id=通过-im2col-来实现多通道卷积>通过 im2col 来实现多通道卷积<a hidden class=anchor aria-hidden=true href=#通过-im2col-来实现多通道卷积>#</a></h2><p>对于多通道并且考虑batch的卷积，其输入shape为N×H×W×C_in，感受野Tensor为N×(W-K+1)×(H-K+1)×K×K×C_in，需要将K×K×C_in展开为一维，卷积核也要将对应位置展开，即reshape后shape为(K×K×C_in)×C_out。</p><p>代码实现为：</p><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td><code>&lt;br> 1&lt;br> 2&lt;br> 3&lt;br> 4&lt;br> 5&lt;br> 6&lt;br> 7&lt;br> 8&lt;br> 9&lt;br>10&lt;br></code></td><td><code>python&lt;br>def conv_im2col(Z, weight):&lt;br> N,H,W,C_in = Z.shape&lt;br> K,_,_,C_out = weight.shape&lt;br> Ns, Hs, Ws, Cs = Z.strides&lt;br> &lt;br> inner_dim = K * K * C_in&lt;br> A = np.lib.stride_tricks.as_strided(Z, shape = (N, H-K+1, W-K+1, K, K, C_in),&lt;br> strides = (Ns, Hs, Ws, Hs, Ws, Cs)).reshape(-1,inner_dim)&lt;br> out = A @ weight.reshape(-1, C_out)&lt;br> return out.reshape(N,H-K+1,W-K+1,C_out)&lt;br></code></td></tr></tbody></table><h1 id=lecture-15-training-large-models>Lecture 15: Training Large Models<a hidden class=anchor aria-hidden=true href=#lecture-15-training-large-models>#</a></h1><h2 id=内存节省技术-techniques-for-memory-saving>内存节省技术 Techniques for memory saving<a hidden class=anchor aria-hidden=true href=#内存节省技术-techniques-for-memory-saving>#</a></h2><p>一直以来，GPU的全局内存大小都是模型大小的制约瓶颈，通过一些内存节省技术可以训练更大的一些模型。</p><p>模型内存消耗主要有如下几个方面：模型权重、优化器状态（动量值等等）、中间激活层的值。</p><p>对于推理来说，保存激活层的内存只需要两块，分别用来保存一层的输入和输出，下一层的输入为上一层的输出，下一层的输出覆盖上一层的输入。其不需要保存中间激活层的值。</p><p>而在训练中，由于在计算每一层的梯度时都用到了该层的输入，所每一个激活层都要一片内存保存下来，即激活层的内存数量为O(N)O(N)，如下图所示：
<img alt=image.png|400 loading=lazy src="https://pics.zhouxin.space/202408302230612.png?x-oss-process=image/quality,q_90/format,webp"></p><p>一种减少激活层内存使用的技术叫做checkpoint，就是每隔一个激活层才保存该层的值，如下图所示：
<img alt=image.png|400 loading=lazy src="https://pics.zhouxin.space/202408302240185.png?x-oss-process=image/quality,q_90/format,webp">
在反向传播时，如果需要用到未保存的隐藏层，则通过上一个隐藏层计算出该层的值即可。这是一种时间换空间的思路。对于一个N层的网络，每隔K个隐藏层保存一次结果，则隐藏层占用的内存空间大小为O(N/K)+O(K)O(N/K)+O(K)，当K=NK=N​时可取到最小值。</p><h2 id=并行和分布式训练-parallel-and-distributed-training>并行和分布式训练 Parallel and distributed training<a hidden class=anchor aria-hidden=true href=#并行和分布式训练-parallel-and-distributed-training>#</a></h2><ul><li>计算图划分</li></ul><p>当有多个GPU时，可以进行并行分布式训练。一种思路是将计算图进行划分，并分配给不同的worker进行执行，通过通讯协议在worker中间传递数据。如下图所示，整个计算图被划分为两部分。<img alt=image.png|400 loading=lazy src="https://pics.zhouxin.space/202408310842410.png?x-oss-process=image/quality,q_90/format,webp">仅仅将计算图划分并不会带来多少的并行性，但是当worker1计算来自worker0的数据时，worker0可以并行计算下一个minibatch的数据，从而实现高并行。</p><ul><li>数据并行训练</li></ul><p>数据并行训练是的是将一个minibatch分割成更小的smaller batch，每个GPU负责一个smaller batch的计算，这样做每隔GPU上都在跑相同的模型。</p><p>在分布式和并行计算中，有一个allreduce原语，其作用是将分布在多个进程或节点上的数据进行规约（reduction）操作，然后将结果广播回所有参与的进程或节点。运用这个原语，我们可以在多GPU上计算出smaller batch的梯度，然后利用该原语将计算出整个minibatch的梯度并进行梯度下降。</p><p>我们还可以将参数使用专门的参数服务器保存，其它设备需要访问或者更新参数时，只需要调用相应API即可。参数服务器的好处是其不需要等待所有的worker都计算结束再更新，支持动态增减worker数量，提高了系统的鲁棒性。</p><ul><li>通信计算重叠 communication computation overlap</li></ul><p>通信计算重叠，就是指在通信同步时使用非阻塞的方式，在等待IO时继续计算。</p><h1 id=lecture-16-generative-adversarial-network>Lecture 16 Generative Adversarial Network<a hidden class=anchor aria-hidden=true href=#lecture-16-generative-adversarial-network>#</a></h1><h2 id=生成对抗训练-generative-adversarial-training>生成对抗训练 Generative adversarial training<a hidden class=anchor aria-hidden=true href=#生成对抗训练-generative-adversarial-training>#</a></h2><p>对于无监督学习，或者称生成式模型，其任务是通过随机向量生成符合数据集分布的样本。这就引入了一个问题：如何评估样本和目标分布之间的距离。这一评价指标作为我们的目标函数，其必须是可微的，以便后续对模型进行优化。</p><p>对抗训练的思路是构造一个oracle classfier D，其作用是辨别生成数据和原始数据，D的输出是输入为生成数据为生成数据的概率。那对于任意一个输入z，生成网络G的输出为G(z)，D对其的判别结果为D(G(z))。那生成器的目标就是尽可能让判别器判别错误，即其损失函数为：</p><p>max⁡G{−Ez∼Noiselog⁡(1−D(G(z)))}Gmax​{−Ez∼Noise​log(1−D(G(z)))}</p><p>需要注意的是，这里并没有现成的辨别器D。我们同样可以用一个神经网络来构造这个辨别器，那这个辨别器的目标就是尽可能判断正确，即其损失函数为：</p><p>min⁡D{−Ez∼Noiselog⁡(1−D(G(z)))−Ex∼Datalog⁡D(x)}Dmin​{−Ez∼Noise​log(1−D(G(z)))−Ex∼Data​logD(x)}</p><h2 id=将对抗训练作为深度学习中的一个模块-adversarial-training-as-a-module-in-deep-learning-models>将对抗训练作为深度学习中的一个模块 Adversarial training as a module in deep learning models<a hidden class=anchor aria-hidden=true href=#将对抗训练作为深度学习中的一个模块-adversarial-training-as-a-module-in-deep-learning-models>#</a></h2><p>接下来我们考虑如何将对抗模型模块化。我们可以将整个判别器作为一个损失函数来实现，当然，其和我们之前实现的损失函数是不一样的，判别器的参数在每轮反向传播时都要更新。</p><p>【这一节课似乎没有具体说明如何模块化，后边似乎在介绍GAN网络的各个变种】</p><p>在DCGAN中，使用了一种被称为反卷积（转置卷积、Conv2dTranspose）的模块，其作用是进行上采样。</p><p>CycleGAN是一个用于风格迁移的模型。对于风格迁移模型来说，一种有监督的训练思路是收集风格迁移前后的图片配对数据集，进行有监督训练。然而，此类配对数据集是很难获取的。如何通过未配对的数据集进行无监督训练呢？可以使用GAN网络，一个生成器G用于升成风格迁移后的图片，使用一个判别器进行对抗训练。另外有一个生成器F，用于还原图片，其也使用一个判别器进行对抗训练。而整个CycleGAN模型还需要保证循环一致性，即将数据集中的一个图片经过G之后，再经过F，应当还原成原始图片，故循环一致性的损失函数就是两个图片之间的L2 Norm。</p><p>在下节课中，将讨论GAN系列网络的具体实现。</p><h1 id=lecture-17-generative-adversarial-networks-implementations>Lecture 17: Generative Adversarial Networks implementations<a hidden class=anchor aria-hidden=true href=#lecture-17-generative-adversarial-networks-implementations>#</a></h1><p>本节课中，我们将学习GAN网络的具体实现。</p><p>在课程中，使用二维高斯分布作为真实数据集，训练一个生成器用于升成该分布的数据。训练集数据准备如下：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>A</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>],</span> <span class=p>[</span><span class=o>-</span><span class=mf>0.2</span><span class=p>,</span> <span class=mf>0.5</span><span class=p>]])</span>
</span></span><span class=line><span class=cl><span class=n>mu</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=c1># total number of sample data to generated</span>
</span></span><span class=line><span class=cl><span class=n>num_sample</span> <span class=o>=</span> <span class=mi>3200</span>
</span></span><span class=line><span class=cl><span class=n>data</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>normal</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=p>(</span><span class=n>num_sample</span><span class=p>,</span> <span class=mi>2</span><span class=p>))</span> <span class=o>@</span> <span class=n>A</span> <span class=o>+</span> <span class=n>mu</span>
</span></span></code></pre></td></tr></table></div></div><p>生成器使用一个简单的全连接层即可：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>model_G</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>sample_G</span><span class=p>(</span><span class=n>model_G</span><span class=p>,</span> <span class=n>num_samples</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>Z</span> <span class=o>=</span> <span class=n>ndl</span><span class=o>.</span><span class=n>Tensor</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>normal</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=p>(</span><span class=n>num_samples</span><span class=p>,</span> <span class=mi>2</span><span class=p>)))</span>
</span></span><span class=line><span class=cl>    <span class=n>fake_X</span> <span class=o>=</span> <span class=n>model_G</span><span class=p>(</span><span class=n>Z</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>fake_X</span><span class=o>.</span><span class=n>numpy</span><span class=p>()</span>
</span></span></code></pre></td></tr></table></div></div><p>判别器是一个三层的感知机，损失函数为softmax loss：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>model_D</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>20</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>    <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>20</span><span class=p>,</span> <span class=mi>10</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>    <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>loss_D</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>SoftmaxLoss</span><span class=p>()</span>
</span></span></code></pre></td></tr></table></div></div><p>优化生成器G的过程就是使用G随机生成一些数据G(z)，计算D(G(z))的输出和label 1之间的损失：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>opt_G</span> <span class=o>=</span> <span class=n>ndl</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>Adam</span><span class=p>(</span><span class=n>model_G</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=mf>0.01</span><span class=p>)</span><span class=err>、</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>update_G</span><span class=p>(</span><span class=n>Z</span><span class=p>,</span> <span class=n>model_G</span><span class=p>,</span> <span class=n>model_D</span><span class=p>,</span> <span class=n>loss_D</span><span class=p>,</span> <span class=n>opt_G</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>fake_X</span> <span class=o>=</span> <span class=n>model_G</span><span class=p>(</span><span class=n>Z</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>fake_Y</span> <span class=o>=</span> <span class=n>model_D</span><span class=p>(</span><span class=n>fake_X</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>batch_size</span> <span class=o>=</span> <span class=n>Z</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>ones</span> <span class=o>=</span> <span class=n>ndl</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=s2>&#34;int32&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>loss</span> <span class=o>=</span> <span class=n>loss_D</span><span class=p>(</span><span class=n>fake_Y</span><span class=p>,</span> <span class=n>ones</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>opt_G</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</span></span></code></pre></td></tr></table></div></div><p>同样，判别器的更新过程就是计算D(x)和label 1之间的损失，D(G(z))和label 0之间的损失，x是真实数据：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>opt_D</span> <span class=o>=</span> <span class=n>ndl</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>Adam</span><span class=p>(</span><span class=n>model_D</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=mf>0.01</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>update_D</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>Z</span><span class=p>,</span> <span class=n>model_G</span><span class=p>,</span> <span class=n>model_D</span><span class=p>,</span> <span class=n>loss_D</span><span class=p>,</span> <span class=n>opt_D</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>fake_X</span> <span class=o>=</span> <span class=n>model_G</span><span class=p>(</span><span class=n>Z</span><span class=p>)</span><span class=o>.</span><span class=n>detach</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>fake_Y</span> <span class=o>=</span> <span class=n>model_D</span><span class=p>(</span><span class=n>fake_X</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>real_Y</span> <span class=o>=</span> <span class=n>model_D</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>assert</span> <span class=n>X</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=o>==</span> <span class=n>Z</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>batch_size</span> <span class=o>=</span> <span class=n>X</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>ones</span> <span class=o>=</span> <span class=n>ndl</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=s2>&#34;int32&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>zeros</span> <span class=o>=</span> <span class=n>ndl</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=s2>&#34;int32&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>loss</span> <span class=o>=</span> <span class=n>loss_D</span><span class=p>(</span><span class=n>real_Y</span><span class=p>,</span> <span class=n>ones</span><span class=p>)</span> <span class=o>+</span> <span class=n>loss_D</span><span class=p>(</span><span class=n>fake_Y</span><span class=p>,</span> <span class=n>zeros</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>opt_D</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</span></span></code></pre></td></tr></table></div></div><p>训练过程则是每次迭代中，将随机向量送入生成器，再将生成器的输出喂给判别器，然后分别更新二者的参数即可，注意以下代码中epoch指的是训练了几个batch，而不是指在训练集上完整训练了几轮：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>train_gan</span><span class=p>(</span><span class=n>data</span><span class=p>,</span> <span class=n>batch_size</span><span class=p>,</span> <span class=n>num_epochs</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>assert</span> <span class=n>data</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=o>%</span> <span class=n>batch_size</span> <span class=o>==</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>    <span class=n>data</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_epochs</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>begin</span> <span class=o>=</span> <span class=p>(</span><span class=n>batch_size</span> <span class=o>*</span> <span class=n>epoch</span><span class=p>)</span> <span class=o>%</span> <span class=n>data</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=n>X</span> <span class=o>=</span> <span class=n>data</span><span class=p>[</span><span class=n>begin</span><span class=p>:</span> <span class=n>begin</span><span class=o>+</span><span class=n>batch_size</span><span class=p>,</span> <span class=p>:]</span>
</span></span><span class=line><span class=cl>        <span class=n>Z</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>normal</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=mi>2</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>X</span> <span class=o>=</span> <span class=n>ndl</span><span class=o>.</span><span class=n>Tensor</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>Z</span> <span class=o>=</span> <span class=n>ndl</span><span class=o>.</span><span class=n>Tensor</span><span class=p>(</span><span class=n>Z</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>update_D</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>Z</span><span class=p>,</span> <span class=n>model_G</span><span class=p>,</span> <span class=n>model_D</span><span class=p>,</span> <span class=n>loss_D</span><span class=p>,</span> <span class=n>opt_D</span><span class=p>)</span> 
</span></span><span class=line><span class=cl>        <span class=n>update_G</span><span class=p>(</span><span class=n>Z</span><span class=p>,</span> <span class=n>model_G</span><span class=p>,</span> <span class=n>model_D</span><span class=p>,</span> <span class=n>loss_D</span><span class=p>,</span> <span class=n>opt_G</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>train_gan</span><span class=p>(</span><span class=n>data</span><span class=p>,</span> <span class=mi>32</span><span class=p>,</span> <span class=mi>2000</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>以上就是训练一个GAN网络的全过程，接下来我们考虑如何GAN Loss模块化。GAN Loss的作用是给定一个生成器的输出，返回一个损失值。此外，当生成器拿到损失值后就会直接进行生成器的参数更新，因此GAN Loss内部必须隐式更新自身的参数，即：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>GANLoss</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>model_D</span><span class=p>,</span> <span class=n>opt_D</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>model_D</span> <span class=o>=</span> <span class=n>model_D</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>opt_D</span> <span class=o>=</span> <span class=n>opt_D</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>loss_D</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>SoftmaxLoss</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>_update_D</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>real_X</span><span class=p>,</span> <span class=n>fake_X</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>real_Y</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>model_D</span><span class=p>(</span><span class=n>real_X</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>fake_Y</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>model_D</span><span class=p>(</span><span class=n>fake_X</span><span class=o>.</span><span class=n>detach</span><span class=p>())</span>
</span></span><span class=line><span class=cl>        <span class=n>batch_size</span> <span class=o>=</span> <span class=n>real_X</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=n>ones</span> <span class=o>=</span> <span class=n>ndl</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=s2>&#34;float32&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>zeros</span> <span class=o>=</span> <span class=n>ndl</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=s2>&#34;float32&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>loss</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>loss_D</span><span class=p>(</span><span class=n>real_Y</span><span class=p>,</span> <span class=n>ones</span><span class=p>)</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>loss_D</span><span class=p>(</span><span class=n>fake_Y</span><span class=p>,</span> <span class=n>zeros</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>opt_D</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>fake_X</span><span class=p>,</span> <span class=n>real_X</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>_update_D</span><span class=p>(</span><span class=n>real_X</span><span class=p>,</span> <span class=n>fake_X</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>fake_Y</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>model_D</span><span class=p>(</span><span class=n>fake_X</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>batch_size</span> <span class=o>=</span> <span class=n>real_X</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=n>ones</span> <span class=o>=</span> <span class=n>ndl</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=s2>&#34;float32&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>loss</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>loss_D</span><span class=p>(</span><span class=n>fake_Y</span><span class=p>,</span> <span class=n>ones</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>loss</span>
</span></span></code></pre></td></tr></table></div></div><h1 id=lecture-18-sequence-modeling-and-recurrent-networks>Lecture 18: Sequence Modeling and Recurrent Networks<a hidden class=anchor aria-hidden=true href=#lecture-18-sequence-modeling-and-recurrent-networks>#</a></h1><h2 id=序列建模-sequence-modeling>序列建模 Sequence modeling<a hidden class=anchor aria-hidden=true href=#序列建模-sequence-modeling>#</a></h2><p>在前面的模型中，我们都做了一个隐式假设：x和y之间是独立同分布的，但是在实践中，很多任务的y都是与x相关的，尤其是当y是一个时间序列数据。</p><p>对于序列数据来说，有一类预测模型是自回归模型，其基本思想是利用序列自身的历史值来预测未来值。</p><h2 id=循环神经网络-recurrent-neural-networks>循环神经网络 Recurrent neural networks<a hidden class=anchor aria-hidden=true href=#循环神经网络-recurrent-neural-networks>#</a></h2><p>循环网络也能用于解决序列数据的建模问题。RNN网络的思想是构建一个网络模型用于模拟输入序列中的时序信息。</p><p>如下图所示，h表示模型中的隐藏层，隐藏层的输入为前一个隐藏层的状态和当前输入x，经过非线性变换后得到该隐藏层，当前输入的对应输出则由对应隐藏层经过非线性变换后得到。 即：</p><p>ht=f(ht−1,xt)yt=g(h)ht​yt​​=f(ht−1​,xt​)=g(h)​</p><p><img alt=image.png|400 loading=lazy src="https://pics.zhouxin.space/202409041829344.png?x-oss-process=image/quality,q_90/format,webp"></p><p>理论上来说，如果建模得当，这种模式在预测ytyt​时可以获取前面所有时刻的时序信息。</p><p>RNN的训练时需要配对的x和y作为数据集，损失函数由每一个预测值和真实值之间的损失累加得到。显然，这个损失函数很难通过笔纸进行推导，但得益于我们之前构建的自动微分系统，我们不需要手动计算任何梯度。</p><p>可以将多个RNN堆叠在一起，得到stacking RNN，如下图所示：<img alt=image.png|400 loading=lazy src="https://pics.zhouxin.space/202409041908220.png?x-oss-process=image/quality,q_90/format,webp"></p><p>RNN在训练过程中很容易出现梯度/激活层爆炸和梯度/激活层消失问题。之前的lecture提到，当训练很深的网络时，初始化参数是很重要的。在RNN上，这个问题更加严重，因为RNN的模型通常很深很深。</p><p>一个解决梯度问题的方法是着眼于激活函数。ReLU作为激活函数其一个问题是其输出可以无限大。然而，将激活函数修改为有界函数，例如sigmoid或者tanh并不能解决这一问题，尤其是，其不能解决激活层/梯度消失问题。如下所示，对于tanh，当x在0附近时，其输出仍在0附近，这会导致隐藏层消失；但于两个函数，当输入在-5和5附近时，其梯度很小，这会导致梯度消失。</p><p><img alt=image.png loading=lazy src="https://pics.zhouxin.space/202409041923270.png?x-oss-process=image/quality,q_90/format,webp"></p><h2 id=lstms>LSTMs<a hidden class=anchor aria-hidden=true href=#lstms>#</a></h2><p>LSTM一定程度上减轻了RNN中存在的梯度消失和爆炸问题。LSTM在原版RNN的基础上对隐藏层进行了一定改进。如下图所示，LSTM将原始hidden state分裂为两个组件hidden state和cell state。<img alt=image.png|400 loading=lazy src="https://pics.zhouxin.space/202409041935116.png?x-oss-process=image/quality,q_90/format,webp"></p><p>其次，LSTM中具体定义了hidden state和cell state的具体更新公式。LSTM中定义了一些中间变量用于更简洁地描述这一公式，中间变量有forget gate、input gate、output gate，还有一个候选状态g_t。这些中间变量和状态的更新公式如下所示：</p><p>[itftgtot]=(sigmoidsigmoidtanhsigmoid)(Whhht−1+Whxxt+bh)ct=ct−1∘ft+it∘gtht=tanh⁡(ct)∘otit,ft,gt,ot,ct,ht∈RdWhh,Whx∈R4d×d​​it​ft​gt​ot​​​=​sigmoidsigmoidtanhsigmoid​​(Whh​ht−1​+Whx​xt​+bh​)ct​=ct−1​∘ft​+it​∘gt​ht​=tanh(ct​)∘ot​it​,ft​,gt​,ot​,ct​,ht​∈RdWhh​,Whx​∈R4d×d​</p><p>Whh,Whx∈R4d×dWhh​,Whx​∈R4d×d意味着，计算中间变量的权重彼此都是独立的。</p><p>？？？？！！！这公式怎么来的，为啥子这个公式管用？有很多工作试图对此进行解释，但大多是一家之言。Zico Kolter教授对此的解释是：gtgt​在经过sigmoid以后是一个0-1变量，用于决定是否要保留前一状态对应位置的cell state信息，itit​同样是个0-1变量，而gtgt​是个有界项，这一组合决定了是否要在cell state的位置上添加一些额外的信息；htht​的更新公式则是一个有界变量，其作用是防止梯度爆炸或者消失。</p><h2 id=beyond-simple-sequential-models>Beyond “simple” sequential models<a hidden class=anchor aria-hidden=true href=#beyond-simple-sequential-models>#</a></h2><p>除了对序列数据进行建模，RNN能做的还有很多。例如，翻译句子，有一种sequence to sequence架构采用了两个RNN模型，一个用于输入原始句子，提取中间状态，另一个用于根据最后一个中间状态，输出翻译后的句子。<img alt=image.png|400 loading=lazy src="https://pics.zhouxin.space/202409042233558.png?x-oss-process=image/quality,q_90/format,webp"></p><p>这意味着，RNN可以作为一个encoder对语义信息进行提取和编码，也可以作为decoder对语义信息进行解码。</p><p>RNN有一种变体是双向RNN，其作用是xixi​时刻的输出与前后都相关，在一些任务，例如完形填空中可以有较好的表现。</p><h1 id=lecture-19-lstm-implementation>Lecture 19: LSTM Implementation<a hidden class=anchor aria-hidden=true href=#lecture-19-lstm-implementation>#</a></h1><h2 id=lstm-cell>LSTM cell<a hidden class=anchor aria-hidden=true href=#lstm-cell>#</a></h2><p>本节课，我们将在NumPy实现LSTM。首先来实现LSTM cell，一个cell是hidden state和cell state的集合，其状态更新公式为：</p><p>it=σ(Wiixt+bii+Whiht−1+bhi)ft=σ(Wifxt+bif+Whfht−1+bhf)gt=tanh⁡(Wigxt+big+Whght−1+bhg)ot=σ(Wioxt+bio+Whoht−1+bho)ct=ft⊙ct−1+it⊙gtht=ot⊙tanh⁡(ct)it​ft​gt​ot​ct​ht​​=σ(Wii​xt​+bii​+Whi​ht−1​+bhi​)=σ(Wif​xt​+bif​+Whf​ht−1​+bhf​)=tanh(Wig​xt​+big​+Whg​ht−1​+bhg​)=σ(Wio​xt​+bio​+Who​ht−1​+bho​)=ft​⊙ct−1​+it​⊙gt​=ot​⊙tanh(ct​)​</p><p>上述公式在上节课中，可以记为矩阵的形式，即：</p><p>[itftgtot]=(sigmoidsigmoidtanhsigmoid)(Whhht−1+Whxxt+bh)ct=ct−1∘ft+it∘gtht=tanh⁡(ct)∘otit,ft,gt,ot,ct,ht∈RdWhh,Whx∈R4d×d​​it​ft​gt​ot​​​=​sigmoidsigmoidtanhsigmoid​​(Whh​ht−1​+Whx​xt​+bh​)ct​=ct−1​∘ft​+it​∘gt​ht​=tanh(ct​)∘ot​it​,ft​,gt​,ot​,ct​,ht​∈RdWhh​,Whx​∈R4d×d​</p><p>在PyTorch中，已经有LSTM的具体实现，当我们实例化一个20×10020×100的cell，即输入向量长度为20，中间状态特征长度为100，那WhhWhh​和WhxWhx​的形状就是400×100400×100和400×20400×20。</p><p>根据上述更新公式，可以得到计算一个LSTM cell 的方法：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span><span class=lnt>9
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>sigmoid</span><span class=p>(</span><span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=mi>1</span><span class=o>/</span><span class=p>(</span><span class=mi>1</span><span class=o>+</span><span class=n>np</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=o>-</span><span class=n>x</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>lstm_cell</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>h</span><span class=p>,</span> <span class=n>c</span><span class=p>,</span> <span class=n>W_hh</span><span class=p>,</span> <span class=n>W_ih</span><span class=p>,</span> <span class=n>b</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>i</span><span class=p>,</span><span class=n>f</span><span class=p>,</span><span class=n>g</span><span class=p>,</span><span class=n>o</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>split</span><span class=p>(</span><span class=n>W_ih</span><span class=nd>@x</span> <span class=o>+</span> <span class=n>W_hh</span><span class=nd>@h</span> <span class=o>+</span> <span class=n>b</span><span class=p>,</span> <span class=mi>4</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>i</span><span class=p>,</span><span class=n>f</span><span class=p>,</span><span class=n>g</span><span class=p>,</span><span class=n>o</span> <span class=o>=</span> <span class=n>sigmoid</span><span class=p>(</span><span class=n>i</span><span class=p>),</span> <span class=n>sigmoid</span><span class=p>(</span><span class=n>f</span><span class=p>),</span> <span class=n>np</span><span class=o>.</span><span class=n>tanh</span><span class=p>(</span><span class=n>g</span><span class=p>),</span> <span class=n>sigmoid</span><span class=p>(</span><span class=n>o</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>c_out</span> <span class=o>=</span> <span class=n>f</span><span class=o>*</span><span class=n>c</span> <span class=o>+</span> <span class=n>i</span><span class=o>*</span><span class=n>g</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>    <span class=n>h_out</span> <span class=o>=</span> <span class=n>o</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>tanh</span><span class=p>(</span><span class=n>c_out</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>h_out</span><span class=p>,</span> <span class=n>c_out</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=full-sequence-lstm>Full sequence LSTM<a hidden class=anchor aria-hidden=true href=#full-sequence-lstm>#</a></h2><p>基于PyTorch的传统，在实现LSTM时，返回所有的hidden state以及最后一个cell state。前面没有提到，LSTM中各个cell的参数的权重是共享的。那LSTM实际上就是根据序列的长度重复执行<code>lstm_cell</code>即可：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>lstm</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>h</span><span class=p>,</span> <span class=n>c</span><span class=p>,</span> <span class=n>W_hh</span><span class=p>,</span> <span class=n>W_ih</span><span class=p>,</span> <span class=n>b</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>H</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>zeros</span><span class=p>((</span><span class=n>X</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=n>h</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]))</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>t</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>X</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]):</span>
</span></span><span class=line><span class=cl>        <span class=n>h</span><span class=p>,</span> <span class=n>c</span> <span class=o>=</span> <span class=n>lstm_cell</span><span class=p>(</span><span class=n>X</span><span class=p>[</span><span class=n>t</span><span class=p>],</span> <span class=n>h</span><span class=p>,</span> <span class=n>c</span><span class=p>,</span> <span class=n>W_hh</span><span class=p>,</span> <span class=n>W_ih</span><span class=p>,</span> <span class=n>b</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>H</span><span class=p>[</span><span class=n>t</span><span class=p>,:]</span> <span class=o>=</span> <span class=n>h</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>H</span><span class=p>,</span> <span class=n>c</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=batching-efficiently>Batching efficiently<a hidden class=anchor aria-hidden=true href=#batching-efficiently>#</a></h2><p>接下来我们考虑如何实现batch LSTM，一种符合习惯的做法是将batch作为第一个维度将输入X堆叠起来，即<code>X[NUM_BATCHES][NUM_TIMESTEPS][INPUT_SIZE]</code>，这种格式被称为NTC格式。如果采用改格式，那么在经过lstm时，第i个cell访问的元素为<code>X[:,i,:]</code>，注意，这些元素在内存中不是紧密排列的，cache命中率较低。</p><p>如果将时间维度放在第一个，即采用TNC格式，则能够解决该问题。</p><p>其余代码几乎不需要改动，矩阵乘法时要注意将三维的X放到@运算符前面：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>lstm_cell</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>h</span><span class=p>,</span> <span class=n>c</span><span class=p>,</span> <span class=n>W_hh</span><span class=p>,</span> <span class=n>W_ih</span><span class=p>,</span> <span class=n>b</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>i</span><span class=p>,</span><span class=n>f</span><span class=p>,</span><span class=n>g</span><span class=p>,</span><span class=n>o</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>split</span><span class=p>(</span><span class=n>x</span><span class=nd>@W_ih</span> <span class=o>+</span> <span class=n>h</span><span class=nd>@W_hh</span> <span class=o>+</span> <span class=n>b</span><span class=p>[</span><span class=kc>None</span><span class=p>,:],</span> <span class=mi>4</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>i</span><span class=p>,</span><span class=n>f</span><span class=p>,</span><span class=n>g</span><span class=p>,</span><span class=n>o</span> <span class=o>=</span> <span class=n>sigmoid</span><span class=p>(</span><span class=n>i</span><span class=p>),</span> <span class=n>sigmoid</span><span class=p>(</span><span class=n>f</span><span class=p>),</span> <span class=n>np</span><span class=o>.</span><span class=n>tanh</span><span class=p>(</span><span class=n>g</span><span class=p>),</span> <span class=n>sigmoid</span><span class=p>(</span><span class=n>o</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>c_out</span> <span class=o>=</span> <span class=n>f</span><span class=o>*</span><span class=n>c</span> <span class=o>+</span> <span class=n>i</span><span class=o>*</span><span class=n>g</span>
</span></span><span class=line><span class=cl>    <span class=n>h_out</span> <span class=o>=</span> <span class=n>o</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>tanh</span><span class=p>(</span><span class=n>c_out</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>h_out</span><span class=p>,</span> <span class=n>c_out</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>lstm</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>h</span><span class=p>,</span> <span class=n>c</span><span class=p>,</span> <span class=n>W_hh</span><span class=p>,</span> <span class=n>W_ih</span><span class=p>,</span> <span class=n>b</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>H</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>zeros</span><span class=p>((</span><span class=n>X</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=n>X</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>1</span><span class=p>],</span> <span class=n>h</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>1</span><span class=p>]))</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>t</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>X</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]):</span>
</span></span><span class=line><span class=cl>        <span class=n>h</span><span class=p>,</span> <span class=n>c</span> <span class=o>=</span> <span class=n>lstm_cell</span><span class=p>(</span><span class=n>X</span><span class=p>[</span><span class=n>t</span><span class=p>],</span> <span class=n>h</span><span class=p>,</span> <span class=n>c</span><span class=p>,</span> <span class=n>W_hh</span><span class=p>,</span> <span class=n>W_ih</span><span class=p>,</span> <span class=n>b</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>H</span><span class=p>[</span><span class=n>t</span><span class=p>,:,:]</span> <span class=o>=</span> <span class=n>h</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>H</span><span class=p>,</span> <span class=n>c</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=training-lstms>Training LSTMs<a hidden class=anchor aria-hidden=true href=#training-lstms>#</a></h2><p>训练一个单层LSTM很简单，不赘述，直接看代码：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>train_lstm</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>Y</span><span class=p>,</span> <span class=n>h0</span><span class=p>,</span> <span class=n>c0</span><span class=p>,</span> <span class=n>parameters</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>H</span><span class=p>,</span> <span class=n>cn</span> <span class=o>=</span> <span class=n>lstm</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>h0</span><span class=p>,</span> <span class=n>c0</span><span class=p>,</span> <span class=n>parameters</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>l</span> <span class=o>=</span> <span class=n>loss</span><span class=p>(</span><span class=n>H</span><span class=p>,</span> <span class=n>Y</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>l</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>opt</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</span></span></code></pre></td></tr></table></div></div><p>训练一个多层LSTM也不难，可以选择先在深度或者时间维度上正向传播，再在另一个维度上正向传播。示例代码采用先时间再深度的形式：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>train_lstm</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>Y</span><span class=p>,</span> <span class=n>h0</span><span class=p>,</span> <span class=n>c0</span><span class=p>,</span> <span class=n>parameters</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>H</span> <span class=o>=</span> <span class=n>X</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>depth</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>H</span><span class=p>,</span> <span class=n>cn</span> <span class=o>=</span> <span class=n>lstm</span><span class=p>(</span><span class=n>H</span><span class=p>,</span> <span class=n>h0</span><span class=p>[</span><span class=n>i</span><span class=p>],</span> <span class=n>c0</span><span class=p>[</span><span class=n>i</span><span class=p>],</span> <span class=n>parameters</span><span class=p>[</span><span class=n>i</span><span class=p>])</span>
</span></span><span class=line><span class=cl>    <span class=n>l</span> <span class=o>=</span> <span class=n>loss</span><span class=p>(</span><span class=n>H</span><span class=p>,</span> <span class=n>Y</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>l</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>opt</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</span></span></code></pre></td></tr></table></div></div><p>接下来重头戏来了。如果我们的序列长度很长，那么进行一次正向传播需要保存的中间变量就很多很多，显存可能不够，怎么解决这个问题？</p><p>我们可以把这个序列按照某个固定长度进行截断，首先计算第一段中的loss，并进行反向传播，然后对后一段继续进行正向传播，同时将第一段的最后一个cell state作为第二段的初始state传入，然后反向传播…</p><p>一直等到整个序列处理完毕，再更新参数。理解这个过程后，不难发现，阶段版本和完整版本是完全等价的，这也是为什么lstm需要返回最后一个cell state。上述过程可描述为：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>train_lstm</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>Y</span><span class=p>,</span> <span class=n>h0</span><span class=p>,</span> <span class=n>c0</span><span class=p>,</span> <span class=n>parameters</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>H</span><span class=p>,</span> <span class=n>cn</span> <span class=o>=</span> <span class=n>lstm</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>h0</span><span class=p>,</span> <span class=n>c0</span><span class=p>,</span> <span class=n>parameters</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>l</span> <span class=o>=</span> <span class=n>loss</span><span class=p>(</span><span class=n>H</span><span class=p>,</span> <span class=n>Y</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>l</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>opt</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>H</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>data</span><span class=p>,</span> <span class=n>cn</span><span class=o>.</span><span class=n>data</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>h0</span><span class=p>,</span> <span class=n>c0</span> <span class=o>=</span> <span class=n>zeros</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span><span class=n>X</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span><span class=n>BLOCK_SIZE</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>h0</span><span class=p>,</span> <span class=n>c0</span> <span class=o>=</span> <span class=n>train_lstm</span><span class=p>(</span><span class=n>X</span><span class=p>[</span><span class=n>i</span><span class=p>:</span><span class=n>i</span><span class=o>+</span><span class=n>BLOCK_SIZE</span><span class=p>],</span> <span class=n>Y</span><span class=p>[</span><span class=n>i</span><span class=p>:</span><span class=n>i</span><span class=o>+</span><span class=n>BLOCK_SIZE</span><span class=p>],</span> <span class=n>h0</span><span class=p>,</span> <span class=n>c0</span><span class=p>,</span> <span class=n>parameters</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><h1 id=lecture-20-transformers-and-attention>Lecture 20: Transformers and Attention<a hidden class=anchor aria-hidden=true href=#lecture-20-transformers-and-attention>#</a></h1><h2 id=两种为时间序列建模的方法-the-two-approaches-to-time-series-modeling>两种为时间序列建模的方法 The two approaches to time series modeling<a hidden class=anchor aria-hidden=true href=#两种为时间序列建模的方法-the-two-approaches-to-time-series-modeling>#</a></h2><p>RNN在对时间序列建模时采用了一种被称为潜在状态latent state的方式，具体来说，其使用t时刻的hidden state来描述t及t时刻往前的所有信息。这种方法的优点是其理论上可以聚合无限长时刻的信息，缺点是其难以有效记住较远时刻的信息，并且存在梯度爆炸和消失问题。</p><p>而另一种建模方式被称为直接预测 direct prediction，具体来说，直接使用t和t时刻之前的sequence来预测t时刻的输出。这种方式的优点时，对于大部分输出，其计算路径要短的，缺点是没有明确的状态表示，在实践中往往序列长度有限。Transformer就属于这种直接预测方式对时间序列进行建模。</p><p>【此处跳过对CNN用于时间序列建模及其优缺点的介绍】</p><h2 id=自注意力机制和transformer-self-attention-and-transformers>自注意力机制和Transformer Self-attention and transformers<a hidden class=anchor aria-hidden=true href=#自注意力机制和transformer-self-attention-and-transformers>#</a></h2><p>Attention机制本质上指的是任何对状态进行加权求和的机制，这个权重显然不应该由我们自己决定，而是可学习的参数，再经过一层softmax后得到的权重。</p><p>而自注意力机制，顾名思义，就是由状态自己来决定权重，然后对状态按权重求和的机制。</p><p>在自注意力中，KQV是三个shape相同的矩阵，即K,Q,V∈RT×dK,Q,V∈RT×d ，KQV都是由输入XX乘上不同的权重得到的WKWQWVWK​WQ​WV​得到，self-attention算子的定义为：</p><p>SelfAttention(K,Q,V)=softmax(KQTd)VSelfAttention(K,Q,V)=softmax(d​KQT​)V</p><p>其中，softmax操作是对每一行进行的。</p><p>接下来我们尝试理解这个式子在做什么。首先我们要明确，KQV的每一行都是由X对应行加权求和得到的，也就是说，KQV每一行并没有其它行的时序信息（X的每一行表示一个时间的输入）。KQTKQT是一个T×T的矩阵，其第i行第j个元素是由K的第i行和j的第i列作内积得到，在这里，时序信息进行了交换。对于KQTKQT的第i行，其每个元素的值大小在一定程度上反应了Q中每一列与之的相似度，然后对这一行进行了softmax操作，得到权重。接下来将这个权重矩阵乘上V，得到自注意力的值。最后得到的结果矩阵中，每一行结果都是根据权重矩阵对V进行加权求和得到的，也正是在这里，发生了时序信息的混合。</p><p>自注意力有如下几个特点</p><ul><li>对KQV的排列具有不变性（实际上是等价性）。也就是说，如果按行重排KQV，自注意力的结果不会因此改变，只会因此发生对应的重排。</li><li>自注意力机制会在所有的时间步上起作用，也就是自注意力可以混合时序信息。</li><li>计算开销为O(T2d)O(T2d)。</li></ul><p>一个Transformer Block结构如下图所示：<img alt=image.png|150 loading=lazy src="https://pics.zhouxin.space/202409080919833.png?x-oss-process=image/quality,q_90/format,webp"></p><p>这个流程用公式表示为：</p><p>Z~:=SelfAttention(Z(i)WK,Z(i)WQ,Z(i)WV)=softmax(Z(i)WKWVT(Z(i))Td1/2)Z(i)WVZ~:=LayerNorm(Z(i)+Z~)Z(i+1):=LayerNorm(ReLU(Z<del>W)+Z</del>)Z<del>Z</del>Z(i+1)​:=SelfAttention(Z(i)WK​,Z(i)WQ​,Z(i)WV​)=softmax(d1/2Z(i)WK​WVT​(Z(i))T​)Z(i)WV​:=LayerNorm(Z(i)+Z~):=LayerNorm(ReLU(Z<del>W)+Z</del>)​</p><p>Transformer的优点是：</p><ul><li>可以在一个block中混合所有时间步的时序信息；</li><li>随着时间步的增加，Transformer不需要额外引入新的参数。</li></ul><p>其缺点是：</p><ul><li>每个输出都依赖于所有时间步的输入；</li><li>输入没有时序，也就是说可以将时序打乱再输入给Transformer，结果还是一样的。</li></ul><p>接下来介绍两种技术针对缺点进行改进。</p><p>首先是掩码自注意力，即masked self-attention。之前提到，在自注意力的计算公式中，softmax后的KQTKQT是一个密集矩阵，每一行都是表示一个权重，会将所有时刻的状态加权求和。而掩码自注意力的做法是，将让KQTKQT的上三角部分减去无穷大，这样权重矩阵的上三角部分为0，即只对t之前的时刻加权求和，以防止获取未来信息。</p><p>为了解决输入时序的问题，引入了位置编码position encoding技术，给输入加上一个用于表示时间信息的矩阵，如下所示：</p><p>X∈Rn=[−x1⊤−−x2⊤−−xT⊤−]+[sin⁡(ω1⋅1)⋯sin⁡(ωn⋅1)sin⁡(ω1⋅2)⋯sin⁡(ωn⋅2)⋱sin⁡(ω1⋅T)⋯sin⁡(ωn⋅T)]X∈Rn=​−−−​x1⊤​x2⊤​⋮xT⊤​​−−−​​+​sin(ω1​⋅1)sin(ω1​⋅2)⋮sin(ω1​⋅T)​⋯⋯⋱⋯​sin(ωn​⋅1)sin(ωn​⋅2)⋮sin(ωn​⋅T)​​</p><p>通常，其中的wiwi​根据对数函数的变化趋势来选择。</p><h1 id=lecture-21-transformer-implementation>Lecture 21: Transformer Implementation<a hidden class=anchor aria-hidden=true href=#lecture-21-transformer-implementation>#</a></h1><p>本节课中，我们将使用NumPy来实现Transformer。</p><h2 id=自注意力机制-self-attention>自注意力机制 Self-attention<a hidden class=anchor aria-hidden=true href=#自注意力机制-self-attention>#</a></h2><p>自注意力的公式为：</p><p>Y=(softmax(XWKWQTXTd)XWV)WoY=(softmax(d​XWK​WQT​XT​)XWV​)Wo​</p><p>与上一讲有些许不同之处在于在输出前进行了一次额外的线性变换。</p><p>注意到公式中我们需要将X与三个W分别相乘以得到KQV，可以将这三次矩阵运算变为一个运算，即将三个矩阵concat在一起，然后与X相乘，一下子得到concat在一起的KQV。一个自注意力模块为：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>self_attention</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>mask</span><span class=p>,</span> <span class=n>W_KQV</span><span class=p>,</span> <span class=n>W_out</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>K</span><span class=p>,</span><span class=n>Q</span><span class=p>,</span><span class=n>V</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>split</span><span class=p>(</span><span class=n>X</span><span class=nd>@W_KQV</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=n>axis</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>attn</span> <span class=o>=</span> <span class=n>softmax</span><span class=p>(</span><span class=n>K</span><span class=nd>@Q.swapaxes</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span><span class=o>-</span><span class=mi>2</span><span class=p>)</span> <span class=o>/</span> <span class=n>np</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>X</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>])</span> <span class=o>+</span> <span class=n>mask</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>attn</span><span class=nd>@V@W_out</span><span class=p>,</span> <span class=n>attn</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=minibatching-with-batch-matrix-multiply>Minibatching with batch matrix multiply<a hidden class=anchor aria-hidden=true href=#minibatching-with-batch-matrix-multiply>#</a></h2><p>自注意力不是按照时间循序进行前向传播的，因此X仍按照正常BTD的顺序在内存中组织。当我们实现批量self-attention时，就涉及到了批量矩阵乘法的概念。</p><p>具体来说，公式中K@QTK@QT这一步的矩阵乘法，K和Q的shape都是B×T×T，这就涉及到了批量矩阵乘法。对我们都自注意力来说，想要的应该是K[i,:,:]与Q.T[i,:,:]相乘，碰巧，批量矩阵乘法正是这么定义的，也就是说，批量矩阵乘法要求两个矩阵之间除了倒数两个维度符合矩阵乘法要求，剩余其它维度要么不存在或为1进行广播，要么就是要相等的。</p><h2 id=multihead-attention-多头注意力>Multihead attention 多头注意力<a hidden class=anchor aria-hidden=true href=#multihead-attention-多头注意力>#</a></h2><p>多头自注意力的动机来自于K@QTK@QT这一步，结果每个元素都是长度为d的两个向量内积得到的。为了降低计算成本，提出了一种多头注意力机制。即，将KQV的每一行分为h个部分，进行注意力操作，然后再拼接起来。这样，K@QTK@QT每个值都是长度为d/h向量进行内积得到的。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>multihead_attention</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>mask</span><span class=p>,</span> <span class=n>heads</span><span class=p>,</span> <span class=n>W_KQV</span><span class=p>,</span> <span class=n>W_out</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>N</span><span class=p>,</span><span class=n>T</span><span class=p>,</span><span class=n>d</span> <span class=o>=</span> <span class=n>X</span><span class=o>.</span><span class=n>shape</span>
</span></span><span class=line><span class=cl>    <span class=n>K</span><span class=p>,</span><span class=n>Q</span><span class=p>,</span><span class=n>V</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>split</span><span class=p>(</span><span class=n>X</span><span class=nd>@W_KQV</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=n>axis</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>K</span><span class=p>,</span><span class=n>Q</span><span class=p>,</span><span class=n>V</span> <span class=o>=</span> <span class=p>[</span><span class=n>a</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>N</span><span class=p>,</span><span class=n>T</span><span class=p>,</span><span class=n>heads</span><span class=p>,</span><span class=n>d</span><span class=o>//</span><span class=n>heads</span><span class=p>)</span><span class=o>.</span><span class=n>swapaxes</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span><span class=mi>2</span><span class=p>)</span> <span class=k>for</span> <span class=n>a</span> <span class=ow>in</span> <span class=p>(</span><span class=n>K</span><span class=p>,</span><span class=n>Q</span><span class=p>,</span><span class=n>V</span><span class=p>)]</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=n>attn</span> <span class=o>=</span> <span class=n>softmax</span><span class=p>(</span><span class=n>K</span><span class=nd>@Q.swapaxes</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span><span class=o>-</span><span class=mi>2</span><span class=p>)</span> <span class=o>/</span> <span class=n>np</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>d</span><span class=o>//</span><span class=n>heads</span><span class=p>)</span> <span class=o>+</span> <span class=n>mask</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=p>(</span><span class=n>attn</span><span class=nd>@V</span><span class=p>)</span><span class=o>.</span><span class=n>swapaxes</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span><span class=mi>2</span><span class=p>)</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>N</span><span class=p>,</span><span class=n>T</span><span class=p>,</span><span class=n>d</span><span class=p>)</span> <span class=o>@</span> <span class=n>W_out</span><span class=p>,</span> <span class=n>attn</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=transformer-block>Transformer block<a hidden class=anchor aria-hidden=true href=#transformer-block>#</a></h2><p>一个Transformer Block结构如下图所示：
<img alt=image.png loading=lazy src="https://pics.zhouxin.space/202409080919833.png?x-oss-process=image/quality,q_90/format,webp"></p><p>应用已经实现的各个组件，我们可以轻松地写出一个支持多头自注意力的Transformer块：</p><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td><code>&lt;br>1&lt;br>2&lt;br>3&lt;br>4&lt;br>5&lt;br>6&lt;br>7&lt;br>8&lt;br>9&lt;br></code></td><td><code>python&lt;br>def layer_norm(Z, eps):&lt;br> return (Z - Z.mean(axis=-1, keepdims=True)) / np.sqrt(Z.var(axis=-1, keepdims=True) + eps)&lt;br> &lt;br>def relu(Z):&lt;br> return np.maximum(Z,0)&lt;br>&lt;br>def transformer(X, mask, heads, W_KQV, W_out, W_ff1, W_ff2, eps):&lt;br> Z = layer_norm(multihead_attention(X, mask, heads, W_KQV, W_out)[0] + X, eps)&lt;br> return layer_norm(Z + relu(Z@W_ff1)@W_ff2, eps)&lt;br></code></td></tr></tbody></table><h1 id=lecture-23-moel-deployment>Lecture 23 Moel Deployment<a hidden class=anchor aria-hidden=true href=#lecture-23-moel-deployment>#</a></h1><h2 id=模型部署概览-model-deployment-overview>模型部署概览 Model deployment overview<a hidden class=anchor aria-hidden=true href=#模型部署概览-model-deployment-overview>#</a></h2><p>在特定的设备上部署训练好的模型是一件比较麻烦的事情，其受设备的影响很大。现在有一些用于部署推理的框架，例如NVIDIA设备上的TensorRT，在嵌入式设备上有ARMComputeLib和TFLite，苹果有CoreML。</p><p>上述框架都需要一种推理模型格式的输入，这个输入能够描述模型的计算流程，这种格式目前有ONNX、CoreML和TFLite。模型通过Python编写，其好处是提高了编码效率，带来的缺点就是可能某些模型没办法完美转换为上述通用格式。</p><p>许多推理框架都是以计算图解释器的形式组织的，其通过预分配和重用内存、算子融合、精度量化等优化手段，实现更高效的推理。但同样，其也有很多限制，例如他们支持的算子类别是有限的。</p><h2 id=机器学习编译-machine-learning-compilation>机器学习编译 Machine learning compilation<a hidden class=anchor aria-hidden=true href=#机器学习编译-machine-learning-compilation>#</a></h2><p>机器学习编译试图打破需要为每种设备定制推理库的现状，其目标是将输入的深度学习模型转换为可以直接在终端上运行的代码。</p><p>一个ML程序可以被称为一个模块，这个模块由多个函数构成，函数之间互相调用。下图这种格式被称为中间状态表示IR，下图这一模块被称为IR模块。</p><p><img alt=image.png|400 loading=lazy src="https://pics.zhouxin.space/202409081958517.png?x-oss-process=image/quality,q_90/format,webp"></p><p>ML编译的流程大致有：</p><ul><li><p>从深度学习框架中导入模型；<img alt=|400 loading=lazy src="https://pics.zhouxin.space/202409082011257.png?x-oss-process=image/quality,q_90/format,webp"></p></li><li><p>对IR模块进行变换，算子融合<img alt=image.png|400 loading=lazy src="https://pics.zhouxin.space/202409082011947.png?x-oss-process=image/quality,q_90/format,webp"></p></li><li><p>将中间状态翻译为更低级的循环代码<img alt=image.png|400 loading=lazy src="https://pics.zhouxin.space/202409082011782.png?x-oss-process=image/quality,q_90/format,webp"></p></li><li><p>进行更低级的变换，进行算子融合<img alt=image.png|400 loading=lazy src="https://pics.zhouxin.space/202409082013131.png?x-oss-process=image/quality,q_90/format,webp"></p></li><li><p>进行代码生成<img alt=image.png|400 loading=lazy src="https://pics.zhouxin.space/202409082014722.png?x-oss-process=image/quality,q_90/format,webp">
本讲后续内容和下一讲均为介绍MLC，计划后面继续学习MLC，这里就不浅尝辄止了。</p></li></ul><p>全文完。</p><h1 id=参考文档>参考文档<a hidden class=anchor aria-hidden=true href=#参考文档>#</a></h1><hr><ol><li><p><a href=https://blog.csdn.net/qq_36892712/article/details/133774755>指数移动平均EMA_ema移动平均数怎么算-CSDN博客</a> <a href=https://www.zhouxin.space/notes/notes-on-cmu-10-414-deep-learning-system/#fnref:1>↩︎</a></p></li><li><p><a href=https://zhuanlan.zhihu.com/p/22810533>zhuanlan.zhihu.com/p/22810533</a> <a href=https://www.zhouxin.space/notes/notes-on-cmu-10-414-deep-learning-system/#fnref:2>↩︎</a></p></li><li><p><a href=https://www.ruder.io/optimizing-gradient-descent/#Nesterov%20accelerated%20gradient>An overview of gradient descent optimization algorithms</a> <a href=https://www.zhouxin.space/notes/notes-on-cmu-10-414-deep-learning-system/#fnref:3>↩︎</a></p></li><li><p><a href=https://numpy.org/doc/stable/reference/generated/numpy.lib.stride_tricks.as_strided.html>numpy.lib.stride_tricks.as_strided — NumPy v2.1 Manual</a> <a href=https://www.zhouxin.space/notes/notes-on-cmu-10-414-deep-learning-system/#fnref:4>↩︎</a></p></li></ol><ul><li><a href=https://www.zhouxin.space/tags/cuda/>CUDA</a></li><li><a href=https://www.zhouxin.space/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F/>深度学习系统</a></li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:13448/tags/system/>System</a></li></ul><nav class=paginav><a class=prev href=http://localhost:13448/post/cmu10-174/><span class=title>« Prev</span><br><span>CMU10-414 Assigenment</span>
</a><a class=next href=http://localhost:13448/post/web_changing/><span class=title>Next »</span><br><span>Web Changing</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=http://localhost:13448/>Shihong Yuan's Course Note</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><span class=topInner><svg class="topSvg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
<span id=read_progress></span>
</span></a><script>document.addEventListener("scroll",function(){const t=document.getElementById("read_progress"),n=document.documentElement.scrollHeight,s=document.documentElement.clientHeight,o=document.documentElement.scrollTop||document.body.scrollTop;t.innerText=((o/(n-s)).toFixed(2)*100).toFixed(0)})</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>